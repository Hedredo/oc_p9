{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdbc30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# …existing code…\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from torchvision import transforms\n",
    "\n",
    "# Personnalisation du pipeline d'entraînement\n",
    "train_transform = create_transform(\n",
    "    input_size=INPUT_RESOLUTION,\n",
    "    is_training=True,\n",
    "    mean=model.config.mean,\n",
    "    std=model.config.std,\n",
    "    crop_mode=model.config.crop_mode,\n",
    "    crop_pct=model.config.crop_pct,\n",
    "    color_jitter=(0.3, 0.3, 0.3, 0.1),   # ajustement du jitter\n",
    "    re_prob=0.2,                         # probabilité de RandomErasing\n",
    "    re_count=1,\n",
    "    re_mode='pixel',\n",
    "    interpolation='bicubic'\n",
    ")\n",
    "\n",
    "# Ou un pipeline 100% torchvision si vous préférez un contrôle total\n",
    "custom_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(INPUT_RESOLUTION, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=model.config.mean, std=model.config.std),\n",
    "])\n",
    "\n",
    "# Personnalisation du pipeline d'entraînement\n",
    "train_transform = create_transform(\n",
    "    input_size=INPUT_RESOLUTION,\n",
    "    is_training=True,\n",
    "    mean=model.config.mean,\n",
    "    std=model.config.std,\n",
    "    crop_mode=model.config.crop_mode,\n",
    "    crop_pct=model.config.crop_pct,\n",
    "    color_jitter=(0.3, 0.3, 0.3, 0.1),   # ajustement du jitter\n",
    "    re_prob=0.2,                         # probabilité de RandomErasing\n",
    "    re_count=1,\n",
    "    re_mode='pixel',\n",
    "    interpolation='bicubic'\n",
    ")\n",
    "\n",
    "# Ou un pipeline 100% torchvision si vous préférez un contrôle total\n",
    "from torchvision import transforms\n",
    "custom_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(INPUT_RESOLUTION, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=model.config.mean, std=model.config.std),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fab1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"nvidia/MambaVision-L-21K\", trust_remote_code=True)\n",
    "\n",
    "# eval mode for inference\n",
    "model.cuda().eval()\n",
    "\n",
    "# prepare image for the model\n",
    "url = 'http://images.cocodataset.org/val2017/000000020247.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=False,\n",
    "                             mean=model.config.mean,\n",
    "                             std=model.config.std,\n",
    "                             crop_mode=model.config.crop_mode,\n",
    "                             crop_pct=model.config.crop_pct)\n",
    "inputs = transform(image).unsqueeze(0).cuda()\n",
    "# model inference\n",
    "out_avg_pool, features = model(inputs)\n",
    "print(\"Size of the averaged pool features:\", out_avg_pool.size())  # torch.Size([1, 768])\n",
    "print(\"Number of stages in extracted features:\", len(features)) # 4 stages\n",
    "print(\"Size of extracted features in stage 1:\", features[0].size()) # torch.Size([1, 96, 56, 56])\n",
    "print(\"Size of extracted features in stage 4:\", features[3].size()) # torch.Size([1, 768, 7, 7])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
