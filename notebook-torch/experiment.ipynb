{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc430ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL\n",
    "# TODO : Compléter toutes les parties en MARKDOWN\n",
    "# TODO : IF NEEDED créer une data aug avec albumentations\n",
    "# TODO : Lire l'extension devcontainers\n",
    "\n",
    "# EXPERIMENTS\n",
    "# TODO : Ajouter une card dans le tensorboard pour les paramètres du modèle\n",
    "# TODO : warning PIL decompression bomb warning ,\n",
    "# TODO : Tester le batch size de 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b170e",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "#TODO Ce notebook présente une analyse de données sur les ventes de jeux vidéo. L'objectif est d'explorer les tendances des ventes en fonction de la plateforme, du genre et de la région."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccda75",
   "metadata": {},
   "source": [
    "# PRE-REQUIS\n",
    "\n",
    "Ce bloc contient tout ce qui est nécessaire pour le fonctionnement des expériences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e97c4",
   "metadata": {},
   "source": [
    "## Imports & Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d9aa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from pathlib import PosixPath\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor, AutoModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from constants import ROOT_FOLDER, IMAGE_FOLDER, ARTIFACTS_FOLDER, DATASET_PATH\n",
    "from constants import SEED, VAL_SIZE, TEST_SIZE, BATCH_SIZE, SAMPLING, INPUT_RESOLUTION\n",
    "from constants import MAMBA_HIDDEN_SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad8ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion des avertissements\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "651be0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  2.6.0+cu124\n",
      "Cuda version:  12.4\n",
      "CUDNN version:  90100\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration de cuda avec PyTorch\n",
    "print(\"Torch version: \", torch.__version__)\n",
    "print(\"Cuda version: \", torch.version.cuda)\n",
    "print(\"CUDNN version: \", torch.backends.cudnn.version())\n",
    "\n",
    "# Set the device to GPU if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab3639",
   "metadata": {},
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64198e15",
   "metadata": {},
   "source": [
    "### Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f68941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1050, 2)\n",
      "Dataset columns: Index(['image', 'class'], dtype='object')\n",
      "Number of classes: 7\n",
      "Classes: ['Baby Care', 'Beauty and Personal Care', 'Computers', 'Home Decor & Festive Needs', 'Home Furnishing', 'Kitchen & Dining', 'Watches']\n"
     ]
    }
   ],
   "source": [
    "# Loading the pickle dataset_cleaned used with the previous project as a pandas df\n",
    "df = pd.read_pickle(DATASET_PATH).drop(columns=[\"product_name\", \"description\"])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Dataset columns: {df.columns}\")\n",
    "\n",
    "# Encode the labels with LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df[\"class\"])\n",
    "N_CLASSES = len(le.classes_)\n",
    "CLASSES = le.classes_.tolist()\n",
    "print(f\"Number of classes: {N_CLASSES}\")\n",
    "print(f\"Classes: {CLASSES}\")\n",
    "\n",
    "# Finally transform the class column to the encoded labels\n",
    "df[\"class\"] = le.transform(df[\"class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a8b51",
   "metadata": {},
   "source": [
    "### Séparation des données (train/validation/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a8c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, train_path, val_path, test_path):\n",
    "        # Splitting the datasets into train, val and test sets\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            df[\"image\"],\n",
    "            df[\"class\"],\n",
    "            test_size=TEST_SIZE,\n",
    "            random_state=SEED,\n",
    "            stratify=df[\"class\"],\n",
    "            shuffle=True,\n",
    "        )\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=VAL_SIZE, random_state=SEED, stratify=y_temp, shuffle=True\n",
    "        )\n",
    "\n",
    "        # Concat X and y for each set\n",
    "        train = (\n",
    "            pd.concat([X_train, y_train], axis=1).sample(SAMPLING)\n",
    "            if SAMPLING\n",
    "            else pd.concat([X_train, y_train], axis=1)\n",
    "        )\n",
    "        train.to_pickle(train_path)\n",
    "        val = (\n",
    "            pd.concat([X_val, y_val], axis=1).sample(SAMPLING)\n",
    "            if SAMPLING\n",
    "            else pd.concat([X_val, y_val], axis=1)\n",
    "        )\n",
    "        val.to_pickle(val_path)\n",
    "        test = (\n",
    "            pd.concat([X_test, y_test], axis=1).sample(SAMPLING)\n",
    "            if SAMPLING\n",
    "            else pd.concat([X_test, y_test], axis=1)\n",
    "        )\n",
    "        test.to_pickle(test_path)\n",
    "    \n",
    "def load_splits(train_path, val_path, test_path): \n",
    "    # Load the saved files if they exist\n",
    "    try:\n",
    "        train = pd.read_pickle(train_path)\n",
    "        val = pd.read_pickle(val_path)\n",
    "        test = pd.read_pickle(test_path)\n",
    "    except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            print(\"This file has not been found. Please check the paths before.\")\n",
    "    \n",
    "    # Finally print the shapes of the datasets\n",
    "    print(f\"Train shape: {train.shape}\")\n",
    "    print(f\"Val shape: {val.shape}\")\n",
    "    print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91d9afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (32, 2)\n",
      "Val shape: (32, 2)\n",
      "Test shape: (32, 2)\n"
     ]
    }
   ],
   "source": [
    "# Define the path to save the splitted cleaned datasets\n",
    "completion = SAMPLING if SAMPLING else \"full\"\n",
    "train_path = ROOT_FOLDER / \"data\" / f\"trainset_{completion}.pickle\"\n",
    "val_path = ROOT_FOLDER / \"data\" / f\"valset_{completion}.pickle\"\n",
    "test_path = ROOT_FOLDER / \"data\" / f\"testset_{completion}.pickle\"\n",
    "\n",
    "# Load the splitted datasets if they exist\n",
    "if os.path.exists(train_path) and os.path.exists(val_path) and os.path.exists(test_path):\n",
    "    train, val, test = load_splits(train_path, val_path, test_path)\n",
    "else:\n",
    "    # If the one or more files do not exist, split the dataset and save/overwrite the files\n",
    "    split_dataset(df, train_path, val_path, test_path)\n",
    "    train, val, test = load_splits(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d4ee5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "07ccc0f6-ede2-45d0-9ed6-0eca2fb3a196",
       "rows": [
        [
         "369",
         "53c4f1e5cb1767f1a6ba05d32dfaf107.jpg",
         "2"
        ],
        [
         "814",
         "414d2065e1aed847064bcf14538eabc8.jpg",
         "4"
        ],
        [
         "786",
         "c705a5735a94aeee547d1798e3e46ec4.jpg",
         "6"
        ],
        [
         "293",
         "c2efa8aa11898bdb5fc4e46201973a42.jpg",
         "0"
        ],
        [
         "549",
         "8a15fa23b8c39ac1c7eaac94cbbe6fc6.jpg",
         "6"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>53c4f1e5cb1767f1a6ba05d32dfaf107.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>414d2065e1aed847064bcf14538eabc8.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>c705a5735a94aeee547d1798e3e46ec4.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>c2efa8aa11898bdb5fc4e46201973a42.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>8a15fa23b8c39ac1c7eaac94cbbe6fc6.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "369  53c4f1e5cb1767f1a6ba05d32dfaf107.jpg      2\n",
       "814  414d2065e1aed847064bcf14538eabc8.jpg      4\n",
       "786  c705a5735a94aeee547d1798e3e46ec4.jpg      6\n",
       "293  c2efa8aa11898bdb5fc4e46201973a42.jpg      0\n",
       "549  8a15fa23b8c39ac1c7eaac94cbbe6fc6.jpg      6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4eede8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "3fc6f979-c0a4-41b8-86c6-02e8e90cb52e",
       "rows": [
        [
         "165",
         "785b4383b12106dd172306d427d8f7b2.jpg",
         "5"
        ],
        [
         "135",
         "fd369a1bb58ae02cd925dda7e9e1d00a.jpg",
         "5"
        ],
        [
         "218",
         "ce02ab514e4c623dcf7acd7182a7762d.jpg",
         "5"
        ],
        [
         "698",
         "10b6738d7058738a9a7d107bbb2833f4.jpg",
         "4"
        ],
        [
         "110",
         "f1ee63a5f0db853e2c1b377778580d39.jpg",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>785b4383b12106dd172306d427d8f7b2.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>fd369a1bb58ae02cd925dda7e9e1d00a.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>ce02ab514e4c623dcf7acd7182a7762d.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>10b6738d7058738a9a7d107bbb2833f4.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>f1ee63a5f0db853e2c1b377778580d39.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "165  785b4383b12106dd172306d427d8f7b2.jpg      5\n",
       "135  fd369a1bb58ae02cd925dda7e9e1d00a.jpg      5\n",
       "218  ce02ab514e4c623dcf7acd7182a7762d.jpg      5\n",
       "698  10b6738d7058738a9a7d107bbb2833f4.jpg      4\n",
       "110  f1ee63a5f0db853e2c1b377778580d39.jpg      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6ba2b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c1aa193c-15d1-4bb0-aa34-eba19bb1fc0e",
       "rows": [
        [
         "228",
         "9f76aaed44b540869da862c5dd4b266f.jpg",
         "3"
        ],
        [
         "235",
         "e1e133cbe616bee3b3e9e6f4f0882e3d.jpg",
         "5"
        ],
        [
         "25",
         "3c74db45cbfc14a78b8466dca494b3bb.jpg",
         "6"
        ],
        [
         "405",
         "a231459c1562ee1117e95a61128820b7.jpg",
         "1"
        ],
        [
         "431",
         "281dfd9b94b34362c295b3e5f8da5dfa.jpg",
         "2"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>9f76aaed44b540869da862c5dd4b266f.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>e1e133cbe616bee3b3e9e6f4f0882e3d.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3c74db45cbfc14a78b8466dca494b3bb.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>a231459c1562ee1117e95a61128820b7.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>281dfd9b94b34362c295b3e5f8da5dfa.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "228  9f76aaed44b540869da862c5dd4b266f.jpg      3\n",
       "235  e1e133cbe616bee3b3e9e6f4f0882e3d.jpg      5\n",
       "25   3c74db45cbfc14a78b8466dca494b3bb.jpg      6\n",
       "405  a231459c1562ee1117e95a61128820b7.jpg      1\n",
       "431  281dfd9b94b34362c295b3e5f8da5dfa.jpg      2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116edde",
   "metadata": {},
   "source": [
    "## Classes et Fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028814d7",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "[ ] ***TODO*** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8012a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block DataLoader\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe: pd.DataFrame,\n",
    "        image_dir: PosixPath,\n",
    "        processor: AutoImageProcessor = None,\n",
    "        transform: callable = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing image file names and labels.\n",
    "            image_dir (PosixPath): Directory where images are stored.\n",
    "            processor (AutoImageProcessor, optional): Hugging Face processor for image preprocessing. Defaults to None.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(\n",
    "            self.image_dir, self.dataframe.iloc[idx, 0]\n",
    "        )  # Assuming image file names are in the first column\n",
    "        image = Image.open(img_name).convert(\"RGB\")  # Ensure consistent color format\n",
    "\n",
    "        label = self.dataframe.iloc[idx, 1]  # Assuming labels are in the second column\n",
    "\n",
    "        if self.processor:\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            image = inputs[\"pixel_values\"].squeeze()  # Remove batch dimension\n",
    "        elif self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc9e31",
   "metadata": {},
   "source": [
    "### MambaClassifier\n",
    "\n",
    "[ ] ***TODO*** : L'ajout de la classe MambaClassifier permet de créer un classificateur basé sur le modèle MambaVision en tenant compte de ses spécificités. Cette classe hérite de la classe `nn.Module` de PyTorch et encapsule le modèle MambaVision, permettant ainsi de l'utiliser comme un classificateur dans le cadre d'une tâche de classification d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "971655ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block MambaClassifier\n",
    "class MambaClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: AutoModel,\n",
    "        num_classes: int,\n",
    "        hidden_dim: int,  # The hidden dimension of the backbone is stored in the MAMBA_HIDDEN_SIZES dict with the model card as the key\n",
    "        fc_layer: int = None,  # Add the int number of layers before the classifier\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.config = self.backbone.config\n",
    "        if fc_layer:\n",
    "            self.fc_layers = nn.ModuleList()\n",
    "            for i in range(fc_layer):\n",
    "                if i == 0:\n",
    "                    self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                else:\n",
    "                    self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                self.fc_layers.append(nn.ReLU())\n",
    "                self.fc_layers.append(nn.Dropout(0.1))\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def create_transform(self, training: bool, auto_augment=None):\n",
    "        transform = create_transform(\n",
    "            input_size=INPUT_RESOLUTION,\n",
    "            is_training=training,  # Add a ColorJitter augmentation during training\n",
    "            mean=self.config.mean,\n",
    "            std=self.config.std,\n",
    "            crop_mode=self.config.crop_mode,\n",
    "            crop_pct=self.config.crop_pct,\n",
    "            auto_augment=auto_augment, # \"rand-m9-mstd0.5-inc1\"\n",
    "            )\n",
    "        return transform\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_avg_pool, _ = self.backbone(x)\n",
    "        if hasattr(self, \"fc_layers\"):\n",
    "            for layer in self.fc_layers:\n",
    "                out_avg_pool = layer(out_avg_pool)\n",
    "        logits = self.classifier(out_avg_pool)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5bc876",
   "metadata": {},
   "source": [
    "### TorchPipeline\n",
    "\n",
    "[ ] ***TODO*** : Un pipeline de traitement des données est créé pour gérer les transformations d'images et les normalisations nécessaires avant de passer les données au modèle. Ce pipeline utilise la bibliothèque `torchvision` pour appliquer des transformations telles que le redimensionnement, le recadrage, la conversion en tenseur et la normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44510476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchPipeline:\n",
    "    def __init__(self, model_card, model, processor=None, train_transform=None, test_transform=None):\n",
    "        # Initialize attributes from parameters\n",
    "        self.model_card = model_card\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.train_transform = train_transform\n",
    "        self.test_transform = test_transform\n",
    "\n",
    "        # Check if the model is a MambaClassifier instance\n",
    "        self.mamba = \"MambaClassifier\" in type(model).__name__\n",
    "\n",
    "        # Initialize FIXED attributes from constants.py\n",
    "        self.device = DEVICE\n",
    "        self.root_folder = ROOT_FOLDER\n",
    "        self.artifacts_folder = ARTIFACTS_FOLDER\n",
    "        self.dataset_path = DATASET_PATH\n",
    "        self.image_folder = IMAGE_FOLDER\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.classes = CLASSES\n",
    "        self.n_classes = N_CLASSES\n",
    "\n",
    "        # Initialize empty attributes\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.test_loader = None\n",
    "        self.writer = None\n",
    "        self.experiment_id = None\n",
    "\n",
    "    @staticmethod\n",
    "    def classification_report_to_markdown(y_true, y_pred, target_names):\n",
    "        \"\"\"\n",
    "        Convert the classification report to a markdown table.\n",
    "        \"\"\"\n",
    "        report = classification_report(\n",
    "            y_true, y_pred, target_names=target_names, zero_division=0, output_dict=True\n",
    "        )\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_df = report_df.drop(columns=[\"support\"])\n",
    "        report_df = report_df.rename_axis(\"Classes\").reset_index()\n",
    "        return report_df.to_markdown(index=False)\n",
    "\n",
    "    def generate_experiment_id(self):\n",
    "        \"\"\"\n",
    "        Generate a unique experiment ID based on the current date and time.\n",
    "        \"\"\"\n",
    "        return \"_\".join(\n",
    "            [datetime.now().strftime(\"%Y%m%d-%H%M%S\"), self.model_card.split(\"/\")[-1]]\n",
    "        )\n",
    "\n",
    "    # Load the data, apply the processor and transform, and create the dataloaders\n",
    "    def generate_dataloader(self, train, val, test):\n",
    "        # Apply the processor and transform\n",
    "        train_dataset = ImageDataset(dataframe=train, image_dir=self.image_folder, processor=self.processor, transform=self.train_transform)\n",
    "        val_dataset = ImageDataset(dataframe=val, image_dir=self.image_folder, processor=self.processor, transform=self.test_transform)\n",
    "        test_dataset = ImageDataset(dataframe=test, image_dir=self.image_folder, processor=self.processor, transform=self.test_transform)\n",
    "\n",
    "        # Create the dataloaders\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "    def train_and_eval_model(self, criterion, optimizer, num_epochs):\n",
    "        # Generate the experiment ID\n",
    "        self.experiment_id = self.generate_experiment_id()\n",
    "        # Create the writer\n",
    "        log_dir = (\n",
    "            ROOT_FOLDER\n",
    "            # TODO: Delete if OK / os.getcwd().split(\"/\")[-1]\n",
    "            / \"runs\"\n",
    "            / self.experiment_id\n",
    "        )\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "\n",
    "        # Initialize the training metrics\n",
    "        running_train_time_by_step = 0.0\n",
    "        running_train_time_by_epoch = 0.0\n",
    "        running_train_steps = 0\n",
    "        self.writer.add_scalar(\"TimingByEpoch/train\", running_train_time_by_epoch, 0)\n",
    "\n",
    "        # Initialize the validation metrics\n",
    "        best_val_metric = float(\"-inf\")\n",
    "        running_val_time_by_step = 0.0\n",
    "        running_val_time_by_epoch = 0.0\n",
    "        running_val_steps = 0\n",
    "        self.writer.add_scalar(\"TimingByEpoch/validation\", running_val_time_by_epoch, 0)\n",
    "\n",
    "        self.writer.add_scalar(\"TimingByEpoch/train\", running_train_time_by_epoch, 0)\n",
    "        # Move model to the device\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "        # Training loop\n",
    "        print(\"TRAINING EXPERIMENT ID <{}>\".format(self.experiment_id))\n",
    "        print(\"==========================\")\n",
    "        for epoch in range(num_epochs):\n",
    "            # --- 1. TRAINING LOOP ---\n",
    "            self.model.train()\n",
    "            train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "            epoch_time = time()\n",
    "            for i, (inputs, labels) in enumerate(\n",
    "                self.train_loader\n",
    "            ):  # Use enumerate for step count\n",
    "                step_time = time()\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                # Compute the model outputs given the mamba variable\n",
    "                outputs = self.model(inputs).logits if not self.mamba else self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step_loss = loss.item() * inputs.size(0)\n",
    "                train_loss += step_loss\n",
    "                preds_train = outputs.argmax(dim=1)\n",
    "                correct_train += (preds_train == labels).sum().item()\n",
    "                total_train += inputs.size(0)\n",
    "                running_train_steps += 1\n",
    "                running_train_time_by_step += time() - step_time\n",
    "                self.writer.add_scalar(\n",
    "                    \"TimingByStep/train\", running_train_time_by_step, running_train_steps\n",
    "                )\n",
    "                self.writer.add_scalar(\"LossByStep/train\", step_loss, running_train_steps)\n",
    "\n",
    "            running_train_time_by_epoch += time() - epoch_time\n",
    "            epoch_train_loss = train_loss / total_train\n",
    "            epoch_train_acc = correct_train / total_train\n",
    "            # Log training metrics per epoch\n",
    "            self.writer.add_scalar(\"LossByEpoch/train\", epoch_train_loss, epoch)\n",
    "            self.writer.add_scalar(\"Accuracy/train\", epoch_train_acc, epoch)\n",
    "            self.writer.add_scalar(\"TimingByEpoch/train\", running_train_time_by_epoch, epoch)\n",
    "            stats = f\"Epoch [{epoch + 1}/{num_epochs}] | Train_Loss: {epoch_train_loss:.4f} | Train_Acc: {epoch_train_acc:.4f}\"\n",
    "\n",
    "            # --- 2. VALIDATION LOOP ---\n",
    "            self.model.eval()\n",
    "            val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "            epoch_time = time()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in self.val_loader:\n",
    "                    # Move images and labels to the device\n",
    "                    step_time = time()\n",
    "                    inputs = inputs.to(DEVICE)\n",
    "                    labels = labels.to(DEVICE)\n",
    "                    outputs = self.model(inputs).logits if not self.mamba else self.model(inputs)\n",
    "                    val_step_loss = criterion(outputs, labels).item() * inputs.size(0)\n",
    "                    val_loss += val_step_loss\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    correct_val += (preds == labels).sum().item()\n",
    "                    total_val += inputs.size(0)\n",
    "                    running_val_steps += 1\n",
    "                    running_val_time_by_step += time() - step_time\n",
    "                    self.writer.add_scalar(\n",
    "                        \"TimingByStep/validation\",\n",
    "                        running_val_time_by_step,\n",
    "                        running_val_steps,\n",
    "                    )\n",
    "                    self.writer.add_scalar(\n",
    "                        \"LossByStep/validation\", val_step_loss, running_train_steps\n",
    "                    )\n",
    "\n",
    "            running_val_time_by_epoch += time() - epoch_time\n",
    "            epoch_val_loss = val_loss / total_val\n",
    "            epoch_val_acc = correct_val / total_val\n",
    "            # Log validation metrics per epoch\n",
    "            self.writer.add_scalar(\"Loss/validation\", epoch_val_loss, epoch)\n",
    "            self.writer.add_scalar(\"Accuracy/validation\", epoch_val_acc, epoch)\n",
    "            self.writer.add_scalar(\"TimingByEpoch/validation\", running_val_time_by_epoch, epoch)\n",
    "            stats += f\" | Val_Loss: {epoch_val_loss:.4f} | Val_Acc: {epoch_val_acc:.4f}\"\n",
    "\n",
    "            # --- 3. UPDATE BEST MODEL ---\n",
    "            # Save the model if the validation accuracy is better than the best one\n",
    "            if epoch_val_acc > best_val_metric:\n",
    "                best_val_metric = epoch_val_acc\n",
    "                torch.save(self.model.state_dict(), ARTIFACTS_FOLDER / f\"{self.experiment_id}.pth\")\n",
    "                stats += \" -> Best model updated\"\n",
    "            # Print the stats at the end of each epoch\n",
    "            print(stats)\n",
    "         \n",
    "        # Delete the model from GPU memory\n",
    "        del inputs, labels, outputs, optimizer, criterion, loss, val_loss, step_loss, val_step_loss\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Training completed. Best validation accuracy: {best_val_metric:.4f}. Running steps training time: {running_train_time_by_epoch:.2f} s.\\n\")\n",
    "    \n",
    "    def test_model(self, criterion, with_id=None,):\n",
    "        # Case when no id has been provided\n",
    "        if with_id is None:\n",
    "            try:\n",
    "                self.model.load_state_dict(torch.load(ARTIFACTS_FOLDER / f\"{self.experiment_id}.pth\"))\n",
    "            except FileNotFoundError:\n",
    "                print(\"No id has been provided and no model has been trained yet. Train the model first before testing.\")\n",
    "                return\n",
    "        # Case when id has been provided\n",
    "        else:\n",
    "            try:\n",
    "                self.model.load_state_dict(torch.load(ARTIFACTS_FOLDER / f\"{with_id}.pth\"))\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model {with_id}.pth not found in {ARTIFACTS_FOLDER}\")\n",
    "                print(\"Please provide a valid model ID.\")\n",
    "                return\n",
    "      \n",
    "        # If no error is raised, the model is loaded successfully\n",
    "        self.model.to(DEVICE)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Initialize the test metrics\n",
    "        test_loss, correct_test, total_test = 0.0, 0, 0\n",
    "        running_test_steps = 0\n",
    "        running_test_time_by_step = 0.0\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        print(f\"TESTING EXPERIMENT ID <{with_id if with_id else self.experiment_id}>\")\n",
    "        print(\"==========================\")\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.test_loader:\n",
    "                step_time = time()\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                outputs = self.model(inputs).logits if not self.mamba else self.model(inputs)\n",
    "                test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct_test += (preds == labels).sum().item()\n",
    "                total_test += inputs.size(0)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "                running_test_time_by_step += time() - step_time\n",
    "                running_test_steps += 1\n",
    "                if self.writer:\n",
    "                    self.writer.add_scalar(\n",
    "                        \"TimingByStep/test\", running_test_time_by_step, running_test_steps\n",
    "                    )\n",
    "\n",
    "        epoch_test_loss = test_loss / total_test\n",
    "        epoch_test_acc = correct_test / total_test\n",
    "        if self.writer:\n",
    "            self.writer.add_scalar(\"Accuracy/test\", epoch_test_acc, 0)\n",
    "\n",
    "        print(f\"Test Loss: {epoch_test_loss:.4f} | Test Acc: {epoch_test_acc:.4f} | Running steps test time: {running_test_time_by_step:.2f} s.\")\n",
    "\n",
    "        # Create the Classification report\n",
    "        classification_report_md = self.classification_report_to_markdown(\n",
    "            y_true, y_pred, target_names=self.classes\n",
    "        )\n",
    "\n",
    "        # Create the confusion matrix\n",
    "        cm = ConfusionMatrixDisplay.from_predictions(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            labels=range(self.n_classes),\n",
    "            normalize=\"true\",\n",
    "            display_labels=self.classes,\n",
    "            xticks_rotation=\"vertical\",\n",
    "            cmap=plt.cm.Blues,\n",
    "        )\n",
    "        # Save the classification report as a markdown file in tensorboard\n",
    "        if self.writer:\n",
    "            self.writer.add_text(\"ClassificationReport/test\", classification_report_md, 0)\n",
    "            self.writer.add_figure(\"ConfusionMatrix/test\", cm.figure_, 0)\n",
    "            self.writer.close() # Close the writer after the end of the pipeline\n",
    "        # If no writer is provided, print the classification report & confusion matrix\n",
    "        else:\n",
    "            print(classification_report_md)\n",
    "            cm.figure_.show()\n",
    "        \n",
    "        # Delete the model from GPU memory\n",
    "        del self.model, inputs, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c3a8c",
   "metadata": {},
   "source": [
    "# EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03437169",
   "metadata": {},
   "source": [
    "## **Google VIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e82faadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Define the model\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_card, num_labels=N_CLASSES, trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Define the image processor\n",
    "processor = AutoImageProcessor.from_pretrained(model_card)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3259ffc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250502-194155_vit-base-patch16-224-in21k>\n",
      "==========================\n",
      "Epoch [1/3] | Train_Loss: 1.9381 | Train_Acc: 0.1562 | Val_Loss: 1.9423 | Val_Acc: 0.1250 -> Best model updated\n",
      "Epoch [2/3] | Train_Loss: 1.8552 | Train_Acc: 0.5312 | Val_Loss: 1.9340 | Val_Acc: 0.1562 -> Best model updated\n",
      "Epoch [3/3] | Train_Loss: 1.7899 | Train_Acc: 0.7188 | Val_Loss: 1.9253 | Val_Acc: 0.1875 -> Best model updated\n",
      "Training completed. Best validation accuracy: 0.1875. Running steps training time: 10.79 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250502-194155_vit-base-patch16-224-in21k>\n",
      "==========================\n",
      "Test Loss: 1.9142 | Test Acc: 0.2812 | Running steps test time: 0.81 s.\n"
     ]
    }
   ],
   "source": [
    "# Define the num_epochs and learning rate\n",
    "num_epochs = 3\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea071149",
   "metadata": {},
   "source": [
    "## MOBILENETV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"google/mobilenet_v2_1.0_224\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e274bf",
   "metadata": {},
   "source": [
    "## **MAMBA T 1 K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "982b5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"nvidia/MambaVision-T-1K\"\n",
    "\n",
    "# Define the model with the MambaClassifier class\n",
    "model = MambaClassifier(\n",
    "    AutoModel.from_pretrained(model_card, trust_remote_code=True),\n",
    "    num_classes=N_CLASSES,\n",
    "    hidden_dim=MAMBA_HIDDEN_SIZES.get(model_card),\n",
    ")\n",
    "\n",
    "# Define the image transform (processor and transform)\n",
    "train_transform = model.create_transform(training=True) # auto_augment=\"rand-m9-mstd0.5-inc1\"\n",
    "test_transform = model.create_transform(training=False)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    train_transform=train_transform,\n",
    "    test_transform=test_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41c0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250505-165152_MambaVision-T-1K>\n",
      "==========================\n",
      "Epoch [1/3] | Train_Loss: 1.9517 | Train_Acc: 0.1250 | Val_Loss: 1.9428 | Val_Acc: 0.2188 -> Best model updated\n",
      "Epoch [2/3] | Train_Loss: 1.9425 | Train_Acc: 0.0938 | Val_Loss: 1.9319 | Val_Acc: 0.1875\n",
      "Epoch [3/3] | Train_Loss: 1.9401 | Train_Acc: 0.1250 | Val_Loss: 1.9229 | Val_Acc: 0.2500 -> Best model updated\n",
      "Training completed. Best validation accuracy: 0.2500. Running steps training time: 9.72 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250505-165152_MambaVision-T-1K>\n",
      "==========================\n",
      "Test Loss: 1.9338 | Test Acc: 0.1562 | Running steps test time: 0.23 s.\n"
     ]
    }
   ],
   "source": [
    "# Define the num_epoch and LR\n",
    "num_epochs = 3\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da230c",
   "metadata": {},
   "source": [
    "## **MAMBA B 21 K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6d6291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = \"nvidia/MambaVision-B-21K\"\n",
    "\n",
    "# Define the model with the MambaClassifier class\n",
    "model = MambaClassifier(\n",
    "    AutoModel.from_pretrained(model_card, trust_remote_code=True),\n",
    "    num_classes=N_CLASSES,\n",
    "    hidden_dim=MAMBA_HIDDEN_SIZES.get(model_card),\n",
    ")\n",
    "\n",
    "# Define the image transform (processor and transform)\n",
    "train_transform = model.create_transform(training=True)\n",
    "test_transform = model.create_transform(training=False)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    train_transform=train_transform,\n",
    "    test_transform=test_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe35508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250502-194242_MambaVision-B-21K>\n",
      "==========================\n",
      "Epoch [1/3] | Train_Loss: 2.0714 | Train_Acc: 0.0312 | Val_Loss: 2.1670 | Val_Acc: 0.0312 -> Best model updated\n",
      "Epoch [2/3] | Train_Loss: 1.9613 | Train_Acc: 0.2500 | Val_Loss: 2.1341 | Val_Acc: 0.0625 -> Best model updated\n",
      "Epoch [3/3] | Train_Loss: 1.9565 | Train_Acc: 0.1562 | Val_Loss: 2.1045 | Val_Acc: 0.0938 -> Best model updated\n",
      "Training completed. Best validation accuracy: 0.0938. Running steps training time: 7.05 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250502-194242_MambaVision-B-21K>\n",
      "==========================\n",
      "Test Loss: 2.0276 | Test Acc: 0.1875 | Running steps test time: 0.59 s.\n"
     ]
    }
   ],
   "source": [
    "# Define the num_epoch and LR\n",
    "num_epochs = 3\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
