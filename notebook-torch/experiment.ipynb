{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc430ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL\n",
    "# TODO : Compléter toutes les parties en MARKDOWN\n",
    "# TODO : Update le README.md avec les instructions pour regarder tensorboard\n",
    "\n",
    "# EXPERIMENTS\n",
    "# TODO : Tester le batch size de 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b170e",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "#TODO Ce notebook présente une analyse de données sur les ventes de jeux vidéo. L'objectif est d'explorer les tendances des ventes en fonction de la plateforme, du genre et de la région."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccda75",
   "metadata": {},
   "source": [
    "# PRE-REQUIS\n",
    "\n",
    "Ce bloc contient tout ce qui est nécessaire pour le fonctionnement des expériences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e97c4",
   "metadata": {},
   "source": [
    "## Imports & Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9aa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from pathlib import PosixPath\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor, AutoModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from constants import ROOT_FOLDER, IMAGE_FOLDER, ARTIFACTS_FOLDER, DATASET_PATH\n",
    "from constants import SEED, VAL_SIZE, TEST_SIZE, BATCH_SIZE, SAMPLING, INPUT_RESOLUTION\n",
    "from constants import MAMBA_HIDDEN_SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad8ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion des avertissements\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651be0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  2.6.0+cu124\n",
      "Cuda version:  12.4\n",
      "CUDNN version:  90100\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration de cuda avec PyTorch\n",
    "print(\"Torch version: \", torch.__version__)\n",
    "print(\"Cuda version: \", torch.version.cuda)\n",
    "print(\"CUDNN version: \", torch.backends.cudnn.version())\n",
    "\n",
    "# Set the device to GPU if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116edde",
   "metadata": {},
   "source": [
    "## Classes et Fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eeffe0",
   "metadata": {},
   "source": [
    "### Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c31724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, train_path, val_path, test_path):\n",
    "    # Splitting the datasets into train, val and test sets\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        df[\"image\"],\n",
    "        df[\"class\"],\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=SEED,\n",
    "        stratify=df[\"class\"],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=VAL_SIZE,\n",
    "        random_state=SEED,\n",
    "        stratify=y_temp,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Concat X and y for each set\n",
    "    train = (\n",
    "        pd.concat([X_train, y_train], axis=1).sample(SAMPLING)\n",
    "        if SAMPLING\n",
    "        else pd.concat([X_train, y_train], axis=1)\n",
    "    )\n",
    "    train.to_pickle(train_path)\n",
    "    val = (\n",
    "        pd.concat([X_val, y_val], axis=1).sample(SAMPLING)\n",
    "        if SAMPLING\n",
    "        else pd.concat([X_val, y_val], axis=1)\n",
    "    )\n",
    "    val.to_pickle(val_path)\n",
    "    test = (\n",
    "        pd.concat([X_test, y_test], axis=1).sample(SAMPLING)\n",
    "        if SAMPLING\n",
    "        else pd.concat([X_test, y_test], axis=1)\n",
    "    )\n",
    "    test.to_pickle(test_path)\n",
    "\n",
    "\n",
    "def load_splits(train_path, val_path, test_path):\n",
    "    # Load the saved files if they exist\n",
    "    try:\n",
    "        train = pd.read_pickle(train_path)\n",
    "        val = pd.read_pickle(val_path)\n",
    "        test = pd.read_pickle(test_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        print(\"This file has not been found. Please check the paths before.\")\n",
    "\n",
    "    # Print the shapes of the datasets\n",
    "    print(f\"Train shape: {train.shape}\")\n",
    "    print(f\"Val shape: {val.shape}\")\n",
    "    print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899dbab5",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "[ ] ***TODO*** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15f40f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block DataLoader\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe: pd.DataFrame,\n",
    "        image_dir: PosixPath,\n",
    "        processor: AutoImageProcessor = None,\n",
    "        transform: callable = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing image file names and labels.\n",
    "            image_dir (PosixPath): Directory where images are stored.\n",
    "            processor (AutoImageProcessor, optional): Hugging Face processor for image preprocessing. Defaults to None.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(\n",
    "            self.image_dir, self.dataframe.iloc[idx, 0]\n",
    "        )  # Assuming image file names are in the first column\n",
    "        image = Image.open(img_name).convert(\"RGB\")  # Ensure consistent color format\n",
    "\n",
    "        label = self.dataframe.iloc[idx, 1]  # Assuming labels are in the second column\n",
    "\n",
    "        if self.processor:\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            image = inputs[\"pixel_values\"].squeeze()  # Remove batch dimension\n",
    "        elif self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e1f426",
   "metadata": {},
   "source": [
    "### MambaClassifier\n",
    "\n",
    "[ ] ***TODO*** : L'ajout de la classe MambaClassifier permet de créer un classificateur basé sur le modèle MambaVision en tenant compte de ses spécificités. Cette classe hérite de la classe `nn.Module` de PyTorch et encapsule le modèle MambaVision, permettant ainsi de l'utiliser comme un classificateur dans le cadre d'une tâche de classification d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7646c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block MambaClassifier\n",
    "class MambaClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: AutoModel,\n",
    "        num_classes: int,\n",
    "        hidden_dim: int,  # The hidden dimension of the backbone is stored in the MAMBA_HIDDEN_SIZES dict with the model card as the key\n",
    "        fc_layer: int = None,  # Add the int number of layers before the classifier\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.config = self.backbone.config\n",
    "        self.fc_layer = fc_layer\n",
    "        if fc_layer:\n",
    "            self.fc_layers = nn.ModuleList()\n",
    "            for i in range(fc_layer):\n",
    "                if i == 0:\n",
    "                    self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                else:\n",
    "                    self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                self.fc_layers.append(nn.ReLU())\n",
    "                self.fc_layers.append(nn.Dropout(0.1))\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def create_transform(self, training: bool, auto_augment=None):\n",
    "        transform = create_transform(\n",
    "            input_size=INPUT_RESOLUTION,\n",
    "            is_training=training,  # Add a ColorJitter augmentation during training\n",
    "            mean=self.config.mean,\n",
    "            std=self.config.std,\n",
    "            crop_mode=self.config.crop_mode,\n",
    "            crop_pct=self.config.crop_pct,\n",
    "            auto_augment=auto_augment,  # \"rand-m9-mstd0.5-inc1\"\n",
    "        )\n",
    "        return transform\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_avg_pool, _ = self.backbone(x)\n",
    "        if hasattr(self, \"fc_layers\"):\n",
    "            for layer in self.fc_layers:\n",
    "                out_avg_pool = layer(out_avg_pool)\n",
    "        logits = self.classifier(out_avg_pool)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603114e3",
   "metadata": {},
   "source": [
    "### TorchPipeline\n",
    "\n",
    "[ ] ***TODO*** : Un pipeline de traitement des données est créé pour gérer les transformations d'images et les normalisations nécessaires avant de passer les données au modèle. Ce pipeline utilise la bibliothèque `torchvision` pour appliquer des transformations telles que le redimensionnement, le recadrage, la conversion en tenseur et la normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb085c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_card,\n",
    "        model,\n",
    "        processor=None,\n",
    "        train_transform=None,\n",
    "        test_transform=None,\n",
    "    ):\n",
    "        # Initialize attributes from parameters\n",
    "        self.model_card = model_card\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.train_transform = train_transform\n",
    "        self.test_transform = test_transform\n",
    "\n",
    "        # Check if the model is a MambaClassifier instance\n",
    "        self.mamba = \"MambaClassifier\" in type(model).__name__\n",
    "\n",
    "        # Initialize FIXED attributes from constants.py\n",
    "        self.device = DEVICE\n",
    "        self.root_folder = ROOT_FOLDER\n",
    "        self.artifacts_folder = ARTIFACTS_FOLDER\n",
    "        self.dataset_path = DATASET_PATH\n",
    "        self.image_folder = IMAGE_FOLDER\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.classes = CLASSES\n",
    "        self.n_classes = N_CLASSES\n",
    "\n",
    "        # Initialize empty attributes\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.test_loader = None\n",
    "        self.writer = None\n",
    "        self.experiment_id = None\n",
    "\n",
    "    @staticmethod\n",
    "    def split_labels_on_and_or_ampersand(labels):\n",
    "        \"\"\"\n",
    "        Insert a newline after each '&' ou 'and' in label names.\n",
    "        \"\"\"\n",
    "        return [re.sub(r'\\s*(and|&)\\s*', r'\\n\\1 ', label) for label in labels]\n",
    "\n",
    "\n",
    "    def generate_experiment_id(self, freeze_backbone):\n",
    "        \"\"\"\n",
    "        Generate a unique experiment ID based on the current date and time.\n",
    "        \"\"\"\n",
    "        freeze_str = \"freezed\" if freeze_backbone else \"unfreezed\"\n",
    "        return \"_\".join(\n",
    "            [datetime.now().strftime(\"%Y%m%d-%H%M%S\"), self.model_card.split(\"/\")[-1], freeze_str]\n",
    "        )\n",
    "\n",
    "    def freeze_backbone(self, freeze_backbone):\n",
    "        \"\"\"\n",
    "        Freeze the backbone of the model if specified.\n",
    "        \"\"\"\n",
    "        if freeze_backbone:\n",
    "            # Freeze the backbone parameters\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            # Unfreeze the classifier parameters\n",
    "            for param in self.model.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "            # Unfreeze any additional layers if model has fc_layers\n",
    "            if hasattr(model, \"fc_layers\"):\n",
    "                for layer in self.model.fc_layers.parameters():\n",
    "                    layer.requires_grad = True\n",
    "\n",
    "    # Load the data, apply the processor and transform, and create the dataloaders\n",
    "    def generate_dataloader(self, train, val, test):\n",
    "        # Apply the processor and transform\n",
    "        train_dataset = ImageDataset(\n",
    "            dataframe=train,\n",
    "            image_dir=self.image_folder,\n",
    "            processor=self.processor,\n",
    "            transform=self.train_transform,\n",
    "        )\n",
    "        val_dataset = ImageDataset(\n",
    "            dataframe=val,\n",
    "            image_dir=self.image_folder,\n",
    "            processor=self.processor,\n",
    "            transform=self.test_transform,\n",
    "        )\n",
    "        test_dataset = ImageDataset(\n",
    "            dataframe=test,\n",
    "            image_dir=self.image_folder,\n",
    "            processor=self.processor,\n",
    "            transform=self.test_transform,\n",
    "        )\n",
    "\n",
    "        # Create the dataloaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def train_and_eval_model(\n",
    "        self, criterion, optimizer, num_epochs, freeze_backbone=False\n",
    "    ):\n",
    "        # Freeze the backbone if specified\n",
    "        self.freeze_backbone(freeze_backbone)\n",
    "        \n",
    "        # Generate the experiment ID\n",
    "        self.experiment_id = self.generate_experiment_id(freeze_backbone)\n",
    "        # Create the writer\n",
    "        log_dir = (\n",
    "            ROOT_FOLDER\n",
    "            / \"runs\"\n",
    "            / self.experiment_id\n",
    "        )\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "\n",
    "        # Initialize the training metrics\n",
    "        running_train_time_by_step = 0.0\n",
    "        running_train_time_by_epoch = 0.0\n",
    "        running_train_steps = 0\n",
    "        self.writer.add_scalar(\"TimingByEpoch/train\", running_train_time_by_epoch, 0)\n",
    "\n",
    "        # Initialize the validation metrics\n",
    "        best_val_metric = float(\"-inf\")\n",
    "        running_val_time_by_step = 0.0\n",
    "        running_val_time_by_epoch = 0.0\n",
    "        running_val_steps = 0\n",
    "        self.writer.add_scalar(\"TimingByEpoch/validation\", running_val_time_by_epoch, 0)\n",
    "\n",
    "        self.writer.add_scalar(\"TimingByEpoch/train\", running_train_time_by_epoch, 0)\n",
    "        # Move model to the device\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "        # Training loop\n",
    "        print(\"TRAINING EXPERIMENT ID <{}>\".format(self.experiment_id))\n",
    "        print(\"==========================\")\n",
    "        for epoch in range(num_epochs):\n",
    "            # --- 1. TRAINING LOOP ---\n",
    "            self.model.train()\n",
    "            train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "            epoch_time = time()\n",
    "            for i, (inputs, labels) in enumerate(\n",
    "                self.train_loader\n",
    "            ):  # Use enumerate for step count\n",
    "                step_time = time()\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                # Compute the model outputs given the mamba variable\n",
    "                outputs = (\n",
    "                    self.model(inputs).logits if not self.mamba else self.model(inputs)\n",
    "                )\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step_loss = loss.item() * inputs.size(0)\n",
    "                train_loss += step_loss\n",
    "                preds_train = outputs.argmax(dim=1)\n",
    "                correct_train += (preds_train == labels).sum().item()\n",
    "                total_train += inputs.size(0)\n",
    "                running_train_steps += 1\n",
    "                running_train_time_by_step += time() - step_time\n",
    "                self.writer.add_scalar(\n",
    "                    \"TimingByStep/train\",\n",
    "                    running_train_time_by_step,\n",
    "                    running_train_steps,\n",
    "                )\n",
    "                self.writer.add_scalar(\n",
    "                    \"LossByStep/train\", step_loss, running_train_steps\n",
    "                )\n",
    "\n",
    "            running_train_time_by_epoch += time() - epoch_time\n",
    "            epoch_train_loss = train_loss / total_train\n",
    "            epoch_train_acc = correct_train / total_train\n",
    "            # Log training metrics per epoch\n",
    "            self.writer.add_scalar(\"LossByEpoch/train\", epoch_train_loss, epoch)\n",
    "            self.writer.add_scalar(\"Accuracy/train\", epoch_train_acc, epoch)\n",
    "            self.writer.add_scalar(\n",
    "                \"TimingByEpoch/train\", running_train_time_by_epoch, epoch\n",
    "            )\n",
    "            stats = f\"Epoch [{epoch + 1}/{num_epochs}] | Train_Loss: {epoch_train_loss:.4f} | Train_Acc: {epoch_train_acc:.4f}\"\n",
    "\n",
    "            # --- 2. VALIDATION LOOP ---\n",
    "            self.model.eval()\n",
    "            val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "            epoch_time = time()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in self.val_loader:\n",
    "                    # Move images and labels to the device\n",
    "                    step_time = time()\n",
    "                    inputs = inputs.to(DEVICE)\n",
    "                    labels = labels.to(DEVICE)\n",
    "                    outputs = (\n",
    "                        self.model(inputs).logits\n",
    "                        if not self.mamba\n",
    "                        else self.model(inputs)\n",
    "                    )\n",
    "                    val_step_loss = criterion(outputs, labels).item() * inputs.size(0)\n",
    "                    val_loss += val_step_loss\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    correct_val += (preds == labels).sum().item()\n",
    "                    total_val += inputs.size(0)\n",
    "                    running_val_steps += 1\n",
    "                    running_val_time_by_step += time() - step_time\n",
    "                    self.writer.add_scalar(\n",
    "                        \"TimingByStep/validation\",\n",
    "                        running_val_time_by_step,\n",
    "                        running_val_steps,\n",
    "                    )\n",
    "                    self.writer.add_scalar(\n",
    "                        \"LossByStep/validation\", val_step_loss, running_train_steps\n",
    "                    )\n",
    "\n",
    "            running_val_time_by_epoch += time() - epoch_time\n",
    "            epoch_val_loss = val_loss / total_val\n",
    "            epoch_val_acc = correct_val / total_val\n",
    "            # Log validation metrics per epoch\n",
    "            self.writer.add_scalar(\"Loss/validation\", epoch_val_loss, epoch)\n",
    "            self.writer.add_scalar(\"Accuracy/validation\", epoch_val_acc, epoch)\n",
    "            self.writer.add_scalar(\n",
    "                \"TimingByEpoch/validation\", running_val_time_by_epoch, epoch\n",
    "            )\n",
    "            stats += f\" | Val_Loss: {epoch_val_loss:.4f} | Val_Acc: {epoch_val_acc:.4f}\"\n",
    "\n",
    "            # --- 3. UPDATE BEST MODEL ---\n",
    "            # Save the model if the validation accuracy is better than the best one\n",
    "            if epoch_val_acc > best_val_metric:\n",
    "                best_val_metric = epoch_val_acc\n",
    "                torch.save(\n",
    "                    self.model.state_dict(),\n",
    "                    ARTIFACTS_FOLDER / f\"{self.experiment_id}.pth\",\n",
    "                )\n",
    "                stats += \" -> Best model updated\"\n",
    "            # Print the stats at the end of each epoch\n",
    "            print(stats)\n",
    "\n",
    "        # Delete the model from GPU memory\n",
    "        del (\n",
    "            inputs,\n",
    "            labels,\n",
    "            outputs,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            loss,\n",
    "            val_loss,\n",
    "            step_loss,\n",
    "            val_step_loss,\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\n",
    "            f\"Training completed. Best validation accuracy: {best_val_metric:.4f}. Running steps training time: {running_train_time_by_epoch:.2f} s.\\n\"\n",
    "        )\n",
    "\n",
    "    def test_model(\n",
    "        self,\n",
    "        criterion,\n",
    "        with_id=None,\n",
    "    ):\n",
    "        # Case when no id has been provided\n",
    "        if with_id is None:\n",
    "            try:\n",
    "                self.model.load_state_dict(\n",
    "                    torch.load(ARTIFACTS_FOLDER / f\"{self.experiment_id}.pth\")\n",
    "                )\n",
    "            except FileNotFoundError:\n",
    "                print(\n",
    "                    \"No id has been provided and no model has been trained yet. Train the model first before testing.\"\n",
    "                )\n",
    "                return\n",
    "        # Case when id has been provided\n",
    "        else:\n",
    "            try:\n",
    "                self.model.load_state_dict(\n",
    "                    torch.load(ARTIFACTS_FOLDER / f\"{with_id}.pth\")\n",
    "                )\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model {with_id}.pth not found in {ARTIFACTS_FOLDER}\")\n",
    "                print(\"Please provide a valid model ID.\")\n",
    "                return\n",
    "\n",
    "        # If no error is raised, the model is loaded successfully\n",
    "        self.model.to(DEVICE)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Initialize the test metrics\n",
    "        test_loss, correct_test, total_test = 0.0, 0, 0\n",
    "        running_test_steps = 0\n",
    "        running_test_time_by_step = 0.0\n",
    "        y_true, y_pred = [], []\n",
    "        \n",
    "        # Starting the test loop\n",
    "        print(f\"TESTING EXPERIMENT ID <{with_id if with_id else self.experiment_id}>\")\n",
    "        print(\"==========================\")\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.test_loader:\n",
    "                step_time = time()\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                outputs = (\n",
    "                    self.model(inputs).logits if not self.mamba else self.model(inputs)\n",
    "                )\n",
    "                test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct_test += (preds == labels).sum().item()\n",
    "                total_test += inputs.size(0)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "                running_test_time_by_step += time() - step_time\n",
    "                running_test_steps += 1\n",
    "                if self.writer:\n",
    "                    self.writer.add_scalar(\n",
    "                        \"TimingByStep/test\",\n",
    "                        running_test_time_by_step,\n",
    "                        running_test_steps,\n",
    "                    )\n",
    "\n",
    "        epoch_test_loss = test_loss / total_test\n",
    "        epoch_test_acc = correct_test / total_test\n",
    "        # Add the accuracy to the tensorboard\n",
    "        if self.writer:\n",
    "            self.writer.add_scalar(\"Accuracy/test\", epoch_test_acc, 0)\n",
    "        \n",
    "        # Print the test metrics\n",
    "        print(\n",
    "            f\"Test Loss: {epoch_test_loss:.4f} | Test Acc: {epoch_test_acc:.4f} | Running steps test time: {running_test_time_by_step:.2f} s.\"\n",
    "        )\n",
    "\n",
    "        # Create the Classification report\n",
    "        classifier_report = classification_report(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            target_names=self.classes,\n",
    "            zero_division=0,\n",
    "            output_dict=True,\n",
    "            )\n",
    "\n",
    "        # Split the labels on '&' or 'and' for better readability for CM\n",
    "        formatted_labels = self.split_labels_on_and_or_ampersand(self.classes)\n",
    "\n",
    "        # Create the confusion matrix\n",
    "        cm = ConfusionMatrixDisplay.from_predictions(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            labels=range(self.n_classes),\n",
    "            normalize=None,\n",
    "            display_labels=formatted_labels,\n",
    "            values_format=\".2g\",\n",
    "            xticks_rotation=\"vertical\",\n",
    "            colorbar=False,\n",
    "            cmap=plt.cm.Blues,\n",
    "        )\n",
    "        cm.figure_.tight_layout()\n",
    "        # Save the classification report as a markdown file in tensorboard\n",
    "        if self.writer:\n",
    "            for label, metrics in classifier_report.items():\n",
    "                if isinstance(metrics, dict):\n",
    "                    for metric, value in metrics.items():\n",
    "                        self.writer.add_scalar(\n",
    "                            f\"ClassificationReport/{label}/{metric}\", value, 0\n",
    "                        )\n",
    "                else:\n",
    "                    self.writer.add_scalar(f\"ClassificationReport/{label}\", metrics, 0)\n",
    "            self.writer.add_figure(\"ConfusionMatrix/test\", cm.figure_, 0)\n",
    "            self.writer.close()  # Close the writer after the end of the pipeline\n",
    "        # If no writer is provided, print the classification report & confusion matrix\n",
    "        else:\n",
    "            print(classifier_report)\n",
    "            cm.figure_.show()\n",
    "\n",
    "        # Delete the model from GPU memory\n",
    "        del self.model, inputs, labels, outputs\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "194d4d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2.0},\n",
       " 'second': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2.0},\n",
       " 'accuracy': 0.5,\n",
       " 'macro avg': {'precision': 0.5,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.5,\n",
       "  'support': 4.0},\n",
       " 'weighted avg': {'precision': 0.5,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.5,\n",
       "  'support': 4.0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(\n",
    "            [0, 1, 1, 0],\n",
    "            [0, 0, 1, 1],\n",
    "            target_names=[\"first\", \"second\"],\n",
    "            zero_division=0,\n",
    "            output_dict=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab3639",
   "metadata": {},
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64198e15",
   "metadata": {},
   "source": [
    "### Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37f68941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1050, 2)\n",
      "Dataset columns: Index(['image', 'class'], dtype='object')\n",
      "Number of classes: 7\n",
      "Classes: ['Baby Care', 'Beauty and Personal Care', 'Computers', 'Home Decor & Festive Needs', 'Home Furnishing', 'Kitchen & Dining', 'Watches']\n"
     ]
    }
   ],
   "source": [
    "# Loading the pickle dataset_cleaned used with the previous project as a pandas df\n",
    "df = pd.read_pickle(DATASET_PATH).drop(columns=[\"product_name\", \"description\"])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Dataset columns: {df.columns}\")\n",
    "\n",
    "# Encode the labels with LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df[\"class\"])\n",
    "N_CLASSES = len(le.classes_)\n",
    "CLASSES = le.classes_.tolist()\n",
    "print(f\"Number of classes: {N_CLASSES}\")\n",
    "print(f\"Classes: {CLASSES}\")\n",
    "\n",
    "# Finally transform the class column to the encoded labels\n",
    "df[\"class\"] = le.transform(df[\"class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a8b51",
   "metadata": {},
   "source": [
    "### Séparation des données (train/validation/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d9afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (758, 2)\n",
      "Val shape: (134, 2)\n",
      "Test shape: (158, 2)\n"
     ]
    }
   ],
   "source": [
    "# Define the path to save the splitted cleaned datasets\n",
    "completion = SAMPLING if SAMPLING else \"full\"\n",
    "train_path = ROOT_FOLDER / \"data\" / f\"trainset_{completion}.pickle\"\n",
    "val_path = ROOT_FOLDER / \"data\" / f\"valset_{completion}.pickle\"\n",
    "test_path = ROOT_FOLDER / \"data\" / f\"testset_{completion}.pickle\"\n",
    "\n",
    "# Load the splitted datasets if they exist\n",
    "if (\n",
    "    os.path.exists(train_path)\n",
    "    and os.path.exists(val_path)\n",
    "    and os.path.exists(test_path)\n",
    "):\n",
    "    train, val, test = load_splits(train_path, val_path, test_path)\n",
    "else:\n",
    "    # If the one or more files do not exist, split the dataset and save/overwrite the files\n",
    "    print(\"Splitting the dataset and saving it locally in artifacts/data/...\")\n",
    "    split_dataset(df, train_path, val_path, test_path)\n",
    "    train, val, test = load_splits(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08d4ee5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "53020cbd-b305-4eee-90de-c815f81504b9",
       "rows": [
        [
         "229",
         "caabe6014b914fe2874a9a8d7284f79b.jpg",
         "3"
        ],
        [
         "450",
         "95feec21a9d076cff084159d61bf9b8e.jpg",
         "0"
        ],
        [
         "798",
         "9993de7e2bcced43dc9edb3b2c81f23d.jpg",
         "1"
        ],
        [
         "230",
         "968a2b3be84193e3f755c2fe71033a2c.jpg",
         "3"
        ],
        [
         "293",
         "c2efa8aa11898bdb5fc4e46201973a42.jpg",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>caabe6014b914fe2874a9a8d7284f79b.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>95feec21a9d076cff084159d61bf9b8e.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>9993de7e2bcced43dc9edb3b2c81f23d.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>968a2b3be84193e3f755c2fe71033a2c.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>c2efa8aa11898bdb5fc4e46201973a42.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "229  caabe6014b914fe2874a9a8d7284f79b.jpg      3\n",
       "450  95feec21a9d076cff084159d61bf9b8e.jpg      0\n",
       "798  9993de7e2bcced43dc9edb3b2c81f23d.jpg      1\n",
       "230  968a2b3be84193e3f755c2fe71033a2c.jpg      3\n",
       "293  c2efa8aa11898bdb5fc4e46201973a42.jpg      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4eede8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "de39d4bd-a4f2-4122-97fc-1389073480e8",
       "rows": [
        [
         "979",
         "c44a5dc5b5ebe5b3e0535b7c2b7921e4.jpg",
         "0"
        ],
        [
         "49",
         "02a53d335775b652f22f41b529b9d646.jpg",
         "1"
        ],
        [
         "567",
         "97fba8a02361aa56eaa9fa51bc1d7661.jpg",
         "6"
        ],
        [
         "494",
         "a124d6e4c30b00918c594289266a383c.jpg",
         "6"
        ],
        [
         "773",
         "109e235d4838002246599f987d935c21.jpg",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>c44a5dc5b5ebe5b3e0535b7c2b7921e4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>02a53d335775b652f22f41b529b9d646.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>97fba8a02361aa56eaa9fa51bc1d7661.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>a124d6e4c30b00918c594289266a383c.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>109e235d4838002246599f987d935c21.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "979  c44a5dc5b5ebe5b3e0535b7c2b7921e4.jpg      0\n",
       "49   02a53d335775b652f22f41b529b9d646.jpg      1\n",
       "567  97fba8a02361aa56eaa9fa51bc1d7661.jpg      6\n",
       "494  a124d6e4c30b00918c594289266a383c.jpg      6\n",
       "773  109e235d4838002246599f987d935c21.jpg      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6ba2b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "9f938c03-8694-4462-9cac-d6afb7e81292",
       "rows": [
        [
         "11",
         "08452abdadb3db1e686b94a9c52fc7b6.jpg",
         "6"
        ],
        [
         "548",
         "2541b59d54a3a9f2681c0049f7ddd85c.jpg",
         "6"
        ],
        [
         "696",
         "82fbc93cd45ab747e7e606f2c52c7335.jpg",
         "3"
        ],
        [
         "238",
         "2e8df36b35d22cf219cf8bae6c2af752.jpg",
         "5"
        ],
        [
         "963",
         "bcb51cec3d290e6a661586d0df30e091.jpg",
         "4"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>08452abdadb3db1e686b94a9c52fc7b6.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>2541b59d54a3a9f2681c0049f7ddd85c.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>82fbc93cd45ab747e7e606f2c52c7335.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2e8df36b35d22cf219cf8bae6c2af752.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>bcb51cec3d290e6a661586d0df30e091.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "11   08452abdadb3db1e686b94a9c52fc7b6.jpg      6\n",
       "548  2541b59d54a3a9f2681c0049f7ddd85c.jpg      6\n",
       "696  82fbc93cd45ab747e7e606f2c52c7335.jpg      3\n",
       "238  2e8df36b35d22cf219cf8bae6c2af752.jpg      5\n",
       "963  bcb51cec3d290e6a661586d0df30e091.jpg      4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c3a8c",
   "metadata": {},
   "source": [
    "# EXPERIMENTS IN_1K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea071149",
   "metadata": {},
   "source": [
    "## MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38b9c557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileNetV2ForImageClassification were not initialized from the model checkpoint at google/mobilenet_v2_1.0_224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1001]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1001, 1280]) in the checkpoint and torch.Size([7, 1280]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250506-135332_mobilenet_v2_1.0_224_unfreezed>\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train_Loss: 1.9202 | Train_Acc: 0.2665 | Val_Loss: 1.7999 | Val_Acc: 0.3209 -> Best model updated\n",
      "Epoch [2/25] | Train_Loss: 1.6296 | Train_Acc: 0.4367 | Val_Loss: 1.5680 | Val_Acc: 0.4478 -> Best model updated\n",
      "Epoch [3/25] | Train_Loss: 1.4741 | Train_Acc: 0.5343 | Val_Loss: 1.4983 | Val_Acc: 0.5224 -> Best model updated\n",
      "Epoch [4/25] | Train_Loss: 1.2699 | Train_Acc: 0.6412 | Val_Loss: 1.3640 | Val_Acc: 0.5672 -> Best model updated\n",
      "Epoch [5/25] | Train_Loss: 1.1674 | Train_Acc: 0.6755 | Val_Loss: 1.2039 | Val_Acc: 0.6045 -> Best model updated\n",
      "Epoch [6/25] | Train_Loss: 1.0351 | Train_Acc: 0.7361 | Val_Loss: 1.2482 | Val_Acc: 0.5000\n",
      "Epoch [7/25] | Train_Loss: 0.9518 | Train_Acc: 0.7269 | Val_Loss: 0.9866 | Val_Acc: 0.6866 -> Best model updated\n",
      "Epoch [8/25] | Train_Loss: 0.8765 | Train_Acc: 0.7625 | Val_Loss: 1.0129 | Val_Acc: 0.6940 -> Best model updated\n",
      "Epoch [9/25] | Train_Loss: 0.8263 | Train_Acc: 0.7810 | Val_Loss: 0.9713 | Val_Acc: 0.6866\n",
      "Epoch [10/25] | Train_Loss: 0.7614 | Train_Acc: 0.7876 | Val_Loss: 0.8421 | Val_Acc: 0.7090 -> Best model updated\n",
      "Epoch [11/25] | Train_Loss: 0.7170 | Train_Acc: 0.7916 | Val_Loss: 0.9913 | Val_Acc: 0.6716\n",
      "Epoch [12/25] | Train_Loss: 0.6698 | Train_Acc: 0.8127 | Val_Loss: 0.9433 | Val_Acc: 0.6642\n",
      "Epoch [13/25] | Train_Loss: 0.6505 | Train_Acc: 0.8272 | Val_Loss: 0.8606 | Val_Acc: 0.7015\n",
      "Epoch [14/25] | Train_Loss: 0.5915 | Train_Acc: 0.8351 | Val_Loss: 0.9510 | Val_Acc: 0.6866\n",
      "Epoch [15/25] | Train_Loss: 0.5426 | Train_Acc: 0.8404 | Val_Loss: 0.7411 | Val_Acc: 0.7313 -> Best model updated\n",
      "Epoch [16/25] | Train_Loss: 0.5367 | Train_Acc: 0.8562 | Val_Loss: 0.6905 | Val_Acc: 0.7612 -> Best model updated\n",
      "Epoch [17/25] | Train_Loss: 0.5411 | Train_Acc: 0.8443 | Val_Loss: 0.8582 | Val_Acc: 0.6791\n",
      "Epoch [18/25] | Train_Loss: 0.5380 | Train_Acc: 0.8377 | Val_Loss: 0.7094 | Val_Acc: 0.7537\n",
      "Epoch [19/25] | Train_Loss: 0.4467 | Train_Acc: 0.8681 | Val_Loss: 0.9033 | Val_Acc: 0.6866\n",
      "Epoch [20/25] | Train_Loss: 0.4550 | Train_Acc: 0.8839 | Val_Loss: 0.7941 | Val_Acc: 0.7313\n",
      "Epoch [21/25] | Train_Loss: 0.4011 | Train_Acc: 0.8879 | Val_Loss: 0.6723 | Val_Acc: 0.7612\n",
      "Epoch [22/25] | Train_Loss: 0.3658 | Train_Acc: 0.9142 | Val_Loss: 0.8512 | Val_Acc: 0.7164\n",
      "Epoch [23/25] | Train_Loss: 0.3820 | Train_Acc: 0.8905 | Val_Loss: 0.7502 | Val_Acc: 0.7463\n",
      "Epoch [24/25] | Train_Loss: 0.3500 | Train_Acc: 0.8984 | Val_Loss: 0.7224 | Val_Acc: 0.7687 -> Best model updated\n",
      "Epoch [25/25] | Train_Loss: 0.3232 | Train_Acc: 0.9222 | Val_Loss: 0.8981 | Val_Acc: 0.6791\n",
      "Training completed. Best validation accuracy: 0.7687. Running steps training time: 647.41 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250506-135332_mobilenet_v2_1.0_224_unfreezed>\n",
      "==========================\n",
      "Test Loss: 0.9389 | Test Acc: 0.7152 | Running steps test time: 1.06 s.\n"
     ]
    }
   ],
   "source": [
    "model_card = \"google/mobilenet_v2_1.0_224\"\n",
    "\n",
    "# Define the num_epochs and learning rate\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-5\n",
    "freeze_backbone = False\n",
    "\n",
    "# Define the model\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_card, num_labels=N_CLASSES, trust_remote_code=True, ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Define the image processor\n",
    "processor = AutoImageProcessor.from_pretrained(model_card)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    ")\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    freeze_backbone=freeze_backbone,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e1e4798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileNetV2ForImageClassification were not initialized from the model checkpoint at google/mobilenet_v2_1.0_224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1001]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1001, 1280]) in the checkpoint and torch.Size([7, 1280]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250506-140732_mobilenet_v2_1.0_224_freezed>\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train_Loss: 1.9851 | Train_Acc: 0.1979 | Val_Loss: 1.9905 | Val_Acc: 0.1940 -> Best model updated\n",
      "Epoch [2/25] | Train_Loss: 1.9607 | Train_Acc: 0.1821 | Val_Loss: 2.0150 | Val_Acc: 0.1418\n",
      "Epoch [3/25] | Train_Loss: 1.9517 | Train_Acc: 0.1939 | Val_Loss: 1.9377 | Val_Acc: 0.2239 -> Best model updated\n",
      "Epoch [4/25] | Train_Loss: 1.9219 | Train_Acc: 0.2150 | Val_Loss: 1.9573 | Val_Acc: 0.1866\n",
      "Epoch [5/25] | Train_Loss: 1.8723 | Train_Acc: 0.2335 | Val_Loss: 1.8884 | Val_Acc: 0.2463 -> Best model updated\n",
      "Epoch [6/25] | Train_Loss: 1.8575 | Train_Acc: 0.2718 | Val_Loss: 1.8545 | Val_Acc: 0.2687 -> Best model updated\n",
      "Epoch [7/25] | Train_Loss: 1.8505 | Train_Acc: 0.2744 | Val_Loss: 1.8421 | Val_Acc: 0.2313\n",
      "Epoch [8/25] | Train_Loss: 1.8108 | Train_Acc: 0.2770 | Val_Loss: 1.7974 | Val_Acc: 0.3507 -> Best model updated\n",
      "Epoch [9/25] | Train_Loss: 1.7781 | Train_Acc: 0.3417 | Val_Loss: 1.8176 | Val_Acc: 0.3134\n",
      "Epoch [10/25] | Train_Loss: 1.7865 | Train_Acc: 0.3074 | Val_Loss: 1.8397 | Val_Acc: 0.2537\n",
      "Epoch [11/25] | Train_Loss: 1.7495 | Train_Acc: 0.3668 | Val_Loss: 1.7879 | Val_Acc: 0.3209\n",
      "Epoch [12/25] | Train_Loss: 1.7548 | Train_Acc: 0.3562 | Val_Loss: 1.8522 | Val_Acc: 0.3284\n",
      "Epoch [13/25] | Train_Loss: 1.7108 | Train_Acc: 0.3984 | Val_Loss: 1.7542 | Val_Acc: 0.3806 -> Best model updated\n",
      "Epoch [14/25] | Train_Loss: 1.7048 | Train_Acc: 0.3786 | Val_Loss: 1.7212 | Val_Acc: 0.3582\n",
      "Epoch [15/25] | Train_Loss: 1.6801 | Train_Acc: 0.3892 | Val_Loss: 1.8132 | Val_Acc: 0.2761\n",
      "Epoch [16/25] | Train_Loss: 1.6611 | Train_Acc: 0.4156 | Val_Loss: 1.7603 | Val_Acc: 0.3060\n",
      "Epoch [17/25] | Train_Loss: 1.6503 | Train_Acc: 0.4433 | Val_Loss: 1.6629 | Val_Acc: 0.4104 -> Best model updated\n",
      "Epoch [18/25] | Train_Loss: 1.6412 | Train_Acc: 0.4485 | Val_Loss: 1.7397 | Val_Acc: 0.3582\n",
      "Epoch [19/25] | Train_Loss: 1.6313 | Train_Acc: 0.4433 | Val_Loss: 1.6843 | Val_Acc: 0.3955\n",
      "Epoch [20/25] | Train_Loss: 1.5919 | Train_Acc: 0.4842 | Val_Loss: 1.6617 | Val_Acc: 0.4030\n",
      "Epoch [21/25] | Train_Loss: 1.5796 | Train_Acc: 0.4908 | Val_Loss: 1.6485 | Val_Acc: 0.4478 -> Best model updated\n",
      "Epoch [22/25] | Train_Loss: 1.5809 | Train_Acc: 0.4881 | Val_Loss: 1.5674 | Val_Acc: 0.4403\n",
      "Epoch [23/25] | Train_Loss: 1.5374 | Train_Acc: 0.5132 | Val_Loss: 1.6221 | Val_Acc: 0.4104\n",
      "Epoch [24/25] | Train_Loss: 1.5657 | Train_Acc: 0.4868 | Val_Loss: 1.6179 | Val_Acc: 0.4627 -> Best model updated\n",
      "Epoch [25/25] | Train_Loss: 1.5249 | Train_Acc: 0.5211 | Val_Loss: 1.5559 | Val_Acc: 0.4851 -> Best model updated\n",
      "Training completed. Best validation accuracy: 0.4851. Running steps training time: 588.10 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250506-140732_mobilenet_v2_1.0_224_freezed>\n",
      "==========================\n",
      "Test Loss: 1.5196 | Test Acc: 0.5127 | Running steps test time: 1.09 s.\n"
     ]
    }
   ],
   "source": [
    "model_card = \"google/mobilenet_v2_1.0_224\"\n",
    "\n",
    "# Define the num_epochs and learning rate\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-5\n",
    "freeze_backbone = True\n",
    "\n",
    "# Define the model\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_card, num_labels=N_CLASSES, trust_remote_code=True, ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Define the image processor\n",
    "processor = AutoImageProcessor.from_pretrained(model_card)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    ")\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    freeze_backbone=freeze_backbone,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99fae14",
   "metadata": {},
   "source": [
    "## Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efa48683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([7, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250506-143320_swin-base-patch4-window7-224_unfreezed>\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train_Loss: 1.3391 | Train_Acc: 0.6042 | Val_Loss: 0.6251 | Val_Acc: 0.8582 -> Best model updated\n",
      "Epoch [2/25] | Train_Loss: 0.3330 | Train_Acc: 0.9327 | Val_Loss: 0.3113 | Val_Acc: 0.9179 -> Best model updated\n",
      "Epoch [3/25] | Train_Loss: 0.1171 | Train_Acc: 0.9776 | Val_Loss: 0.2609 | Val_Acc: 0.9254 -> Best model updated\n",
      "Epoch [4/25] | Train_Loss: 0.0492 | Train_Acc: 0.9921 | Val_Loss: 0.2588 | Val_Acc: 0.9403 -> Best model updated\n",
      "Epoch [5/25] | Train_Loss: 0.0250 | Train_Acc: 0.9987 | Val_Loss: 0.2441 | Val_Acc: 0.9030\n",
      "Epoch [6/25] | Train_Loss: 0.0141 | Train_Acc: 1.0000 | Val_Loss: 0.2406 | Val_Acc: 0.9179\n",
      "Epoch [7/25] | Train_Loss: 0.0101 | Train_Acc: 1.0000 | Val_Loss: 0.2398 | Val_Acc: 0.9179\n",
      "Epoch [8/25] | Train_Loss: 0.0064 | Train_Acc: 1.0000 | Val_Loss: 0.2411 | Val_Acc: 0.9179\n",
      "Epoch [9/25] | Train_Loss: 0.0048 | Train_Acc: 1.0000 | Val_Loss: 0.2540 | Val_Acc: 0.9179\n",
      "Epoch [10/25] | Train_Loss: 0.0048 | Train_Acc: 1.0000 | Val_Loss: 0.2489 | Val_Acc: 0.9179\n",
      "Epoch [11/25] | Train_Loss: 0.0056 | Train_Acc: 0.9987 | Val_Loss: 0.2617 | Val_Acc: 0.9254\n",
      "Epoch [12/25] | Train_Loss: 0.0035 | Train_Acc: 1.0000 | Val_Loss: 0.2656 | Val_Acc: 0.9030\n",
      "Epoch [13/25] | Train_Loss: 0.0034 | Train_Acc: 1.0000 | Val_Loss: 0.2728 | Val_Acc: 0.9104\n",
      "Epoch [14/25] | Train_Loss: 0.0017 | Train_Acc: 1.0000 | Val_Loss: 0.2616 | Val_Acc: 0.9179\n",
      "Epoch [15/25] | Train_Loss: 0.0057 | Train_Acc: 0.9987 | Val_Loss: 0.2768 | Val_Acc: 0.9104\n",
      "Epoch [16/25] | Train_Loss: 0.0029 | Train_Acc: 1.0000 | Val_Loss: 0.2805 | Val_Acc: 0.9179\n",
      "Epoch [17/25] | Train_Loss: 0.0024 | Train_Acc: 1.0000 | Val_Loss: 0.2993 | Val_Acc: 0.9104\n",
      "Epoch [18/25] | Train_Loss: 0.0009 | Train_Acc: 1.0000 | Val_Loss: 0.2898 | Val_Acc: 0.9254\n",
      "Epoch [19/25] | Train_Loss: 0.0015 | Train_Acc: 1.0000 | Val_Loss: 0.2889 | Val_Acc: 0.9254\n",
      "Epoch [20/25] | Train_Loss: 0.0013 | Train_Acc: 1.0000 | Val_Loss: 0.2989 | Val_Acc: 0.9179\n",
      "Epoch [21/25] | Train_Loss: 0.0013 | Train_Acc: 1.0000 | Val_Loss: 0.2888 | Val_Acc: 0.9179\n",
      "Epoch [22/25] | Train_Loss: 0.0013 | Train_Acc: 1.0000 | Val_Loss: 0.2976 | Val_Acc: 0.9254\n",
      "Epoch [23/25] | Train_Loss: 0.0004 | Train_Acc: 1.0000 | Val_Loss: 0.2983 | Val_Acc: 0.9254\n",
      "Epoch [24/25] | Train_Loss: 0.0008 | Train_Acc: 1.0000 | Val_Loss: 0.3002 | Val_Acc: 0.9254\n",
      "Epoch [25/25] | Train_Loss: 0.0007 | Train_Acc: 1.0000 | Val_Loss: 0.3030 | Val_Acc: 0.9328\n",
      "Training completed. Best validation accuracy: 0.9403. Running steps training time: 1962.75 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250506-143320_swin-base-patch4-window7-224_unfreezed>\n",
      "==========================\n",
      "Test Loss: 0.3956 | Test Acc: 0.8544 | Running steps test time: 4.29 s.\n"
     ]
    }
   ],
   "source": [
    "model_card = \"microsoft/swin-base-patch4-window7-224\"\n",
    "\n",
    "# Define the num_epochs and learning rate\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-5\n",
    "freeze_backbone = False\n",
    "\n",
    "# Define the model\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_card, num_labels=N_CLASSES, trust_remote_code=True, ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Define the image processor\n",
    "processor = AutoImageProcessor.from_pretrained(model_card)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    ")\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    freeze_backbone=freeze_backbone,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b422dcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([7, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250506-151148_swin-base-patch4-window7-224_freezed>\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train_Loss: 1.9610 | Train_Acc: 0.1794 | Val_Loss: 1.9434 | Val_Acc: 0.2015 -> Best model updated\n",
      "Epoch [2/25] | Train_Loss: 1.8863 | Train_Acc: 0.2546 | Val_Loss: 1.8687 | Val_Acc: 0.2836 -> Best model updated\n",
      "Epoch [3/25] | Train_Loss: 1.8187 | Train_Acc: 0.3338 | Val_Loss: 1.7965 | Val_Acc: 0.3507 -> Best model updated\n",
      "Epoch [4/25] | Train_Loss: 1.7468 | Train_Acc: 0.4129 | Val_Loss: 1.7272 | Val_Acc: 0.4030 -> Best model updated\n",
      "Epoch [5/25] | Train_Loss: 1.6795 | Train_Acc: 0.4868 | Val_Loss: 1.6612 | Val_Acc: 0.4776 -> Best model updated\n",
      "Epoch [6/25] | Train_Loss: 1.6141 | Train_Acc: 0.5330 | Val_Loss: 1.5974 | Val_Acc: 0.5448 -> Best model updated\n",
      "Epoch [7/25] | Train_Loss: 1.5532 | Train_Acc: 0.5805 | Val_Loss: 1.5356 | Val_Acc: 0.5970 -> Best model updated\n",
      "Epoch [8/25] | Train_Loss: 1.4916 | Train_Acc: 0.6306 | Val_Loss: 1.4773 | Val_Acc: 0.6567 -> Best model updated\n",
      "Epoch [9/25] | Train_Loss: 1.4362 | Train_Acc: 0.6702 | Val_Loss: 1.4212 | Val_Acc: 0.6940 -> Best model updated\n",
      "Epoch [10/25] | Train_Loss: 1.3858 | Train_Acc: 0.7032 | Val_Loss: 1.3681 | Val_Acc: 0.7463 -> Best model updated\n",
      "Epoch [11/25] | Train_Loss: 1.3338 | Train_Acc: 0.7388 | Val_Loss: 1.3176 | Val_Acc: 0.7761 -> Best model updated\n",
      "Epoch [12/25] | Train_Loss: 1.2876 | Train_Acc: 0.7480 | Val_Loss: 1.2695 | Val_Acc: 0.7910 -> Best model updated\n",
      "Epoch [13/25] | Train_Loss: 1.2387 | Train_Acc: 0.7691 | Val_Loss: 1.2240 | Val_Acc: 0.8060 -> Best model updated\n",
      "Epoch [14/25] | Train_Loss: 1.1986 | Train_Acc: 0.7850 | Val_Loss: 1.1807 | Val_Acc: 0.8209 -> Best model updated\n",
      "Epoch [15/25] | Train_Loss: 1.1516 | Train_Acc: 0.8074 | Val_Loss: 1.1399 | Val_Acc: 0.8209\n",
      "Epoch [16/25] | Train_Loss: 1.1180 | Train_Acc: 0.8127 | Val_Loss: 1.1009 | Val_Acc: 0.8209\n",
      "Epoch [17/25] | Train_Loss: 1.0779 | Train_Acc: 0.8245 | Val_Loss: 1.0644 | Val_Acc: 0.8284 -> Best model updated\n",
      "Epoch [18/25] | Train_Loss: 1.0412 | Train_Acc: 0.8311 | Val_Loss: 1.0295 | Val_Acc: 0.8358 -> Best model updated\n",
      "Epoch [19/25] | Train_Loss: 1.0157 | Train_Acc: 0.8338 | Val_Loss: 0.9969 | Val_Acc: 0.8731 -> Best model updated\n",
      "Epoch [20/25] | Train_Loss: 0.9739 | Train_Acc: 0.8404 | Val_Loss: 0.9656 | Val_Acc: 0.8731\n",
      "Epoch [21/25] | Train_Loss: 0.9475 | Train_Acc: 0.8496 | Val_Loss: 0.9361 | Val_Acc: 0.8731\n",
      "Epoch [22/25] | Train_Loss: 0.9212 | Train_Acc: 0.8483 | Val_Loss: 0.9082 | Val_Acc: 0.8806 -> Best model updated\n",
      "Epoch [23/25] | Train_Loss: 0.8905 | Train_Acc: 0.8615 | Val_Loss: 0.8819 | Val_Acc: 0.8806\n",
      "Epoch [24/25] | Train_Loss: 0.8660 | Train_Acc: 0.8654 | Val_Loss: 0.8569 | Val_Acc: 0.8806\n",
      "Epoch [25/25] | Train_Loss: 0.8400 | Train_Acc: 0.8602 | Val_Loss: 0.8332 | Val_Acc: 0.8806\n",
      "Training completed. Best validation accuracy: 0.8806. Running steps training time: 1095.07 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250506-151148_swin-base-patch4-window7-224_freezed>\n",
      "==========================\n",
      "Test Loss: 1.0329 | Test Acc: 0.7532 | Running steps test time: 4.26 s.\n"
     ]
    }
   ],
   "source": [
    "model_card = \"microsoft/swin-base-patch4-window7-224\"\n",
    "\n",
    "# Define the num_epochs and learning rate\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-5\n",
    "freeze_backbone = True\n",
    "\n",
    "# Define the model\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_card, num_labels=N_CLASSES, trust_remote_code=True, ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Define the image processor\n",
    "processor = AutoImageProcessor.from_pretrained(model_card)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    ")\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    freeze_backbone=freeze_backbone,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57ab6e",
   "metadata": {},
   "source": [
    "## MobileViTV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c48cee93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileViTV2ForImageClassification were not initialized from the model checkpoint at apple/mobilevitv2-1.0-imagenet1k-256 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([7, 512]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250506-154003_mobilevitv2-1.0-imagenet1k-256_unfreezed>\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train_Loss: 1.9565 | Train_Acc: 0.1280 | Val_Loss: 1.9601 | Val_Acc: 0.0896 -> Best model updated\n",
      "Epoch [2/25] | Train_Loss: 1.9215 | Train_Acc: 0.2045 | Val_Loss: 1.9279 | Val_Acc: 0.1493 -> Best model updated\n",
      "Epoch [3/25] | Train_Loss: 1.8815 | Train_Acc: 0.3206 | Val_Loss: 1.8951 | Val_Acc: 0.2313 -> Best model updated\n",
      "Epoch [4/25] | Train_Loss: 1.8499 | Train_Acc: 0.4129 | Val_Loss: 1.8580 | Val_Acc: 0.3582 -> Best model updated\n",
      "Epoch [5/25] | Train_Loss: 1.8217 | Train_Acc: 0.4683 | Val_Loss: 1.8229 | Val_Acc: 0.4030 -> Best model updated\n",
      "Epoch [6/25] | Train_Loss: 1.7787 | Train_Acc: 0.5515 | Val_Loss: 1.7835 | Val_Acc: 0.5299 -> Best model updated\n",
      "Epoch [7/25] | Train_Loss: 1.7336 | Train_Acc: 0.6029 | Val_Loss: 1.7379 | Val_Acc: 0.5821 -> Best model updated\n",
      "Epoch [8/25] | Train_Loss: 1.6886 | Train_Acc: 0.6464 | Val_Loss: 1.6778 | Val_Acc: 0.6045 -> Best model updated\n",
      "Epoch [9/25] | Train_Loss: 1.6325 | Train_Acc: 0.6741 | Val_Loss: 1.6121 | Val_Acc: 0.6567 -> Best model updated\n",
      "Epoch [10/25] | Train_Loss: 1.5717 | Train_Acc: 0.7005 | Val_Loss: 1.5622 | Val_Acc: 0.6343\n",
      "Epoch [11/25] | Train_Loss: 1.5082 | Train_Acc: 0.7216 | Val_Loss: 1.4817 | Val_Acc: 0.6791 -> Best model updated\n",
      "Epoch [12/25] | Train_Loss: 1.4308 | Train_Acc: 0.7203 | Val_Loss: 1.4056 | Val_Acc: 0.7388 -> Best model updated\n",
      "Epoch [13/25] | Train_Loss: 1.3721 | Train_Acc: 0.7216 | Val_Loss: 1.3337 | Val_Acc: 0.7090\n",
      "Epoch [14/25] | Train_Loss: 1.3153 | Train_Acc: 0.7243 | Val_Loss: 1.2629 | Val_Acc: 0.7313\n",
      "Epoch [15/25] | Train_Loss: 1.2270 | Train_Acc: 0.7454 | Val_Loss: 1.1963 | Val_Acc: 0.7164\n",
      "Epoch [16/25] | Train_Loss: 1.1746 | Train_Acc: 0.7427 | Val_Loss: 1.1247 | Val_Acc: 0.7687 -> Best model updated\n",
      "Epoch [17/25] | Train_Loss: 1.1167 | Train_Acc: 0.7586 | Val_Loss: 1.0650 | Val_Acc: 0.7612\n",
      "Epoch [18/25] | Train_Loss: 1.0697 | Train_Acc: 0.7573 | Val_Loss: 1.0322 | Val_Acc: 0.7687\n",
      "Epoch [19/25] | Train_Loss: 0.9997 | Train_Acc: 0.7810 | Val_Loss: 0.9570 | Val_Acc: 0.7687\n",
      "Epoch [20/25] | Train_Loss: 0.9650 | Train_Acc: 0.7836 | Val_Loss: 0.9173 | Val_Acc: 0.7836 -> Best model updated\n",
      "Epoch [21/25] | Train_Loss: 0.9365 | Train_Acc: 0.7863 | Val_Loss: 0.8675 | Val_Acc: 0.8284 -> Best model updated\n",
      "Epoch [22/25] | Train_Loss: 0.8621 | Train_Acc: 0.8047 | Val_Loss: 0.8438 | Val_Acc: 0.8209\n",
      "Epoch [23/25] | Train_Loss: 0.8473 | Train_Acc: 0.8034 | Val_Loss: 0.7967 | Val_Acc: 0.8134\n",
      "Epoch [24/25] | Train_Loss: 0.7992 | Train_Acc: 0.8034 | Val_Loss: 0.8003 | Val_Acc: 0.8284\n",
      "Epoch [25/25] | Train_Loss: 0.7684 | Train_Acc: 0.8193 | Val_Loss: 0.7638 | Val_Acc: 0.8134\n",
      "Training completed. Best validation accuracy: 0.8284. Running steps training time: 792.19 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250506-154003_mobilevitv2-1.0-imagenet1k-256_unfreezed>\n",
      "==========================\n",
      "Test Loss: 0.9585 | Test Acc: 0.7342 | Running steps test time: 1.66 s.\n"
     ]
    }
   ],
   "source": [
    "model_card = \"apple/mobilevitv2-1.0-imagenet1k-256\"\n",
    "\n",
    "# Define the num_epochs and learning rate\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-5\n",
    "freeze_backbone = False\n",
    "\n",
    "# Define the model\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_card, num_labels=N_CLASSES, trust_remote_code=True, ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Define the image processor\n",
    "processor = AutoImageProcessor.from_pretrained(model_card)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    ")\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    freeze_backbone=freeze_backbone,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7638c8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileViTV2ForImageClassification were not initialized from the model checkpoint at apple/mobilevitv2-1.0-imagenet1k-256 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([7, 512]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250506-155653_mobilevitv2-1.0-imagenet1k-256_freezed>\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train_Loss: 1.9404 | Train_Acc: 0.1715 | Val_Loss: 1.9284 | Val_Acc: 0.1940 -> Best model updated\n",
      "Epoch [2/25] | Train_Loss: 1.9355 | Train_Acc: 0.1939 | Val_Loss: 1.9196 | Val_Acc: 0.2239 -> Best model updated\n",
      "Epoch [3/25] | Train_Loss: 1.9211 | Train_Acc: 0.2256 | Val_Loss: 1.9108 | Val_Acc: 0.2612 -> Best model updated\n",
      "Epoch [4/25] | Train_Loss: 1.9182 | Train_Acc: 0.2388 | Val_Loss: 1.9037 | Val_Acc: 0.2910 -> Best model updated\n",
      "Epoch [5/25] | Train_Loss: 1.9098 | Train_Acc: 0.2744 | Val_Loss: 1.8943 | Val_Acc: 0.3433 -> Best model updated\n",
      "Epoch [6/25] | Train_Loss: 1.9036 | Train_Acc: 0.2850 | Val_Loss: 1.8845 | Val_Acc: 0.3433\n",
      "Epoch [7/25] | Train_Loss: 1.8979 | Train_Acc: 0.3008 | Val_Loss: 1.8777 | Val_Acc: 0.3806 -> Best model updated\n",
      "Epoch [8/25] | Train_Loss: 1.8837 | Train_Acc: 0.3285 | Val_Loss: 1.8744 | Val_Acc: 0.3657\n",
      "Epoch [9/25] | Train_Loss: 1.8805 | Train_Acc: 0.3377 | Val_Loss: 1.8614 | Val_Acc: 0.3955 -> Best model updated\n",
      "Epoch [10/25] | Train_Loss: 1.8714 | Train_Acc: 0.3707 | Val_Loss: 1.8559 | Val_Acc: 0.3955\n",
      "Epoch [11/25] | Train_Loss: 1.8644 | Train_Acc: 0.3760 | Val_Loss: 1.8441 | Val_Acc: 0.4179 -> Best model updated\n",
      "Epoch [12/25] | Train_Loss: 1.8527 | Train_Acc: 0.4077 | Val_Loss: 1.8377 | Val_Acc: 0.4552 -> Best model updated\n",
      "Epoch [13/25] | Train_Loss: 1.8493 | Train_Acc: 0.4129 | Val_Loss: 1.8318 | Val_Acc: 0.4776 -> Best model updated\n",
      "Epoch [14/25] | Train_Loss: 1.8429 | Train_Acc: 0.4116 | Val_Loss: 1.8226 | Val_Acc: 0.4925 -> Best model updated\n",
      "Epoch [15/25] | Train_Loss: 1.8317 | Train_Acc: 0.4776 | Val_Loss: 1.8118 | Val_Acc: 0.5000 -> Best model updated\n",
      "Epoch [16/25] | Train_Loss: 1.8248 | Train_Acc: 0.4565 | Val_Loss: 1.8055 | Val_Acc: 0.5299 -> Best model updated\n",
      "Epoch [17/25] | Train_Loss: 1.8186 | Train_Acc: 0.4881 | Val_Loss: 1.7974 | Val_Acc: 0.5522 -> Best model updated\n",
      "Epoch [18/25] | Train_Loss: 1.8064 | Train_Acc: 0.5224 | Val_Loss: 1.7894 | Val_Acc: 0.5299\n",
      "Epoch [19/25] | Train_Loss: 1.8036 | Train_Acc: 0.5119 | Val_Loss: 1.7853 | Val_Acc: 0.5522\n",
      "Epoch [20/25] | Train_Loss: 1.7965 | Train_Acc: 0.5119 | Val_Loss: 1.7728 | Val_Acc: 0.5373\n",
      "Epoch [21/25] | Train_Loss: 1.7962 | Train_Acc: 0.5145 | Val_Loss: 1.7673 | Val_Acc: 0.5672 -> Best model updated\n",
      "Epoch [22/25] | Train_Loss: 1.7813 | Train_Acc: 0.5369 | Val_Loss: 1.7574 | Val_Acc: 0.5821 -> Best model updated\n",
      "Epoch [23/25] | Train_Loss: 1.7762 | Train_Acc: 0.5554 | Val_Loss: 1.7520 | Val_Acc: 0.5746\n",
      "Epoch [24/25] | Train_Loss: 1.7714 | Train_Acc: 0.5633 | Val_Loss: 1.7421 | Val_Acc: 0.6045 -> Best model updated\n",
      "Epoch [25/25] | Train_Loss: 1.7636 | Train_Acc: 0.5831 | Val_Loss: 1.7350 | Val_Acc: 0.6045\n",
      "Training completed. Best validation accuracy: 0.6045. Running steps training time: 695.52 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250506-155653_mobilevitv2-1.0-imagenet1k-256_freezed>\n",
      "==========================\n",
      "Test Loss: 1.7723 | Test Acc: 0.5886 | Running steps test time: 1.66 s.\n"
     ]
    }
   ],
   "source": [
    "model_card = \"apple/mobilevitv2-1.0-imagenet1k-256\"\n",
    "\n",
    "# Define the num_epochs and learning rate\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-5\n",
    "freeze_backbone = True\n",
    "\n",
    "# Define the model\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_card, num_labels=N_CLASSES, trust_remote_code=True, ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Define the image processor\n",
    "processor = AutoImageProcessor.from_pretrained(model_card)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    ")\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    freeze_backbone=freeze_backbone,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e274bf",
   "metadata": {},
   "source": [
    "## **MambaVision T**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "982b5796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250506-161235_MambaVision-T-1K_freezed>\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train_Loss: 1.9550 | Train_Acc: 0.1293 | Val_Loss: 1.9230 | Val_Acc: 0.1418 -> Best model updated\n",
      "Epoch [2/25] | Train_Loss: 1.9408 | Train_Acc: 0.1372 | Val_Loss: 1.9035 | Val_Acc: 0.1866 -> Best model updated\n",
      "Epoch [3/25] | Train_Loss: 1.9266 | Train_Acc: 0.2032 | Val_Loss: 1.8807 | Val_Acc: 0.2015 -> Best model updated\n",
      "Epoch [4/25] | Train_Loss: 1.9101 | Train_Acc: 0.2005 | Val_Loss: 1.8604 | Val_Acc: 0.2985 -> Best model updated\n",
      "Epoch [5/25] | Train_Loss: 1.8915 | Train_Acc: 0.2599 | Val_Loss: 1.8413 | Val_Acc: 0.3284 -> Best model updated\n",
      "Epoch [6/25] | Train_Loss: 1.8775 | Train_Acc: 0.2652 | Val_Loss: 1.8234 | Val_Acc: 0.4104 -> Best model updated\n",
      "Epoch [7/25] | Train_Loss: 1.8615 | Train_Acc: 0.3338 | Val_Loss: 1.8074 | Val_Acc: 0.4478 -> Best model updated\n",
      "Epoch [8/25] | Train_Loss: 1.8519 | Train_Acc: 0.3483 | Val_Loss: 1.7831 | Val_Acc: 0.4851 -> Best model updated\n",
      "Epoch [9/25] | Train_Loss: 1.8261 | Train_Acc: 0.4077 | Val_Loss: 1.7634 | Val_Acc: 0.5149 -> Best model updated\n",
      "Epoch [10/25] | Train_Loss: 1.8126 | Train_Acc: 0.4142 | Val_Loss: 1.7462 | Val_Acc: 0.5672 -> Best model updated\n",
      "Epoch [11/25] | Train_Loss: 1.7957 | Train_Acc: 0.4393 | Val_Loss: 1.7252 | Val_Acc: 0.5896 -> Best model updated\n",
      "Epoch [12/25] | Train_Loss: 1.7832 | Train_Acc: 0.4657 | Val_Loss: 1.7106 | Val_Acc: 0.6119 -> Best model updated\n",
      "Epoch [13/25] | Train_Loss: 1.7725 | Train_Acc: 0.5013 | Val_Loss: 1.6905 | Val_Acc: 0.6716 -> Best model updated\n",
      "Epoch [14/25] | Train_Loss: 1.7593 | Train_Acc: 0.5198 | Val_Loss: 1.6721 | Val_Acc: 0.6493\n",
      "Epoch [15/25] | Train_Loss: 1.7493 | Train_Acc: 0.5435 | Val_Loss: 1.6565 | Val_Acc: 0.6940 -> Best model updated\n",
      "Epoch [16/25] | Train_Loss: 1.7343 | Train_Acc: 0.5712 | Val_Loss: 1.6366 | Val_Acc: 0.6866\n",
      "Epoch [17/25] | Train_Loss: 1.7177 | Train_Acc: 0.5805 | Val_Loss: 1.6159 | Val_Acc: 0.7388 -> Best model updated\n",
      "Epoch [18/25] | Train_Loss: 1.7051 | Train_Acc: 0.5897 | Val_Loss: 1.5981 | Val_Acc: 0.7313\n",
      "Epoch [19/25] | Train_Loss: 1.6912 | Train_Acc: 0.6148 | Val_Loss: 1.5834 | Val_Acc: 0.7313\n",
      "Epoch [20/25] | Train_Loss: 1.6851 | Train_Acc: 0.6174 | Val_Loss: 1.5708 | Val_Acc: 0.7388\n",
      "Epoch [21/25] | Train_Loss: 1.6678 | Train_Acc: 0.6266 | Val_Loss: 1.5477 | Val_Acc: 0.7537 -> Best model updated\n",
      "Epoch [22/25] | Train_Loss: 1.6552 | Train_Acc: 0.6148 | Val_Loss: 1.5289 | Val_Acc: 0.7687 -> Best model updated\n",
      "Epoch [23/25] | Train_Loss: 1.6364 | Train_Acc: 0.6451 | Val_Loss: 1.5207 | Val_Acc: 0.7612\n",
      "Epoch [24/25] | Train_Loss: 1.6340 | Train_Acc: 0.6227 | Val_Loss: 1.4986 | Val_Acc: 0.7836 -> Best model updated\n",
      "Epoch [25/25] | Train_Loss: 1.6059 | Train_Acc: 0.6834 | Val_Loss: 1.4879 | Val_Acc: 0.8209 -> Best model updated\n",
      "Training completed. Best validation accuracy: 0.8209. Running steps training time: 425.26 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250506-161235_MambaVision-T-1K_freezed>\n",
      "==========================\n",
      "Test Loss: 1.5522 | Test Acc: 0.7025 | Running steps test time: 1.19 s.\n"
     ]
    }
   ],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"nvidia/MambaVision-T-1K\"\n",
    "\n",
    "# Define the num_epoch and LR\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-5\n",
    "freeze_backbone = True\n",
    "fc_layer = None\n",
    "\n",
    "# Define the model with the MambaClassifier class\n",
    "model = MambaClassifier(\n",
    "    AutoModel.from_pretrained(model_card, trust_remote_code=True),\n",
    "    num_classes=N_CLASSES,\n",
    "    hidden_dim=MAMBA_HIDDEN_SIZES.get(model_card),\n",
    "    fc_layer=fc_layer,  # Add the int number of layers before the classifier\n",
    ")\n",
    "\n",
    "# Define the image transform (processor and transform)\n",
    "train_transform = model.create_transform(\n",
    "    training=True\n",
    ")  # auto_augment=\"rand-m9-mstd0.5-inc1\"\n",
    "test_transform = model.create_transform(training=False)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    train_transform=train_transform,\n",
    "    test_transform=test_transform,\n",
    ")\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    freeze_backbone=freeze_backbone,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b41c0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250506-162559_MambaVision-T-1K_unfreezed>\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train_Loss: 1.9123 | Train_Acc: 0.2216 | Val_Loss: 1.7977 | Val_Acc: 0.4552 -> Best model updated\n",
      "Epoch [2/25] | Train_Loss: 1.7547 | Train_Acc: 0.4868 | Val_Loss: 1.5571 | Val_Acc: 0.7388 -> Best model updated\n",
      "Epoch [3/25] | Train_Loss: 1.5166 | Train_Acc: 0.6491 | Val_Loss: 1.1895 | Val_Acc: 0.8060 -> Best model updated\n",
      "Epoch [4/25] | Train_Loss: 1.2447 | Train_Acc: 0.7058 | Val_Loss: 0.8419 | Val_Acc: 0.8507 -> Best model updated\n",
      "Epoch [5/25] | Train_Loss: 0.9951 | Train_Acc: 0.7480 | Val_Loss: 0.5986 | Val_Acc: 0.8731 -> Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (91043764 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25] | Train_Loss: 0.8258 | Train_Acc: 0.7810 | Val_Loss: 0.5028 | Val_Acc: 0.8881 -> Best model updated\n",
      "Epoch [7/25] | Train_Loss: 0.7693 | Train_Acc: 0.7797 | Val_Loss: 0.4609 | Val_Acc: 0.8881\n",
      "Epoch [8/25] | Train_Loss: 0.7123 | Train_Acc: 0.8047 | Val_Loss: 0.4238 | Val_Acc: 0.8881\n",
      "Epoch [9/25] | Train_Loss: 0.6518 | Train_Acc: 0.8298 | Val_Loss: 0.4029 | Val_Acc: 0.8881\n",
      "Epoch [10/25] | Train_Loss: 0.6238 | Train_Acc: 0.8232 | Val_Loss: 0.3913 | Val_Acc: 0.8806\n",
      "Epoch [11/25] | Train_Loss: 0.5733 | Train_Acc: 0.8364 | Val_Loss: 0.3873 | Val_Acc: 0.8881\n",
      "Epoch [12/25] | Train_Loss: 0.5490 | Train_Acc: 0.8470 | Val_Loss: 0.3722 | Val_Acc: 0.8881\n",
      "Epoch [13/25] | Train_Loss: 0.5104 | Train_Acc: 0.8628 | Val_Loss: 0.3777 | Val_Acc: 0.8657\n",
      "Epoch [14/25] | Train_Loss: 0.4973 | Train_Acc: 0.8628 | Val_Loss: 0.3740 | Val_Acc: 0.8955 -> Best model updated\n",
      "Epoch [15/25] | Train_Loss: 0.4582 | Train_Acc: 0.8826 | Val_Loss: 0.3726 | Val_Acc: 0.8806\n",
      "Epoch [16/25] | Train_Loss: 0.4365 | Train_Acc: 0.8997 | Val_Loss: 0.3780 | Val_Acc: 0.8731\n",
      "Epoch [17/25] | Train_Loss: 0.4360 | Train_Acc: 0.8892 | Val_Loss: 0.3755 | Val_Acc: 0.8881\n",
      "Epoch [18/25] | Train_Loss: 0.4371 | Train_Acc: 0.8760 | Val_Loss: 0.3759 | Val_Acc: 0.8955\n",
      "Epoch [19/25] | Train_Loss: 0.4049 | Train_Acc: 0.8918 | Val_Loss: 0.3695 | Val_Acc: 0.8881\n",
      "Epoch [20/25] | Train_Loss: 0.4165 | Train_Acc: 0.8905 | Val_Loss: 0.3849 | Val_Acc: 0.8955\n",
      "Epoch [21/25] | Train_Loss: 0.4036 | Train_Acc: 0.8773 | Val_Loss: 0.3858 | Val_Acc: 0.8881\n",
      "Epoch [22/25] | Train_Loss: 0.3634 | Train_Acc: 0.9063 | Val_Loss: 0.3848 | Val_Acc: 0.8881\n",
      "Epoch [23/25] | Train_Loss: 0.3316 | Train_Acc: 0.9142 | Val_Loss: 0.3824 | Val_Acc: 0.8955\n",
      "Epoch [24/25] | Train_Loss: 0.3053 | Train_Acc: 0.9208 | Val_Loss: 0.3799 | Val_Acc: 0.8881\n",
      "Epoch [25/25] | Train_Loss: 0.3120 | Train_Acc: 0.9182 | Val_Loss: 0.3965 | Val_Acc: 0.8881\n",
      "Training completed. Best validation accuracy: 0.8955. Running steps training time: 678.47 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250506-162559_MambaVision-T-1K_unfreezed>\n",
      "==========================\n",
      "Test Loss: 0.5494 | Test Acc: 0.8228 | Running steps test time: 1.25 s.\n"
     ]
    }
   ],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"nvidia/MambaVision-T-1K\"\n",
    "\n",
    "# Define the num_epoch and LR\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-5\n",
    "freeze_backbone = False\n",
    "fc_layer = None\n",
    "\n",
    "# Define the model with the MambaClassifier class\n",
    "model = MambaClassifier(\n",
    "    AutoModel.from_pretrained(model_card, trust_remote_code=True),\n",
    "    num_classes=N_CLASSES,\n",
    "    hidden_dim=MAMBA_HIDDEN_SIZES.get(model_card),\n",
    "    fc_layer=fc_layer,  # Add the int number of layers before the classifier\n",
    ")\n",
    "\n",
    "# Define the image transform (processor and transform)\n",
    "train_transform = model.create_transform(\n",
    "    training=True\n",
    ")  # auto_augment=\"rand-m9-mstd0.5-inc1\"\n",
    "test_transform = model.create_transform(training=False)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    train_transform=train_transform,\n",
    "    test_transform=test_transform,\n",
    ")\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    freeze_backbone=freeze_backbone,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99a67e",
   "metadata": {},
   "source": [
    "## MambaVision S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cbbff67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250506-164058_MambaVision-S-1K_unfreezed>\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train_Loss: 1.9188 | Train_Acc: 0.2361 | Val_Loss: 1.8199 | Val_Acc: 0.5597 -> Best model updated\n",
      "Epoch [2/25] | Train_Loss: 1.7813 | Train_Acc: 0.5633 | Val_Loss: 1.6007 | Val_Acc: 0.7612 -> Best model updated\n",
      "Epoch [3/25] | Train_Loss: 1.5573 | Train_Acc: 0.6834 | Val_Loss: 1.1857 | Val_Acc: 0.8358 -> Best model updated\n",
      "Epoch [4/25] | Train_Loss: 1.1687 | Train_Acc: 0.7493 | Val_Loss: 0.7202 | Val_Acc: 0.8284\n",
      "Epoch [5/25] | Train_Loss: 0.8843 | Train_Acc: 0.7757 | Val_Loss: 0.5176 | Val_Acc: 0.8731 -> Best model updated\n",
      "Epoch [6/25] | Train_Loss: 0.7780 | Train_Acc: 0.7955 | Val_Loss: 0.4559 | Val_Acc: 0.8433\n",
      "Epoch [7/25] | Train_Loss: 0.6636 | Train_Acc: 0.8153 | Val_Loss: 0.4216 | Val_Acc: 0.8657\n",
      "Epoch [8/25] | Train_Loss: 0.6210 | Train_Acc: 0.8456 | Val_Loss: 0.4162 | Val_Acc: 0.8881 -> Best model updated\n",
      "Epoch [9/25] | Train_Loss: 0.6097 | Train_Acc: 0.8285 | Val_Loss: 0.3993 | Val_Acc: 0.8731\n",
      "Epoch [10/25] | Train_Loss: 0.5708 | Train_Acc: 0.8417 | Val_Loss: 0.3827 | Val_Acc: 0.8955 -> Best model updated\n",
      "Epoch [11/25] | Train_Loss: 0.5751 | Train_Acc: 0.8483 | Val_Loss: 0.3869 | Val_Acc: 0.8955\n",
      "Epoch [12/25] | Train_Loss: 0.4925 | Train_Acc: 0.8575 | Val_Loss: 0.3799 | Val_Acc: 0.9030 -> Best model updated\n",
      "Epoch [13/25] | Train_Loss: 0.5154 | Train_Acc: 0.8615 | Val_Loss: 0.3842 | Val_Acc: 0.9030\n",
      "Epoch [14/25] | Train_Loss: 0.4703 | Train_Acc: 0.8654 | Val_Loss: 0.3483 | Val_Acc: 0.9030\n",
      "Epoch [15/25] | Train_Loss: 0.5121 | Train_Acc: 0.8522 | Val_Loss: 0.3792 | Val_Acc: 0.9030\n",
      "Epoch [16/25] | Train_Loss: 0.4081 | Train_Acc: 0.8839 | Val_Loss: 0.3814 | Val_Acc: 0.8881\n",
      "Epoch [17/25] | Train_Loss: 0.4262 | Train_Acc: 0.8826 | Val_Loss: 0.3762 | Val_Acc: 0.8881\n",
      "Epoch [18/25] | Train_Loss: 0.4080 | Train_Acc: 0.8865 | Val_Loss: 0.3796 | Val_Acc: 0.8881\n",
      "Epoch [19/25] | Train_Loss: 0.3412 | Train_Acc: 0.9063 | Val_Loss: 0.3704 | Val_Acc: 0.8881\n",
      "Epoch [20/25] | Train_Loss: 0.3512 | Train_Acc: 0.9011 | Val_Loss: 0.3913 | Val_Acc: 0.8806\n",
      "Epoch [21/25] | Train_Loss: 0.3691 | Train_Acc: 0.9024 | Val_Loss: 0.3954 | Val_Acc: 0.8806\n",
      "Epoch [22/25] | Train_Loss: 0.3944 | Train_Acc: 0.8958 | Val_Loss: 0.3751 | Val_Acc: 0.8881\n",
      "Epoch [23/25] | Train_Loss: 0.3417 | Train_Acc: 0.9169 | Val_Loss: 0.3923 | Val_Acc: 0.8881\n",
      "Epoch [24/25] | Train_Loss: 0.3153 | Train_Acc: 0.9129 | Val_Loss: 0.3797 | Val_Acc: 0.8881\n",
      "Epoch [25/25] | Train_Loss: 0.3133 | Train_Acc: 0.9288 | Val_Loss: 0.3902 | Val_Acc: 0.8955\n",
      "Training completed. Best validation accuracy: 0.9030. Running steps training time: 834.99 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250506-164058_MambaVision-S-1K_unfreezed>\n",
      "==========================\n",
      "Test Loss: 0.5697 | Test Acc: 0.8101 | Running steps test time: 1.49 s.\n"
     ]
    }
   ],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"nvidia/MambaVision-S-1K\"\n",
    "\n",
    "# Define the num_epoch and LR\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-5\n",
    "freeze_backbone = False\n",
    "fc_layer = None\n",
    "\n",
    "# Define the model with the MambaClassifier class\n",
    "model = MambaClassifier(\n",
    "    AutoModel.from_pretrained(model_card, trust_remote_code=True),\n",
    "    num_classes=N_CLASSES,\n",
    "    hidden_dim=MAMBA_HIDDEN_SIZES.get(model_card),\n",
    "    fc_layer=fc_layer,  # Add the int number of layers before the classifier\n",
    ")\n",
    "\n",
    "# Define the image transform (processor and transform)\n",
    "train_transform = model.create_transform(\n",
    "    training=True\n",
    ")  # auto_augment=\"rand-m9-mstd0.5-inc1\"\n",
    "test_transform = model.create_transform(training=False)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    train_transform=train_transform,\n",
    "    test_transform=test_transform,\n",
    ")\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    freeze_backbone=freeze_backbone,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c800722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250506-165756_MambaVision-S-1K_freezed>\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train_Loss: 1.9367 | Train_Acc: 0.1464 | Val_Loss: 1.9261 | Val_Acc: 0.1866 -> Best model updated\n",
      "Epoch [2/25] | Train_Loss: 1.9241 | Train_Acc: 0.2032 | Val_Loss: 1.9070 | Val_Acc: 0.2388 -> Best model updated\n",
      "Epoch [3/25] | Train_Loss: 1.9036 | Train_Acc: 0.2480 | Val_Loss: 1.8873 | Val_Acc: 0.2910 -> Best model updated\n",
      "Epoch [4/25] | Train_Loss: 1.8890 | Train_Acc: 0.3140 | Val_Loss: 1.8679 | Val_Acc: 0.3955 -> Best model updated\n",
      "Epoch [5/25] | Train_Loss: 1.8712 | Train_Acc: 0.3588 | Val_Loss: 1.8508 | Val_Acc: 0.4328 -> Best model updated\n",
      "Epoch [6/25] | Train_Loss: 1.8670 | Train_Acc: 0.3905 | Val_Loss: 1.8314 | Val_Acc: 0.4627 -> Best model updated\n",
      "Epoch [7/25] | Train_Loss: 1.8539 | Train_Acc: 0.4288 | Val_Loss: 1.8149 | Val_Acc: 0.4776 -> Best model updated\n",
      "Epoch [8/25] | Train_Loss: 1.8324 | Train_Acc: 0.4802 | Val_Loss: 1.7965 | Val_Acc: 0.5373 -> Best model updated\n",
      "Epoch [9/25] | Train_Loss: 1.8161 | Train_Acc: 0.5198 | Val_Loss: 1.7802 | Val_Acc: 0.5597 -> Best model updated\n",
      "Epoch [10/25] | Train_Loss: 1.8083 | Train_Acc: 0.5290 | Val_Loss: 1.7613 | Val_Acc: 0.6045 -> Best model updated\n",
      "Epoch [11/25] | Train_Loss: 1.7961 | Train_Acc: 0.5396 | Val_Loss: 1.7450 | Val_Acc: 0.6119 -> Best model updated\n",
      "Epoch [12/25] | Train_Loss: 1.7810 | Train_Acc: 0.5699 | Val_Loss: 1.7281 | Val_Acc: 0.6418 -> Best model updated\n",
      "Epoch [13/25] | Train_Loss: 1.7675 | Train_Acc: 0.5976 | Val_Loss: 1.7113 | Val_Acc: 0.6418\n",
      "Epoch [14/25] | Train_Loss: 1.7613 | Train_Acc: 0.5699 | Val_Loss: 1.6937 | Val_Acc: 0.6642 -> Best model updated\n",
      "Epoch [15/25] | Train_Loss: 1.7466 | Train_Acc: 0.6003 | Val_Loss: 1.6788 | Val_Acc: 0.6866 -> Best model updated\n",
      "Epoch [16/25] | Train_Loss: 1.7259 | Train_Acc: 0.6253 | Val_Loss: 1.6656 | Val_Acc: 0.6791\n",
      "Epoch [17/25] | Train_Loss: 1.7176 | Train_Acc: 0.6319 | Val_Loss: 1.6459 | Val_Acc: 0.7015 -> Best model updated\n",
      "Epoch [18/25] | Train_Loss: 1.6948 | Train_Acc: 0.6768 | Val_Loss: 1.6325 | Val_Acc: 0.7164 -> Best model updated\n",
      "Epoch [19/25] | Train_Loss: 1.6873 | Train_Acc: 0.6596 | Val_Loss: 1.6157 | Val_Acc: 0.7164\n",
      "Epoch [20/25] | Train_Loss: 1.6644 | Train_Acc: 0.6741 | Val_Loss: 1.6000 | Val_Acc: 0.7313 -> Best model updated\n",
      "Epoch [21/25] | Train_Loss: 1.6626 | Train_Acc: 0.6781 | Val_Loss: 1.5841 | Val_Acc: 0.7313\n",
      "Epoch [22/25] | Train_Loss: 1.6553 | Train_Acc: 0.6741 | Val_Loss: 1.5713 | Val_Acc: 0.7612 -> Best model updated\n",
      "Epoch [23/25] | Train_Loss: 1.6337 | Train_Acc: 0.6992 | Val_Loss: 1.5557 | Val_Acc: 0.7388\n",
      "Epoch [24/25] | Train_Loss: 1.6209 | Train_Acc: 0.6900 | Val_Loss: 1.5419 | Val_Acc: 0.7463\n",
      "Epoch [25/25] | Train_Loss: 1.6152 | Train_Acc: 0.6913 | Val_Loss: 1.5235 | Val_Acc: 0.7836 -> Best model updated\n",
      "Training completed. Best validation accuracy: 0.7836. Running steps training time: 476.99 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250506-165756_MambaVision-S-1K_freezed>\n",
      "==========================\n",
      "Test Loss: 1.5847 | Test Acc: 0.6962 | Running steps test time: 1.50 s.\n"
     ]
    }
   ],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"nvidia/MambaVision-S-1K\"\n",
    "\n",
    "# Define the num_epoch and LR\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-5\n",
    "freeze_backbone = True\n",
    "fc_layer = None\n",
    "\n",
    "# Define the model with the MambaClassifier class\n",
    "model = MambaClassifier(\n",
    "    AutoModel.from_pretrained(model_card, trust_remote_code=True),\n",
    "    num_classes=N_CLASSES,\n",
    "    hidden_dim=MAMBA_HIDDEN_SIZES.get(model_card),\n",
    "    fc_layer=fc_layer,  # Add the int number of layers before the classifier\n",
    ")\n",
    "\n",
    "# Define the image transform (processor and transform)\n",
    "train_transform = model.create_transform(\n",
    "    training=True\n",
    ")  # auto_augment=\"rand-m9-mstd0.5-inc1\"\n",
    "test_transform = model.create_transform(training=False)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    train_transform=train_transform,\n",
    "    test_transform=test_transform,\n",
    ")\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    freeze_backbone=freeze_backbone,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91550dce",
   "metadata": {},
   "source": [
    "## MambaVision B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43e9ef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250506-170901_MambaVision-B-1K_unfreezed>\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train_Loss: 1.8249 | Train_Acc: 0.4050 | Val_Loss: 1.5972 | Val_Acc: 0.6642 -> Best model updated\n",
      "Epoch [2/25] | Train_Loss: 1.4240 | Train_Acc: 0.6900 | Val_Loss: 0.9693 | Val_Acc: 0.7910 -> Best model updated\n",
      "Epoch [3/25] | Train_Loss: 0.9420 | Train_Acc: 0.7810 | Val_Loss: 0.5282 | Val_Acc: 0.8582 -> Best model updated\n",
      "Epoch [4/25] | Train_Loss: 0.7365 | Train_Acc: 0.7876 | Val_Loss: 0.4041 | Val_Acc: 0.8955 -> Best model updated\n",
      "Epoch [5/25] | Train_Loss: 0.5926 | Train_Acc: 0.8351 | Val_Loss: 0.3853 | Val_Acc: 0.9030 -> Best model updated\n",
      "Epoch [6/25] | Train_Loss: 0.5243 | Train_Acc: 0.8575 | Val_Loss: 0.3495 | Val_Acc: 0.9030\n",
      "Epoch [7/25] | Train_Loss: 0.4519 | Train_Acc: 0.8615 | Val_Loss: 0.3334 | Val_Acc: 0.9179 -> Best model updated\n",
      "Epoch [8/25] | Train_Loss: 0.4198 | Train_Acc: 0.8865 | Val_Loss: 0.3411 | Val_Acc: 0.8806\n",
      "Epoch [9/25] | Train_Loss: 0.3711 | Train_Acc: 0.9129 | Val_Loss: 0.3319 | Val_Acc: 0.8955\n",
      "Epoch [10/25] | Train_Loss: 0.3695 | Train_Acc: 0.8997 | Val_Loss: 0.3094 | Val_Acc: 0.9179\n",
      "Epoch [11/25] | Train_Loss: 0.3802 | Train_Acc: 0.9090 | Val_Loss: 0.3264 | Val_Acc: 0.9179\n",
      "Epoch [12/25] | Train_Loss: 0.3377 | Train_Acc: 0.8997 | Val_Loss: 0.3240 | Val_Acc: 0.9030\n",
      "Epoch [13/25] | Train_Loss: 0.2732 | Train_Acc: 0.9301 | Val_Loss: 0.3405 | Val_Acc: 0.9104\n",
      "Epoch [14/25] | Train_Loss: 0.3117 | Train_Acc: 0.9116 | Val_Loss: 0.3448 | Val_Acc: 0.9179\n",
      "Epoch [15/25] | Train_Loss: 0.2766 | Train_Acc: 0.9314 | Val_Loss: 0.3073 | Val_Acc: 0.9179\n",
      "Epoch [16/25] | Train_Loss: 0.2931 | Train_Acc: 0.9156 | Val_Loss: 0.3528 | Val_Acc: 0.8955\n",
      "Epoch [17/25] | Train_Loss: 0.2346 | Train_Acc: 0.9314 | Val_Loss: 0.3160 | Val_Acc: 0.9030\n",
      "Epoch [18/25] | Train_Loss: 0.2292 | Train_Acc: 0.9340 | Val_Loss: 0.3150 | Val_Acc: 0.9030\n",
      "Epoch [19/25] | Train_Loss: 0.2284 | Train_Acc: 0.9340 | Val_Loss: 0.3709 | Val_Acc: 0.9104\n",
      "Epoch [20/25] | Train_Loss: 0.1918 | Train_Acc: 0.9538 | Val_Loss: 0.3902 | Val_Acc: 0.9104\n",
      "Epoch [21/25] | Train_Loss: 0.2075 | Train_Acc: 0.9393 | Val_Loss: 0.3680 | Val_Acc: 0.9179\n",
      "Epoch [22/25] | Train_Loss: 0.2106 | Train_Acc: 0.9420 | Val_Loss: 0.3622 | Val_Acc: 0.8955\n",
      "Epoch [23/25] | Train_Loss: 0.1687 | Train_Acc: 0.9591 | Val_Loss: 0.3374 | Val_Acc: 0.9104\n",
      "Epoch [24/25] | Train_Loss: 0.1737 | Train_Acc: 0.9591 | Val_Loss: 0.3774 | Val_Acc: 0.9030\n",
      "Epoch [25/25] | Train_Loss: 0.1846 | Train_Acc: 0.9485 | Val_Loss: 0.4264 | Val_Acc: 0.8806\n",
      "Training completed. Best validation accuracy: 0.9179. Running steps training time: 1347.37 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250506-170901_MambaVision-B-1K_unfreezed>\n",
      "==========================\n",
      "Test Loss: 0.5156 | Test Acc: 0.8291 | Running steps test time: 2.98 s.\n"
     ]
    }
   ],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"nvidia/MambaVision-B-1K\"\n",
    "\n",
    "# Define the num_epoch and LR\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-5\n",
    "freeze_backbone = False\n",
    "fc_layer = None\n",
    "\n",
    "# Define the model with the MambaClassifier class\n",
    "model = MambaClassifier(\n",
    "    AutoModel.from_pretrained(model_card, trust_remote_code=True),\n",
    "    num_classes=N_CLASSES,\n",
    "    hidden_dim=MAMBA_HIDDEN_SIZES.get(model_card),\n",
    "    fc_layer=fc_layer,  # Add the int number of layers before the classifier\n",
    ")\n",
    "\n",
    "# Define the image transform (processor and transform)\n",
    "train_transform = model.create_transform(\n",
    "    training=True\n",
    ")  # auto_augment=\"rand-m9-mstd0.5-inc1\"\n",
    "test_transform = model.create_transform(training=False)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    train_transform=train_transform,\n",
    "    test_transform=test_transform,\n",
    ")\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    freeze_backbone=freeze_backbone,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "388415bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250506-173916_MambaVision-B-1K_freezed>\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train_Loss: 1.9316 | Train_Acc: 0.1847 | Val_Loss: 1.9164 | Val_Acc: 0.2015 -> Best model updated\n",
      "Epoch [2/25] | Train_Loss: 1.9113 | Train_Acc: 0.2454 | Val_Loss: 1.8878 | Val_Acc: 0.2836 -> Best model updated\n",
      "Epoch [3/25] | Train_Loss: 1.8913 | Train_Acc: 0.2968 | Val_Loss: 1.8551 | Val_Acc: 0.3582 -> Best model updated\n",
      "Epoch [4/25] | Train_Loss: 1.8654 | Train_Acc: 0.3588 | Val_Loss: 1.8285 | Val_Acc: 0.3955 -> Best model updated\n",
      "Epoch [5/25] | Train_Loss: 1.8446 | Train_Acc: 0.3984 | Val_Loss: 1.8002 | Val_Acc: 0.4328 -> Best model updated\n",
      "Epoch [6/25] | Train_Loss: 1.8208 | Train_Acc: 0.4578 | Val_Loss: 1.7693 | Val_Acc: 0.5597 -> Best model updated\n",
      "Epoch [7/25] | Train_Loss: 1.7987 | Train_Acc: 0.4894 | Val_Loss: 1.7415 | Val_Acc: 0.6119 -> Best model updated\n",
      "Epoch [8/25] | Train_Loss: 1.7787 | Train_Acc: 0.5449 | Val_Loss: 1.7149 | Val_Acc: 0.6716 -> Best model updated\n",
      "Epoch [9/25] | Train_Loss: 1.7607 | Train_Acc: 0.5765 | Val_Loss: 1.6887 | Val_Acc: 0.6866 -> Best model updated\n",
      "Epoch [10/25] | Train_Loss: 1.7347 | Train_Acc: 0.5897 | Val_Loss: 1.6610 | Val_Acc: 0.7313 -> Best model updated\n",
      "Epoch [11/25] | Train_Loss: 1.7185 | Train_Acc: 0.6201 | Val_Loss: 1.6337 | Val_Acc: 0.7761 -> Best model updated\n",
      "Epoch [12/25] | Train_Loss: 1.7085 | Train_Acc: 0.6346 | Val_Loss: 1.6095 | Val_Acc: 0.7612\n",
      "Epoch [13/25] | Train_Loss: 1.6735 | Train_Acc: 0.6412 | Val_Loss: 1.5823 | Val_Acc: 0.7836 -> Best model updated\n",
      "Epoch [14/25] | Train_Loss: 1.6513 | Train_Acc: 0.6662 | Val_Loss: 1.5592 | Val_Acc: 0.7910 -> Best model updated\n",
      "Epoch [15/25] | Train_Loss: 1.6419 | Train_Acc: 0.6689 | Val_Loss: 1.5363 | Val_Acc: 0.7836\n",
      "Epoch [16/25] | Train_Loss: 1.6233 | Train_Acc: 0.6821 | Val_Loss: 1.5120 | Val_Acc: 0.8134 -> Best model updated\n",
      "Epoch [17/25] | Train_Loss: 1.6068 | Train_Acc: 0.6966 | Val_Loss: 1.4865 | Val_Acc: 0.8209 -> Best model updated\n",
      "Epoch [18/25] | Train_Loss: 1.5825 | Train_Acc: 0.7111 | Val_Loss: 1.4656 | Val_Acc: 0.8209\n",
      "Epoch [19/25] | Train_Loss: 1.5606 | Train_Acc: 0.7164 | Val_Loss: 1.4462 | Val_Acc: 0.8209\n",
      "Epoch [20/25] | Train_Loss: 1.5400 | Train_Acc: 0.7335 | Val_Loss: 1.4220 | Val_Acc: 0.8284 -> Best model updated\n",
      "Epoch [21/25] | Train_Loss: 1.5369 | Train_Acc: 0.7296 | Val_Loss: 1.3985 | Val_Acc: 0.8209\n",
      "Epoch [22/25] | Train_Loss: 1.5197 | Train_Acc: 0.7467 | Val_Loss: 1.3790 | Val_Acc: 0.8284\n",
      "Epoch [23/25] | Train_Loss: 1.5007 | Train_Acc: 0.7348 | Val_Loss: 1.3544 | Val_Acc: 0.8284\n",
      "Epoch [24/25] | Train_Loss: 1.4906 | Train_Acc: 0.7335 | Val_Loss: 1.3430 | Val_Acc: 0.8507 -> Best model updated\n",
      "Epoch [25/25] | Train_Loss: 1.4733 | Train_Acc: 0.7414 | Val_Loss: 1.3166 | Val_Acc: 0.8433\n",
      "Training completed. Best validation accuracy: 0.8507. Running steps training time: 659.65 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250506-173916_MambaVision-B-1K_freezed>\n",
      "==========================\n",
      "Test Loss: 1.4275 | Test Acc: 0.7658 | Running steps test time: 2.97 s.\n"
     ]
    }
   ],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"nvidia/MambaVision-B-1K\"\n",
    "\n",
    "# Define the num_epoch and LR\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-5\n",
    "freeze_backbone = True\n",
    "fc_layer = None\n",
    "\n",
    "# Define the model with the MambaClassifier class\n",
    "model = MambaClassifier(\n",
    "    AutoModel.from_pretrained(model_card, trust_remote_code=True),\n",
    "    num_classes=N_CLASSES,\n",
    "    hidden_dim=MAMBA_HIDDEN_SIZES.get(model_card),\n",
    "    fc_layer=fc_layer,  # Add the int number of layers before the classifier\n",
    ")\n",
    "\n",
    "# Define the image transform (processor and transform)\n",
    "train_transform = model.create_transform(\n",
    "    training=True\n",
    ")  # auto_augment=\"rand-m9-mstd0.5-inc1\"\n",
    "test_transform = model.create_transform(training=False)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    train_transform=train_transform,\n",
    "    test_transform=test_transform,\n",
    ")\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    freeze_backbone=freeze_backbone,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef334963",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
