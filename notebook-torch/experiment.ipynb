{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07aa478c",
   "metadata": {},
   "source": [
    "- Boucle d'entraînement\n",
    "    - [ ] Définir proprement les métriques d'évaluation\n",
    "    - [ ] Keep track des métriques d'évaluation\n",
    "    - [ ] Afficher les métriques d'évaluation\n",
    "- Mamba\n",
    "    - [ ] Tester deux configurations de paramètres (celle du papier et une + classique)\n",
    "    - [ ] Créer un environnement avec mamba\n",
    "    - [ ] Installer les dépendances avec mamba\n",
    "- Tester tous les modèles à comparer\n",
    "    - [ ] Créer un script pour tester tous les modèles à comparer\n",
    "    - [ ] Créer un script pour tester tous les modèles à comparer avec mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d9aa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor, AutoModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from constants import ROOT_FOLDER, SEED, VAL_SIZE, TEST_SIZE, BATCH_SIZE, SAMPLING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "651be0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  2.6.0+cu124\n",
      "Cuda version:  12.4\n",
      "CUDNN version:  90100\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Désactiver les alertes de FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Print the torch? cuda and cudnn version\n",
    "print(\"Torch version: \", torch.__version__)\n",
    "print(\"Cuda version: \", torch.version.cuda)\n",
    "print(\"CUDNN version: \", torch.backends.cudnn.version())\n",
    "\n",
    "# Set the device to GPU if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af879910",
   "metadata": {},
   "source": [
    "[COPY] Récupération des paramètres de train-test-split sur le jeu utilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85ca3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the folder path containing the images\n",
    "IMAGE_FOLDER = ROOT_FOLDER / \"data\" / \"images\"\n",
    "IMAGE_TEST = ROOT_FOLDER / \"data\" / \"images\" / \"2aaa6083689193df5ab01fe37dea1b5e.jpg\"\n",
    "# Assign the folder path containing the former H5 efficientnet weights\n",
    "ARTIFACTS_FOLDER = ROOT_FOLDER / \"artifacts\"\n",
    "# Assign the folder path with the pickle dataset with labels, images filenames and metadata\n",
    "DATASET_PATH = ROOT_FOLDER / \"data\" / \"dataset_cleaned.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37f68941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1050, 2)\n",
      "Dataset columns: Index(['image', 'class'], dtype='object')\n",
      "Number of classes: 7\n",
      "Classes: ['Baby Care', 'Beauty and Personal Care', 'Computers', 'Home Decor & Festive Needs', 'Home Furnishing', 'Kitchen & Dining', 'Watches']\n"
     ]
    }
   ],
   "source": [
    "# Loading the pickle dataset_cleaned used with the previous project as a pandas df\n",
    "df = pd.read_pickle(DATASET_PATH).drop(columns=['product_name', 'description'])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Dataset columns: {df.columns}\")\n",
    "\n",
    "# Encode the labels with LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df[\"class\"])\n",
    "n_classes = len(le.classes_)\n",
    "classes = le.classes_.tolist()\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# Finally transform the class column to the encoded labels\n",
    "df[\"class\"] = le.transform(df[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91d9afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (128, 2)\n",
      "Val shape: (128, 2)\n",
      "Test shape: (128, 2)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the datasets into train, val and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(df['image'], df['class'], test_size=TEST_SIZE, random_state=SEED, stratify=df['class'], shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=VAL_SIZE, random_state=SEED, stratify=y_temp, shuffle=True)\n",
    "\n",
    "# Concat X and y for each set\n",
    "train = pd.concat([X_train, y_train], axis=1).sample(SAMPLING) if SAMPLING else pd.concat([X_train, y_train], axis=1)\n",
    "val = pd.concat([X_val, y_val], axis=1).sample(SAMPLING) if SAMPLING else pd.concat([X_val, y_val], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1).sample(SAMPLING) if SAMPLING else pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Print the shape of each set\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Val shape: {val.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08d4ee5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "df168aa1-9f0b-44cf-a637-cc8a53e24f6b",
       "rows": [
        [
         "415",
         "b17d093c269db6ad3bf4b6076461fd72.jpg",
         "1"
        ],
        [
         "129",
         "f1484a63c0dc79e96c98b2a250380ed8.jpg",
         "5"
        ],
        [
         "437",
         "b6c339777814451815fe7950fd4c8536.jpg",
         "1"
        ],
        [
         "802",
         "020d2daba55a723128e4e07c0b0393a9.jpg",
         "1"
        ],
        [
         "205",
         "c109187e7975ee8fdb2b3dfbdd9dce20.jpg",
         "3"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>b17d093c269db6ad3bf4b6076461fd72.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>f1484a63c0dc79e96c98b2a250380ed8.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>b6c339777814451815fe7950fd4c8536.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>020d2daba55a723128e4e07c0b0393a9.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>c109187e7975ee8fdb2b3dfbdd9dce20.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "415  b17d093c269db6ad3bf4b6076461fd72.jpg      1\n",
       "129  f1484a63c0dc79e96c98b2a250380ed8.jpg      5\n",
       "437  b6c339777814451815fe7950fd4c8536.jpg      1\n",
       "802  020d2daba55a723128e4e07c0b0393a9.jpg      1\n",
       "205  c109187e7975ee8fdb2b3dfbdd9dce20.jpg      3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4eede8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "008e337b-7008-42ba-8601-89903804a22d",
       "rows": [
        [
         "346",
         "52c5fe837181962a7c1d5883bb9d7007.jpg",
         "1"
        ],
        [
         "196",
         "40829ac110f344ef35f1c8c32fb311c7.jpg",
         "3"
        ],
        [
         "162",
         "02127f52e96fb72c66bf081d25104ef8.jpg",
         "5"
        ],
        [
         "218",
         "ce02ab514e4c623dcf7acd7182a7762d.jpg",
         "5"
        ],
        [
         "213",
         "e3e7c584f317d1b5d6ac0088c411565e.jpg",
         "3"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>52c5fe837181962a7c1d5883bb9d7007.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>40829ac110f344ef35f1c8c32fb311c7.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>02127f52e96fb72c66bf081d25104ef8.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>ce02ab514e4c623dcf7acd7182a7762d.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>e3e7c584f317d1b5d6ac0088c411565e.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "346  52c5fe837181962a7c1d5883bb9d7007.jpg      1\n",
       "196  40829ac110f344ef35f1c8c32fb311c7.jpg      3\n",
       "162  02127f52e96fb72c66bf081d25104ef8.jpg      5\n",
       "218  ce02ab514e4c623dcf7acd7182a7762d.jpg      5\n",
       "213  e3e7c584f317d1b5d6ac0088c411565e.jpg      3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6ba2b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "99cb65a2-cafb-4930-9900-885a7948d3e1",
       "rows": [
        [
         "927",
         "fb42fce44e11f4dab8de99f8488281f8.jpg",
         "4"
        ],
        [
         "478",
         "a0133654f3911e47f021da4e49d42926.jpg",
         "4"
        ],
        [
         "801",
         "95ea40dc6c0e81407e13d70bd52a5c95.jpg",
         "1"
        ],
        [
         "709",
         "dbca6ab4ebf827884073e2890388b744.jpg",
         "0"
        ],
        [
         "131",
         "de911421573d3e234450040374b55b38.jpg",
         "5"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>fb42fce44e11f4dab8de99f8488281f8.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>a0133654f3911e47f021da4e49d42926.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>95ea40dc6c0e81407e13d70bd52a5c95.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>dbca6ab4ebf827884073e2890388b744.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>de911421573d3e234450040374b55b38.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "927  fb42fce44e11f4dab8de99f8488281f8.jpg      4\n",
       "478  a0133654f3911e47f021da4e49d42926.jpg      4\n",
       "801  95ea40dc6c0e81407e13d70bd52a5c95.jpg      1\n",
       "709  dbca6ab4ebf827884073e2890388b744.jpg      0\n",
       "131  de911421573d3e234450040374b55b38.jpg      5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8012a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block DataLoader\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing image file names and labels.\n",
    "            image_dir (str): Directory where images are stored.\n",
    "            processor (AutoImageProcessor, optional):  Hugging Face processor for image preprocessing. Defaults to None.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])  # Assuming image file names are in the first column\n",
    "        image = Image.open(img_name).convert('RGB')  # Ensure consistent color format\n",
    "\n",
    "        label = self.dataframe.iloc[idx, 1]  # Assuming labels are in the second column\n",
    "\n",
    "        if self.processor:\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            image = inputs['pixel_values'].squeeze()  # Remove batch dimension\n",
    "        elif self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116edde",
   "metadata": {},
   "source": [
    "# **EXPERIMENTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48664e75",
   "metadata": {},
   "source": [
    "## **Google VIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d242b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Define the model\n",
    "model = AutoModelForImageClassification.from_pretrained(model_card, num_labels=n_classes, trust_remote_code=True)\n",
    "\n",
    "# Define the image processor\n",
    "processor = AutoImageProcessor.from_pretrained(model_card)\n",
    "\n",
    "# Prepare the dataloaders for training, validation and testing\n",
    "dataset = ImageDataset(dataframe=train, image_dir=IMAGE_FOLDER, processor=processor)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataset = ImageDataset(dataframe=val, image_dir=IMAGE_FOLDER, processor=processor)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "dataset = ImageDataset(dataframe=test, image_dir=IMAGE_FOLDER, processor=processor)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8edb3b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(experiment_id: str, test_loader: DataLoader, criterion: nn.Module, writer: SummaryWriter = None):\n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(ARTIFACTS_FOLDER / f'{experiment_id}.pth'))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the test metrics\n",
    "    test_loss, correct_test, total_test = .0, 0, 0\n",
    "    running_test_steps = 0\n",
    "    running_test_time_by_step = .0\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            step_time = time()\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(inputs).logits\n",
    "            test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct_test += (preds == labels).sum().item()\n",
    "            total_test += inputs.size(0)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            running_test_time_by_step += time() - step_time\n",
    "            running_test_steps += 1\n",
    "            if writer:\n",
    "                writer.add_scalar('TimingByStep/test', running_test_time_by_step, running_test_steps)\n",
    "\n",
    "    epoch_test_loss = test_loss / total_test\n",
    "    epoch_test_acc  = correct_test / total_test\n",
    "    if writer:\n",
    "        writer.add_scalar('Accuracy/test', epoch_test_acc, 0)\n",
    "\n",
    "    print(f\"Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_acc:.4f}\")\n",
    "    \n",
    "    # Save Classification report in tensorboard\n",
    "    if writer:\n",
    "        writer.add_text('ClassificationReport/test', classification_report(y_true, y_pred, target_names=classes), 0)\n",
    "    \n",
    "    # Delete the model from GPU memory\n",
    "    del model, inputs, labels, outputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_amfic(\n",
    "    model: AutoModelForImageClassification,\n",
    "    model_card: str,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    num_epochs: int,):\n",
    "    # --- LOOP ---\n",
    "    # Initialize SummaryWriter\n",
    "    experiment_id = \"_\".join([datetime.now().strftime(\"%Y%m%d-%H%M%S\"), model_card.split(\"/\")[-1]])\n",
    "    log_dir = ROOT_FOLDER / os.getcwd().split(\"/\")[-1] / \"runs\" / \"_\".join([experiment_id, model_card.split(\"/\")[-1]])\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # Initialize commun metrics\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Initialize the training metrics\n",
    "    running_train_time_by_step = .0\n",
    "    running_train_time_by_epoch = .0\n",
    "    running_train_steps = 0\n",
    "    writer.add_scalar('TimingByEpoch/train', running_train_time_by_epoch, 0)\n",
    "\n",
    "    # Initialize the validation metrics\n",
    "    best_val_metric = float('-inf')\n",
    "    running_val_time_by_step = .0\n",
    "    running_val_time_by_epoch = .0\n",
    "    running_val_steps = 0\n",
    "    writer.add_scalar('TimingByEpoch/validation', running_val_time_by_epoch, 0)\n",
    "    \n",
    "    writer.add_scalar('TimingByEpoch/train', running_train_time_by_epoch, 0)\n",
    "    # Move model to the device\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- 1. TRAINING LOOP ---\n",
    "        model.train()\n",
    "        train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "        epoch_time = time()\n",
    "        for i, (inputs, labels) in enumerate(train_loader): # Use enumerate for step count\n",
    "            step_time = time()\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            preds_train = outputs.argmax(dim=1)\n",
    "            correct_train += (preds_train == labels).sum().item()\n",
    "            total_train += inputs.size(0)\n",
    "            running_train_steps += 1\n",
    "            running_train_time_by_step += time() - step_time\n",
    "            writer.add_scalar('TimingByStep/train', running_train_time_by_step, running_train_steps)\n",
    "\n",
    "        running_train_time_by_epoch += time() - epoch_time\n",
    "        epoch_train_loss = train_loss / total_train\n",
    "        epoch_train_acc = correct_train / total_train\n",
    "        # Log training metrics per epoch\n",
    "        writer.add_scalar('Loss/train', epoch_train_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', epoch_train_acc, epoch)\n",
    "        writer.add_scalar('TimingByEpoch/train', running_train_time_by_epoch, epoch)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}\")\n",
    "\n",
    "\n",
    "        # --- 2. VALIDATION LOOP ---\n",
    "        model.eval()\n",
    "        val_loss, correct_val, total_val = .0, 0, 0\n",
    "        epoch_time = time()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move images and labels to the device\n",
    "                step_time = time()\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                outputs = model(inputs).logits\n",
    "                val_loss   += criterion(outputs, labels).item() * inputs.size(0)\n",
    "                preds      = outputs.argmax(dim=1)\n",
    "                correct_val   += (preds == labels).sum().item()\n",
    "                total_val     += inputs.size(0)\n",
    "                running_val_steps += 1\n",
    "                running_val_time_by_step += time() - step_time\n",
    "                writer.add_scalar('TimingByStep/validation', running_val_time_by_step, running_val_steps)\n",
    "\n",
    "        running_val_time_by_epoch += time() - epoch_time\n",
    "        epoch_val_loss = val_loss / total_val\n",
    "        epoch_val_acc  = correct_val / total_val\n",
    "        # Log validation metrics per epoch\n",
    "        writer.add_scalar('Loss/validation', epoch_val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/validation', epoch_val_acc, epoch)\n",
    "        writer.add_scalar('TimingByEpoch/validation', running_val_time_by_epoch, epoch)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "        # --- 3. UPDATE BEST MODEL ---\n",
    "        # Save the model if the validation accuracy is better than the best one\n",
    "        if epoch_val_acc > best_val_metric:\n",
    "            best_val_metric = epoch_val_acc\n",
    "            best_epoch      = epoch\n",
    "            torch.save(model.state_dict(), ARTIFACTS_FOLDER / f'{experiment_id}.pth')\n",
    "            print(f\"Best model updated at epoch {best_epoch} with val acc: {best_val_metric:.4f}\")\n",
    "    \n",
    "    # Delete the model from GPU memory\n",
    "    del model, inputs, labels, outputs, optimizer, loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- 4. TESTING LOOP ---\n",
    "    # TODO: Implement the testing loop with an external function\n",
    "    test_model(experiment_id, test_loader, criterion, writer)\n",
    "\n",
    "    # --- After the training loop ---\n",
    "    writer.close() # Close the writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b4e2e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5], Train Loss: 1.9082, Train Acc: 0.3281\n",
      "Epoch [0/5], Val Loss: 1.9013, Val Acc: 0.2969\n",
      "Best model updated at epoch 0 with val acc: 0.2969\n",
      "Epoch [1/5], Train Loss: 1.7952, Train Acc: 0.6328\n",
      "Epoch [1/5], Val Loss: 1.8593, Val Acc: 0.4062\n",
      "Best model updated at epoch 1 with val acc: 0.4062\n",
      "Epoch [2/5], Train Loss: 1.6945, Train Acc: 0.7969\n",
      "Epoch [2/5], Val Loss: 1.8126, Val Acc: 0.4766\n",
      "Best model updated at epoch 2 with val acc: 0.4766\n",
      "Epoch [3/5], Train Loss: 1.5821, Train Acc: 0.8906\n",
      "Epoch [3/5], Val Loss: 1.7655, Val Acc: 0.5469\n",
      "Best model updated at epoch 3 with val acc: 0.5469\n",
      "Epoch [4/5], Train Loss: 1.4698, Train Acc: 0.9375\n",
      "Epoch [4/5], Val Loss: 1.7173, Val Acc: 0.5469\n",
      "Test Loss: 1.7690, Test Acc: 0.5391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# TODO : Ajouter une card dans le tensorboard pour les paramètres du modèle\n",
    "# TODO : Retester le problème de mémoire GPU ?\n",
    "# TODO : warning PIL decompression bomb warning\n",
    "# TODO : Add a function to plot the confusion matrix\n",
    "# TODO : Add a function to plot the classification report\n",
    "\n",
    "# Set the training parameters\n",
    "num_epochs = 5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Run the training workflow for AutoModelForImageClassification\n",
    "train_and_eval_amfic(\n",
    "    model = model,\n",
    "    model_card = model_card,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    test_loader = test_loader,\n",
    "    optimizer = optimizer,\n",
    "    criterion = criterion,\n",
    "    num_epochs = num_epochs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29d623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.9417\n",
      "Epoch [1/3], Val Loss: 1.9310, Val Acc: 0.1250\n",
      "Best model updated at epoch 1 with val acc: 0.1250\n",
      "Epoch [2/3], Loss: 1.8607\n",
      "Epoch [2/3], Val Loss: 1.9246, Val Acc: 0.1562\n",
      "Best model updated at epoch 2 with val acc: 0.1562\n",
      "Epoch [3/3], Loss: 1.7995\n",
      "Epoch [3/3], Val Loss: 1.9185, Val Acc: 0.1875\n",
      "Best model updated at epoch 3 with val acc: 0.1875\n"
     ]
    }
   ],
   "source": [
    "best_val_metric = float('-inf')\n",
    "best_epoch = 0\n",
    "\n",
    "# Move model to the device\n",
    "model.to(device)\n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# Set the number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "# Set the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # --- 1. TRAINING LOOP ---\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move images and labels to the device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # --- 2. VALIDATION LOOP ---\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            # Move images and labels to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs).logits\n",
    "            val_loss   += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            preds      = outputs.argmax(dim=1)\n",
    "            correct   += (preds == labels).sum().item()\n",
    "            total     += inputs.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / total\n",
    "    val_acc      = correct / total\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # --- 3. UPDATE BEST MODEL ---\n",
    "    if val_acc > best_val_metric:\n",
    "        best_val_metric = val_acc\n",
    "        best_epoch      = epoch\n",
    "        torch.save(model.state_dict(), ARTIFACTS_FOLDER / f'best_{model_card.split(\"/\")[-1].replace(\"-\", \"_\")}.pth')\n",
    "    print(f\"Best model updated at epoch {best_epoch + 1} with val acc: {best_val_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44467fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 1.5189 seconds\n",
      "Test Loss: 1.9254, Test Acc: 0.1875\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(ARTIFACTS_FOLDER / f'best_{model_card.split(\"/\")[-1].replace(\"-\", \"_\")}.pth'))\n",
    "\n",
    "# --- 4. TESTING LOOP ---\n",
    "model.eval()\n",
    "test_loss, correct, total = 0.0, 0, 0\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Compute the inference time\n",
    "start_time = time()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move images and labels to the device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs).logits\n",
    "        test_loss   += criterion(outputs, labels).item() * inputs.size(0)\n",
    "        preds      = outputs.argmax(dim=1)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        correct   += (preds == labels).sum().item()\n",
    "        total     += inputs.size(0)\n",
    "end_time = time()\n",
    "# Compute the inference time\n",
    "inference_time = end_time - start_time\n",
    "print(f\"Inference time: {inference_time:.4f} seconds\")\n",
    "# Compute the average test loss and accuracy\n",
    "avg_test_loss = test_loss / total\n",
    "test_acc      = correct / total\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8793fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: Predicted  0  1  2  3  4  5  6  All\n",
      "True                               \n",
      "0          2  2  1  1  0  0  1    7\n",
      "1          0  1  0  0  0  1  0    2\n",
      "2          0  0  2  2  0  0  1    5\n",
      "3          3  0  1  0  1  0  1    6\n",
      "4          1  2  0  0  0  2  0    5\n",
      "5          0  1  0  0  0  1  1    3\n",
      "6          2  2  0  0  0  0  0    4\n",
      "All        8  8  4  3  1  4  4   32\n"
     ]
    }
   ],
   "source": [
    "# Compute the confusion matrix\n",
    "print(f\"Confusion matrix: {pd.crosstab(pd.Series(y_true), pd.Series(y_pred), rownames=['True'], colnames=['Predicted'], margins=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ec95a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                 Baby Care       0.25      0.29      0.27         7\n",
      "  Beauty and Personal Care       0.12      0.50      0.20         2\n",
      "                 Computers       0.50      0.40      0.44         5\n",
      "Home Decor & Festive Needs       0.00      0.00      0.00         6\n",
      "           Home Furnishing       0.00      0.00      0.00         5\n",
      "          Kitchen & Dining       0.25      0.33      0.29         3\n",
      "                   Watches       0.00      0.00      0.00         4\n",
      "\n",
      "                  accuracy                           0.19        32\n",
      "                 macro avg       0.16      0.22      0.17        32\n",
      "              weighted avg       0.16      0.19      0.17        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=classes, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea071149",
   "metadata": {},
   "source": [
    "## MOBILENETV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"google/mobilenet_v2_1.0_224\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e274bf",
   "metadata": {},
   "source": [
    "## **MAMBA S 1 K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b494483c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the averaged pool features: torch.Size([1, 768])\n",
      "Number of stages in extracted features: 4\n",
      "Size of extracted features in stage 1: torch.Size([1, 96, 56, 56])\n",
      "Size of extracted features in stage 4: torch.Size([1, 768, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"nvidia/MambaVision-S-1K\", trust_remote_code=True)\n",
    "\n",
    "# eval mode for inference\n",
    "model.cuda().eval()\n",
    "\n",
    "# prepare image for the model\n",
    "url = 'http://images.cocodataset.org/val2017/000000020247.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=False,\n",
    "                             mean=model.config.mean,\n",
    "                             std=model.config.std,\n",
    "                             crop_mode=model.config.crop_mode,\n",
    "                             crop_pct=model.config.crop_pct)\n",
    "inputs = transform(image).unsqueeze(0).cuda()\n",
    "# model inference\n",
    "out_avg_pool, features = model(inputs)\n",
    "print(\"Size of the averaged pool features:\", out_avg_pool.size())  # torch.Size([1, 768])\n",
    "print(\"Number of stages in extracted features:\", len(features)) # 4 stages\n",
    "print(\"Size of extracted features in stage 1:\", features[0].size()) # torch.Size([1, 96, 56, 56])\n",
    "print(\"Size of extracted features in stage 4:\", features[3].size()) # torch.Size([1, 768, 7, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0ad1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "\n",
    "class MambaClassifier(nn.Module):\n",
    "    def __init__(self, backbone, num_classes, hidden_dim=768): # Added hidden_dim\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.config = self.backbone.config\n",
    "        # self.classifier = nn.Linear(640, num_classes) # Original classifier\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes) # Modified classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_avg_pool, _ = self.backbone(x)\n",
    "        logits = self.classifier(out_avg_pool)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "933fb051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilités prédites: tensor([[0.1360, 0.1641, 0.1267, 0.1375, 0.1245, 0.1609, 0.1504]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mamba = MambaClassifier(model, num_classes=num_classes).cuda().eval()\n",
    "\n",
    "image = Image.open(img_test)\n",
    "\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=False,\n",
    "                             mean=mamba.config.mean,\n",
    "                             std=mamba.config.std,\n",
    "                             crop_mode=mamba.config.crop_mode,\n",
    "                             crop_pct=mamba.config.crop_pct)\n",
    "\n",
    "\n",
    "inputs = transform(image).unsqueeze(0).cuda()\n",
    "# Prédiction\n",
    "with torch.no_grad():\n",
    "    logits = mamba(inputs)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "print(\"Probabilités prédites:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5380a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaClassifier(AutoModel.from_pretrained(\"nvidia/MambaVision-S-1K\", trust_remote_code=True), num_classes=num_classes).cuda().train()\n",
    "\n",
    "image = Image.open(img_test)\n",
    "\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=True,\n",
    "                             mean=mamba.config.mean,\n",
    "                             std=mamba.config.std,\n",
    "                             crop_mode=mamba.config.crop_mode,\n",
    "                             crop_pct=mamba.config.crop_pct)\n",
    "\n",
    "dataset = ImageDataset(dataframe=train, image_dir=image_dir, transform=transform)\n",
    "\n",
    "# 5. Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b5f97d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.9875\n",
      "Epoch [2/3], Loss: 1.9445\n",
      "Epoch [3/3], Loss: 1.9724\n"
     ]
    }
   ],
   "source": [
    "# Training loop for the model with the train dataloader\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move model to the device\n",
    "model.to(device)\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# Set the number of epochs\n",
    "num_epochs = 3\n",
    "# Set the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in dataloader:\n",
    "        # Move images and labels to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # inputs = transform(image).unsqueeze(0).cuda()\n",
    "        # model inference\n",
    "        # outputs = model(inputs)\n",
    "        # end_time = time()\n",
    "        # logits = outputs[\"logits\"]\n",
    "        # predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "# Save the model\n",
    "# model.save_pretrained(\"mamba_model\")\n",
    "# processor.save_pretrained(\"mamba_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6d9d5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           Home Furnishing       0.00      0.00      0.00         4\n",
      "                 Baby Care       0.00      0.00      0.00         5\n",
      "                   Watches       0.00      0.00      0.00         4\n",
      "Home Decor & Festive Needs       0.00      0.00      0.00         6\n",
      "          Kitchen & Dining       0.40      0.40      0.40         5\n",
      "  Beauty and Personal Care       0.10      0.20      0.13         5\n",
      "                 Computers       0.00      0.00      0.00         3\n",
      "\n",
      "                  accuracy                           0.09        32\n",
      "                 macro avg       0.07      0.09      0.08        32\n",
      "              weighted avg       0.08      0.09      0.08        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference on test set\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Initialize the test dataset\n",
    "test_dataset = ImageDataset(dataframe=test, image_dir=image_dir, transform=transform)\n",
    "# Initialize the test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "# Initialize the predictions list\n",
    "predictions = []\n",
    "# Initialize the labels list\n",
    "labels_list = []\n",
    "# Inference loop\n",
    "for images, labels in test_dataloader:\n",
    "    # Move images to the device\n",
    "    images = images.to(device)\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    # Get the predicted labels\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    # Append the predictions and labels to the lists\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    labels_list.extend(labels.cpu().numpy())\n",
    "# Convert the predictions and labels to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "labels_list = np.array(labels_list)\n",
    "# Print the classification report\n",
    "print(classification_report(labels_list, predictions, target_names=classes))\n",
    "# Save the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da230c",
   "metadata": {},
   "source": [
    "## **MAMBA B 21 K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6d6291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaClassifier(AutoModel.from_pretrained(\"nvidia/MambaVision-B-21K\", trust_remote_code=True), num_classes=num_classes, hidden_dim=1024).cuda().train()\n",
    "\n",
    "image = Image.open(img_test)\n",
    "\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=True,\n",
    "                             mean=mamba.config.mean,\n",
    "                             std=mamba.config.std,\n",
    "                             crop_mode=mamba.config.crop_mode,\n",
    "                             crop_pct=mamba.config.crop_pct)\n",
    "\n",
    "dataset = ImageDataset(dataframe=train, image_dir=image_dir, transform=transform)\n",
    "\n",
    "# 5. Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbe35508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 2.1026\n",
      "Epoch [2/3], Loss: 2.0404\n",
      "Epoch [3/3], Loss: 1.9177\n"
     ]
    }
   ],
   "source": [
    "# Training loop for the model with the train dataloader\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move model to the device\n",
    "model.to(device)\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# Set the number of epochs\n",
    "num_epochs = 3\n",
    "# Set the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in dataloader:\n",
    "        # Move images and labels to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # inputs = transform(image).unsqueeze(0).cuda()\n",
    "        # model inference\n",
    "        # outputs = model(inputs)\n",
    "        # end_time = time()\n",
    "        # logits = outputs[\"logits\"]\n",
    "        # predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "# Save the model\n",
    "# model.save_pretrained(\"mamba_model\")\n",
    "# processor.save_pretrained(\"mamba_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23fb4b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           Home Furnishing       0.00      0.00      0.00         4\n",
      "                 Baby Care       0.50      0.40      0.44         5\n",
      "                   Watches       0.17      0.25      0.20         4\n",
      "Home Decor & Festive Needs       0.20      0.17      0.18         6\n",
      "          Kitchen & Dining       0.25      0.20      0.22         5\n",
      "  Beauty and Personal Care       0.25      0.20      0.22         5\n",
      "                 Computers       0.00      0.00      0.00         3\n",
      "\n",
      "                  accuracy                           0.19        32\n",
      "                 macro avg       0.20      0.17      0.18        32\n",
      "              weighted avg       0.21      0.19      0.20        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference on test set\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Initialize the test dataset\n",
    "test_dataset = ImageDataset(dataframe=test, image_dir=image_dir, transform=transform)\n",
    "# Initialize the test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "# Initialize the predictions list\n",
    "predictions = []\n",
    "# Initialize the labels list\n",
    "labels_list = []\n",
    "# Inference loop\n",
    "for images, labels in test_dataloader:\n",
    "    # Move images to the device\n",
    "    images = images.to(device)\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    # Get the predicted labels\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    # Append the predictions and labels to the lists\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    labels_list.extend(labels.cpu().numpy())\n",
    "# Convert the predictions and labels to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "labels_list = np.array(labels_list)\n",
    "# Print the classification report\n",
    "print(classification_report(labels_list, predictions, target_names=classes))\n",
    "# Save the predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
