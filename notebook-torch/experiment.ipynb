{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc430ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL\n",
    "# TODO : Compléter toutes les parties en MARKDOWN\n",
    "# TODO : IF NEEDED créer une data aug avec albumentations\n",
    "# TODO : Lire l'extension devcontainers\n",
    "\n",
    "# EXPERIMENTS\n",
    "# TODO : Ajouter une card dans le tensorboard pour les paramètres du modèle\n",
    "# TODO : warning PIL decompression bomb warning ,\n",
    "# TODO : Tester le batch size de 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b170e",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "#TODO Ce notebook présente une analyse de données sur les ventes de jeux vidéo. L'objectif est d'explorer les tendances des ventes en fonction de la plateforme, du genre et de la région."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccda75",
   "metadata": {},
   "source": [
    "# PRE-REQUIS\n",
    "\n",
    "Ce bloc contient tout ce qui est nécessaire pour le fonctionnement des expériences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e97c4",
   "metadata": {},
   "source": [
    "## Imports & Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9aa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor, AutoModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from constants import ROOT_FOLDER, IMAGE_FOLDER, ARTIFACTS_FOLDER, DATASET_PATH\n",
    "from constants import SEED, VAL_SIZE, TEST_SIZE, BATCH_SIZE, SAMPLING, INPUT_RESOLUTION\n",
    "from constants import MAMBA_HIDDEN_SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad8ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion des avertissements\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "651be0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  2.6.0+cu124\n",
      "Cuda version:  12.4\n",
      "CUDNN version:  90100\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration de cuda avec PyTorch\n",
    "print(\"Torch version: \", torch.__version__)\n",
    "print(\"Cuda version: \", torch.version.cuda)\n",
    "print(\"CUDNN version: \", torch.backends.cudnn.version())\n",
    "\n",
    "# Set the device to GPU if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab3639",
   "metadata": {},
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64198e15",
   "metadata": {},
   "source": [
    "### Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f68941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1050, 2)\n",
      "Dataset columns: Index(['image', 'class'], dtype='object')\n",
      "Number of classes: 7\n",
      "Classes: ['Baby Care', 'Beauty and Personal Care', 'Computers', 'Home Decor & Festive Needs', 'Home Furnishing', 'Kitchen & Dining', 'Watches']\n"
     ]
    }
   ],
   "source": [
    "# Loading the pickle dataset_cleaned used with the previous project as a pandas df\n",
    "df = pd.read_pickle(DATASET_PATH).drop(columns=[\"product_name\", \"description\"])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Dataset columns: {df.columns}\")\n",
    "\n",
    "# Encode the labels with LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df[\"class\"])\n",
    "N_CLASSES = len(le.classes_)\n",
    "CLASSES = le.classes_.tolist()\n",
    "print(f\"Number of classes: {N_CLASSES}\")\n",
    "print(f\"Classes: {CLASSES}\")\n",
    "\n",
    "# Finally transform the class column to the encoded labels\n",
    "df[\"class\"] = le.transform(df[\"class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a8b51",
   "metadata": {},
   "source": [
    "### Séparation des données (train/validation/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91d9afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (32, 2)\n",
      "Val shape: (32, 2)\n",
      "Test shape: (32, 2)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the datasets into train, val and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    df[\"image\"],\n",
    "    df[\"class\"],\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"class\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=VAL_SIZE, random_state=SEED, stratify=y_temp, shuffle=True\n",
    ")\n",
    "\n",
    "# Concat X and y for each set\n",
    "train = (\n",
    "    pd.concat([X_train, y_train], axis=1).sample(SAMPLING)\n",
    "    if SAMPLING\n",
    "    else pd.concat([X_train, y_train], axis=1)\n",
    ")\n",
    "val = (\n",
    "    pd.concat([X_val, y_val], axis=1).sample(SAMPLING)\n",
    "    if SAMPLING\n",
    "    else pd.concat([X_val, y_val], axis=1)\n",
    ")\n",
    "test = (\n",
    "    pd.concat([X_test, y_test], axis=1).sample(SAMPLING)\n",
    "    if SAMPLING\n",
    "    else pd.concat([X_test, y_test], axis=1)\n",
    ")\n",
    "\n",
    "# Print the shape of each set\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Val shape: {val.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08d4ee5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "42e51970-4b69-4b1f-8917-47110f0f7fc2",
       "rows": [
        [
         "301",
         "23704dd51c975e845c574b044aae0a9f.jpg",
         "1"
        ],
        [
         "1033",
         "ace154420a51fad090b3543995630051.jpg",
         "4"
        ],
        [
         "200",
         "168618e93387ad7171d4e4e1eeff9d1a.jpg",
         "3"
        ],
        [
         "880",
         "fc3eb6ffed257270c26943e9f9c347b9.jpg",
         "4"
        ],
        [
         "123",
         "5c77aa1fc09901ae07c392c152a70e41.jpg",
         "3"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>23704dd51c975e845c574b044aae0a9f.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>ace154420a51fad090b3543995630051.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>168618e93387ad7171d4e4e1eeff9d1a.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>fc3eb6ffed257270c26943e9f9c347b9.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>5c77aa1fc09901ae07c392c152a70e41.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     image  class\n",
       "301   23704dd51c975e845c574b044aae0a9f.jpg      1\n",
       "1033  ace154420a51fad090b3543995630051.jpg      4\n",
       "200   168618e93387ad7171d4e4e1eeff9d1a.jpg      3\n",
       "880   fc3eb6ffed257270c26943e9f9c347b9.jpg      4\n",
       "123   5c77aa1fc09901ae07c392c152a70e41.jpg      3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4eede8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "077ad87b-d206-45ba-9eba-2bb05b5be90d",
       "rows": [
        [
         "1036",
         "5a54c78b52c984e56500809e5bc27ae1.jpg",
         "2"
        ],
        [
         "661",
         "8c90f3ca64ea1a8ce104c3c3f5fc173f.jpg",
         "3"
        ],
        [
         "297",
         "6ed34e1ab886e8a702ec59dd66ba4dff.jpg",
         "0"
        ],
        [
         "429",
         "9fb8662af03c957ade34d4c816d4e903.jpg",
         "2"
        ],
        [
         "773",
         "109e235d4838002246599f987d935c21.jpg",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>5a54c78b52c984e56500809e5bc27ae1.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>8c90f3ca64ea1a8ce104c3c3f5fc173f.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>6ed34e1ab886e8a702ec59dd66ba4dff.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>9fb8662af03c957ade34d4c816d4e903.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>109e235d4838002246599f987d935c21.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     image  class\n",
       "1036  5a54c78b52c984e56500809e5bc27ae1.jpg      2\n",
       "661   8c90f3ca64ea1a8ce104c3c3f5fc173f.jpg      3\n",
       "297   6ed34e1ab886e8a702ec59dd66ba4dff.jpg      0\n",
       "429   9fb8662af03c957ade34d4c816d4e903.jpg      2\n",
       "773   109e235d4838002246599f987d935c21.jpg      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ba2b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b57d29b6-a073-47f6-abb6-f249d0cdec43",
       "rows": [
        [
         "403",
         "b1c7aa8e0f50e7ee73ed30d12af1a961.jpg",
         "2"
        ],
        [
         "682",
         "91795f4b0e4aac27314477a91b63bfb7.jpg",
         "1"
        ],
        [
         "583",
         "35a68b44bef47a809314493d28535b9d.jpg",
         "6"
        ],
        [
         "709",
         "dbca6ab4ebf827884073e2890388b744.jpg",
         "0"
        ],
        [
         "869",
         "d218c32df572e82db50faecd62179db2.jpg",
         "4"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>b1c7aa8e0f50e7ee73ed30d12af1a961.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>91795f4b0e4aac27314477a91b63bfb7.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>35a68b44bef47a809314493d28535b9d.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>dbca6ab4ebf827884073e2890388b744.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>d218c32df572e82db50faecd62179db2.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "403  b1c7aa8e0f50e7ee73ed30d12af1a961.jpg      2\n",
       "682  91795f4b0e4aac27314477a91b63bfb7.jpg      1\n",
       "583  35a68b44bef47a809314493d28535b9d.jpg      6\n",
       "709  dbca6ab4ebf827884073e2890388b744.jpg      0\n",
       "869  d218c32df572e82db50faecd62179db2.jpg      4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116edde",
   "metadata": {},
   "source": [
    "## Classes et Fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028814d7",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "[ ] ***TODO*** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8012a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block DataLoader\n",
    "from pathlib import PosixPath\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe: pd.DataFrame,\n",
    "        image_dir: PosixPath,\n",
    "        processor: AutoImageProcessor = None,\n",
    "        transform: callable = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing image file names and labels.\n",
    "            image_dir (PosixPath): Directory where images are stored.\n",
    "            processor (AutoImageProcessor, optional): Hugging Face processor for image preprocessing. Defaults to None.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(\n",
    "            self.image_dir, self.dataframe.iloc[idx, 0]\n",
    "        )  # Assuming image file names are in the first column\n",
    "        image = Image.open(img_name).convert(\"RGB\")  # Ensure consistent color format\n",
    "\n",
    "        label = self.dataframe.iloc[idx, 1]  # Assuming labels are in the second column\n",
    "\n",
    "        if self.processor:\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            image = inputs[\"pixel_values\"].squeeze()  # Remove batch dimension\n",
    "        elif self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc9e31",
   "metadata": {},
   "source": [
    "### MambaClassifier\n",
    "\n",
    "[ ] ***TODO*** : L'ajout de la classe MambaClassifier permet de créer un classificateur basé sur le modèle MambaVision en tenant compte de ses spécificités. Cette classe hérite de la classe `nn.Module` de PyTorch et encapsule le modèle MambaVision, permettant ainsi de l'utiliser comme un classificateur dans le cadre d'une tâche de classification d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "971655ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block MambaClassifier\n",
    "class MambaClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: AutoModel,\n",
    "        num_classes: int,\n",
    "        hidden_dim: int,  # The hidden dimension of the backbone is stored in the MAMBA_HIDDEN_SIZES dict with the model card as the key\n",
    "        fc_layer: int = None,  # Add the int number of layers before the classifier\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.config = self.backbone.config\n",
    "        if fc_layer:\n",
    "            self.fc_layers = nn.ModuleList()\n",
    "            for i in range(fc_layer):\n",
    "                if i == 0:\n",
    "                    self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                else:\n",
    "                    self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                self.fc_layers.append(nn.ReLU())\n",
    "                self.fc_layers.append(nn.Dropout(0.1))\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def create_transform(self, training: bool, auto_augment=None):\n",
    "        transform = create_transform(\n",
    "            input_size=INPUT_RESOLUTION,\n",
    "            is_training=training,  # Add a ColorJitter augmentation during training\n",
    "            mean=self.config.mean,\n",
    "            std=self.config.std,\n",
    "            crop_mode=self.config.crop_mode,\n",
    "            crop_pct=self.config.crop_pct,\n",
    "            auto_augment=auto_augment, # \"rand-m9-mstd0.5-inc1\"\n",
    "            )\n",
    "        return transform\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_avg_pool, _ = self.backbone(x)\n",
    "        if hasattr(self, \"fc_layers\"):\n",
    "            for layer in self.fc_layers:\n",
    "                out_avg_pool = layer(out_avg_pool)\n",
    "        logits = self.classifier(out_avg_pool)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5bc876",
   "metadata": {},
   "source": [
    "### TorchPipeline\n",
    "\n",
    "[ ] ***TODO*** : Un pipeline de traitement des données est créé pour gérer les transformations d'images et les normalisations nécessaires avant de passer les données au modèle. Ce pipeline utilise la bibliothèque `torchvision` pour appliquer des transformations telles que le redimensionnement, le recadrage, la conversion en tenseur et la normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44510476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchPipeline:\n",
    "    def __init__(self, model_card, model, processor=None, train_transform=None, test_transform=None):\n",
    "        # Initialize attributes from parameters\n",
    "        self.model_card = model_card\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.train_transform = train_transform\n",
    "        self.test_transform = test_transform\n",
    "\n",
    "        # Check if the model is a MambaClassifier instance\n",
    "        self.mamba = \"MambaClassifier\" in type(model).__name__\n",
    "\n",
    "        # Initialize FIXED attributes from constants.py\n",
    "        self.device = DEVICE\n",
    "        self.root_folder = ROOT_FOLDER\n",
    "        self.artifacts_folder = ARTIFACTS_FOLDER\n",
    "        self.dataset_path = DATASET_PATH\n",
    "        self.image_folder = IMAGE_FOLDER\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.classes = CLASSES\n",
    "        self.n_classes = N_CLASSES\n",
    "\n",
    "        # Initialize empty attributes\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.test_loader = None\n",
    "        self.writer = None\n",
    "        self.experiment_id = None\n",
    "\n",
    "    @staticmethod\n",
    "    def classification_report_to_markdown(y_true, y_pred, target_names):\n",
    "        \"\"\"\n",
    "        Convert the classification report to a markdown table.\n",
    "        \"\"\"\n",
    "        report = classification_report(\n",
    "            y_true, y_pred, target_names=target_names, zero_division=0, output_dict=True\n",
    "        )\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_df = report_df.drop(columns=[\"support\"])\n",
    "        report_df = report_df.rename_axis(\"Classes\").reset_index()\n",
    "        return report_df.to_markdown(index=False)\n",
    "\n",
    "    def generate_experiment_id(self):\n",
    "        \"\"\"\n",
    "        Generate a unique experiment ID based on the current date and time.\n",
    "        \"\"\"\n",
    "        return \"_\".join(\n",
    "            [datetime.now().strftime(\"%Y%m%d-%H%M%S\"), self.model_card.split(\"/\")[-1]]\n",
    "        )\n",
    "\n",
    "    # Load the data, apply the processor and transform, and create the dataloaders\n",
    "    def generate_dataloader(self, train, val, test):\n",
    "        # Apply the processor and transform\n",
    "        train_dataset = ImageDataset(dataframe=train, image_dir=self.image_folder, processor=self.processor, transform=self.train_transform)\n",
    "        val_dataset = ImageDataset(dataframe=val, image_dir=self.image_folder, processor=self.processor, transform=self.test_transform)\n",
    "        test_dataset = ImageDataset(dataframe=test, image_dir=self.image_folder, processor=self.processor, transform=self.test_transform)\n",
    "\n",
    "        # Create the dataloaders\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "    def train_and_eval_model(self, criterion, optimizer, num_epochs):\n",
    "        # Generate the experiment ID\n",
    "        self.experiment_id = self.generate_experiment_id()\n",
    "        # Create the writer\n",
    "        log_dir = (\n",
    "            ROOT_FOLDER\n",
    "            # TODO: Delete if OK / os.getcwd().split(\"/\")[-1]\n",
    "            / \"runs\"\n",
    "            / self.experiment_id\n",
    "        )\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "\n",
    "        # Initialize the training metrics\n",
    "        running_train_time_by_step = 0.0\n",
    "        running_train_time_by_epoch = 0.0\n",
    "        running_train_steps = 0\n",
    "        self.writer.add_scalar(\"TimingByEpoch/train\", running_train_time_by_epoch, 0)\n",
    "\n",
    "        # Initialize the validation metrics\n",
    "        best_val_metric = float(\"-inf\")\n",
    "        running_val_time_by_step = 0.0\n",
    "        running_val_time_by_epoch = 0.0\n",
    "        running_val_steps = 0\n",
    "        self.writer.add_scalar(\"TimingByEpoch/validation\", running_val_time_by_epoch, 0)\n",
    "\n",
    "        self.writer.add_scalar(\"TimingByEpoch/train\", running_train_time_by_epoch, 0)\n",
    "        # Move model to the device\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "        # Training loop\n",
    "        print(\"TRAINING EXPERIMENT ID <{}>\".format(self.experiment_id))\n",
    "        print(\"==========================\")\n",
    "        for epoch in range(num_epochs):\n",
    "            # --- 1. TRAINING LOOP ---\n",
    "            self.model.train()\n",
    "            train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "            epoch_time = time()\n",
    "            for i, (inputs, labels) in enumerate(\n",
    "                self.train_loader\n",
    "            ):  # Use enumerate for step count\n",
    "                step_time = time()\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                # Compute the model outputs given the mamba variable\n",
    "                outputs = self.model(inputs).logits if not self.mamba else self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step_loss = loss.item() * inputs.size(0)\n",
    "                train_loss += step_loss\n",
    "                preds_train = outputs.argmax(dim=1)\n",
    "                correct_train += (preds_train == labels).sum().item()\n",
    "                total_train += inputs.size(0)\n",
    "                running_train_steps += 1\n",
    "                running_train_time_by_step += time() - step_time\n",
    "                self.writer.add_scalar(\n",
    "                    \"TimingByStep/train\", running_train_time_by_step, running_train_steps\n",
    "                )\n",
    "                self.writer.add_scalar(\"LossByStep/train\", step_loss, running_train_steps)\n",
    "\n",
    "            running_train_time_by_epoch += time() - epoch_time\n",
    "            epoch_train_loss = train_loss / total_train\n",
    "            epoch_train_acc = correct_train / total_train\n",
    "            # Log training metrics per epoch\n",
    "            self.writer.add_scalar(\"LossByEpoch/train\", epoch_train_loss, epoch)\n",
    "            self.writer.add_scalar(\"Accuracy/train\", epoch_train_acc, epoch)\n",
    "            self.writer.add_scalar(\"TimingByEpoch/train\", running_train_time_by_epoch, epoch)\n",
    "            stats = f\"Epoch [{epoch + 1}/{num_epochs}] | Train_Loss: {epoch_train_loss:.4f} | Train_Acc: {epoch_train_acc:.4f}\"\n",
    "\n",
    "            # --- 2. VALIDATION LOOP ---\n",
    "            self.model.eval()\n",
    "            val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "            epoch_time = time()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in self.val_loader:\n",
    "                    # Move images and labels to the device\n",
    "                    step_time = time()\n",
    "                    inputs = inputs.to(DEVICE)\n",
    "                    labels = labels.to(DEVICE)\n",
    "                    outputs = self.model(inputs).logits if not self.mamba else self.model(inputs)\n",
    "                    val_step_loss = criterion(outputs, labels).item() * inputs.size(0)\n",
    "                    val_loss += val_step_loss\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    correct_val += (preds == labels).sum().item()\n",
    "                    total_val += inputs.size(0)\n",
    "                    running_val_steps += 1\n",
    "                    running_val_time_by_step += time() - step_time\n",
    "                    self.writer.add_scalar(\n",
    "                        \"TimingByStep/validation\",\n",
    "                        running_val_time_by_step,\n",
    "                        running_val_steps,\n",
    "                    )\n",
    "                    self.writer.add_scalar(\n",
    "                        \"LossByStep/validation\", val_step_loss, running_train_steps\n",
    "                    )\n",
    "\n",
    "            running_val_time_by_epoch += time() - epoch_time\n",
    "            epoch_val_loss = val_loss / total_val\n",
    "            epoch_val_acc = correct_val / total_val\n",
    "            # Log validation metrics per epoch\n",
    "            self.writer.add_scalar(\"Loss/validation\", epoch_val_loss, epoch)\n",
    "            self.writer.add_scalar(\"Accuracy/validation\", epoch_val_acc, epoch)\n",
    "            self.writer.add_scalar(\"TimingByEpoch/validation\", running_val_time_by_epoch, epoch)\n",
    "            stats += f\" | Val_Loss: {epoch_val_loss:.4f} | Val_Acc: {epoch_val_acc:.4f}\"\n",
    "\n",
    "            # --- 3. UPDATE BEST MODEL ---\n",
    "            # Save the model if the validation accuracy is better than the best one\n",
    "            if epoch_val_acc > best_val_metric:\n",
    "                best_val_metric = epoch_val_acc\n",
    "                torch.save(self.model.state_dict(), ARTIFACTS_FOLDER / f\"{self.experiment_id}.pth\")\n",
    "                stats += \" -> Best model updated\"\n",
    "            # Print the stats at the end of each epoch\n",
    "            print(stats)\n",
    "         \n",
    "        # Delete the model from GPU memory\n",
    "        del inputs, labels, outputs, optimizer, criterion, loss, val_loss, step_loss, val_step_loss\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Training completed. Best validation accuracy: {best_val_metric:.4f}. Running steps training time: {running_train_time_by_epoch:.2f} s.\\n\")\n",
    "    \n",
    "    def test_model(self, criterion, with_id=None,):\n",
    "        # Case when no id has been provided\n",
    "        if with_id is None:\n",
    "            try:\n",
    "                self.model.load_state_dict(torch.load(ARTIFACTS_FOLDER / f\"{self.experiment_id}.pth\"))\n",
    "            except FileNotFoundError:\n",
    "                print(\"No id has been provided and no model has been trained yet. Train the model first before testing.\")\n",
    "                return\n",
    "        # Case when id has been provided\n",
    "        else:\n",
    "            try:\n",
    "                self.model.load_state_dict(torch.load(ARTIFACTS_FOLDER / f\"{with_id}.pth\"))\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model {with_id}.pth not found in {ARTIFACTS_FOLDER}\")\n",
    "                print(\"Please provide a valid model ID.\")\n",
    "                return\n",
    "      \n",
    "        # If no error is raised, the model is loaded successfully\n",
    "        self.model.to(DEVICE)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Initialize the test metrics\n",
    "        test_loss, correct_test, total_test = 0.0, 0, 0\n",
    "        running_test_steps = 0\n",
    "        running_test_time_by_step = 0.0\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        print(f\"TESTING EXPERIMENT ID <{with_id if with_id else self.experiment_id}>\")\n",
    "        print(\"==========================\")\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.test_loader:\n",
    "                step_time = time()\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                outputs = self.model(inputs).logits if not self.mamba else self.model(inputs)\n",
    "                test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct_test += (preds == labels).sum().item()\n",
    "                total_test += inputs.size(0)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "                running_test_time_by_step += time() - step_time\n",
    "                running_test_steps += 1\n",
    "                if self.writer:\n",
    "                    self.writer.add_scalar(\n",
    "                        \"TimingByStep/test\", running_test_time_by_step, running_test_steps\n",
    "                    )\n",
    "\n",
    "        epoch_test_loss = test_loss / total_test\n",
    "        epoch_test_acc = correct_test / total_test\n",
    "        if self.writer:\n",
    "            self.writer.add_scalar(\"Accuracy/test\", epoch_test_acc, 0)\n",
    "\n",
    "        print(f\"Test Loss: {epoch_test_loss:.4f} | Test Acc: {epoch_test_acc:.4f} | Running steps test time: {running_test_time_by_step:.2f} s.\")\n",
    "\n",
    "        # Create the Classification report\n",
    "        classification_report_md = self.classification_report_to_markdown(\n",
    "            y_true, y_pred, target_names=self.classes\n",
    "        )\n",
    "\n",
    "        # Create the confusion matrix\n",
    "        cm = ConfusionMatrixDisplay.from_predictions(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            labels=range(self.n_classes),\n",
    "            normalize=\"true\",\n",
    "            display_labels=self.classes,\n",
    "            xticks_rotation=\"vertical\",\n",
    "            cmap=plt.cm.Blues,\n",
    "        )\n",
    "        # Save the classification report as a markdown file in tensorboard\n",
    "        if self.writer:\n",
    "            self.writer.add_text(\"ClassificationReport/test\", classification_report_md, 0)\n",
    "            self.writer.add_figure(\"ConfusionMatrix/test\", cm.figure_, 0)\n",
    "            self.writer.close() # Close the writer after the end of the pipeline\n",
    "        # If no writer is provided, print the classification report & confusion matrix\n",
    "        else:\n",
    "            print(classification_report_md)\n",
    "            cm.figure_.show()\n",
    "        \n",
    "        # Delete the model from GPU memory\n",
    "        del self.model, inputs, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c3a8c",
   "metadata": {},
   "source": [
    "# EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03437169",
   "metadata": {},
   "source": [
    "## **Google VIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e82faadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Define the model\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_card, num_labels=N_CLASSES, trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Define the image processor\n",
    "processor = AutoImageProcessor.from_pretrained(model_card)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3259ffc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250502-194155_vit-base-patch16-224-in21k>\n",
      "==========================\n",
      "Epoch [1/3] | Train_Loss: 1.9381 | Train_Acc: 0.1562 | Val_Loss: 1.9423 | Val_Acc: 0.1250 -> Best model updated\n",
      "Epoch [2/3] | Train_Loss: 1.8552 | Train_Acc: 0.5312 | Val_Loss: 1.9340 | Val_Acc: 0.1562 -> Best model updated\n",
      "Epoch [3/3] | Train_Loss: 1.7899 | Train_Acc: 0.7188 | Val_Loss: 1.9253 | Val_Acc: 0.1875 -> Best model updated\n",
      "Training completed. Best validation accuracy: 0.1875. Running steps training time: 10.79 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250502-194155_vit-base-patch16-224-in21k>\n",
      "==========================\n",
      "Test Loss: 1.9142 | Test Acc: 0.2812 | Running steps test time: 0.81 s.\n"
     ]
    }
   ],
   "source": [
    "# Define the num_epochs, optimizer and criterion\n",
    "num_epochs = 3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea071149",
   "metadata": {},
   "source": [
    "## MOBILENETV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"google/mobilenet_v2_1.0_224\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e274bf",
   "metadata": {},
   "source": [
    "## **MAMBA T 1 K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"nvidia/MambaVision-T-1K\"\n",
    "\n",
    "# Define the model with the MambaClassifier class\n",
    "model = MambaClassifier(\n",
    "    AutoModel.from_pretrained(model_card, trust_remote_code=True),\n",
    "    num_classes=N_CLASSES,\n",
    "    hidden_dim=MAMBA_HIDDEN_SIZES.get(model_card),\n",
    ")\n",
    "\n",
    "# Define the image transform (processor and transform)\n",
    "train_transform = model.create_transform(training=True) # auto_augment=\"rand-m9-mstd0.5-inc1\"\n",
    "test_transform = model.create_transform(training=False)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    train_transform=train_transform,\n",
    "    test_transform=test_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b41c0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250502-195255_MambaVision-T-1K>\n",
      "==========================\n",
      "Epoch [1/3] | Train_Loss: 2.0062 | Train_Acc: 0.0625 | Val_Loss: 2.0011 | Val_Acc: 0.0312 -> Best model updated\n",
      "Epoch [2/3] | Train_Loss: 1.9974 | Train_Acc: 0.1250 | Val_Loss: 1.9874 | Val_Acc: 0.0312\n",
      "Epoch [3/3] | Train_Loss: 1.9758 | Train_Acc: 0.0625 | Val_Loss: 1.9797 | Val_Acc: 0.0625 -> Best model updated\n",
      "Training completed. Best validation accuracy: 0.0625. Running steps training time: 5.36 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250502-195255_MambaVision-T-1K>\n",
      "==========================\n",
      "Test Loss: 1.9905 | Test Acc: 0.0625 | Running steps test time: 0.30 s.\n"
     ]
    }
   ],
   "source": [
    "# Define the num_epochs, optimizer and criterion\n",
    "num_epochs = 3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da230c",
   "metadata": {},
   "source": [
    "## **MAMBA B 21 K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6d6291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = \"nvidia/MambaVision-B-21K\"\n",
    "\n",
    "# Define the model with the MambaClassifier class\n",
    "model = MambaClassifier(\n",
    "    AutoModel.from_pretrained(model_card, trust_remote_code=True),\n",
    "    num_classes=N_CLASSES,\n",
    "    hidden_dim=MAMBA_HIDDEN_SIZES.get(model_card),\n",
    ")\n",
    "\n",
    "# Define the image transform (processor and transform)\n",
    "train_transform = model.create_transform(training=True)\n",
    "test_transform = model.create_transform(training=False)\n",
    "\n",
    "# Create the TorchPipeline object\n",
    "pipeline = TorchPipeline(\n",
    "    model_card=model_card,\n",
    "    model=model,\n",
    "    train_transform=train_transform,\n",
    "    test_transform=test_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbe35508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXPERIMENT ID <20250502-194242_MambaVision-B-21K>\n",
      "==========================\n",
      "Epoch [1/3] | Train_Loss: 2.0714 | Train_Acc: 0.0312 | Val_Loss: 2.1670 | Val_Acc: 0.0312 -> Best model updated\n",
      "Epoch [2/3] | Train_Loss: 1.9613 | Train_Acc: 0.2500 | Val_Loss: 2.1341 | Val_Acc: 0.0625 -> Best model updated\n",
      "Epoch [3/3] | Train_Loss: 1.9565 | Train_Acc: 0.1562 | Val_Loss: 2.1045 | Val_Acc: 0.0938 -> Best model updated\n",
      "Training completed. Best validation accuracy: 0.0938. Running steps training time: 7.05 s.\n",
      "\n",
      "TESTING EXPERIMENT ID <20250502-194242_MambaVision-B-21K>\n",
      "==========================\n",
      "Test Loss: 2.0276 | Test Acc: 0.1875 | Running steps test time: 0.59 s.\n"
     ]
    }
   ],
   "source": [
    "# Define the num_epochs, optimizer and criterion\n",
    "num_epochs = 3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate the dataloaders\n",
    "pipeline.generate_dataloader(train, val, test)\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_eval_model(\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    ")\n",
    "# Test the model\n",
    "pipeline.test_model(criterion=criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
