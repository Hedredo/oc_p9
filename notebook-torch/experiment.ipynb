{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07aa478c",
   "metadata": {},
   "source": [
    "- Difficultés à gérer\n",
    "    - [ ] upskilling timm\n",
    "    - [ ] Définir les paramètres de base du transform\n",
    "    - [ ] Encdor les labels proprement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266321ea",
   "metadata": {},
   "source": [
    "## Les versions de pytorch et tensorflow\n",
    "----------------\n",
    "\n",
    "Afin de trouver un compromis entre les versions de pytorch et tensorflow, il a été décidé d'utiliser les versions suivantes:\n",
    "- Pytorch 2.6\n",
    "- Tensorflow 2.15\n",
    "\n",
    "La version de `pytorch==2.7` est incompatible avec `mamba_ssm` et soulève l'erreur suivante:\n",
    "<div style=\"background-color:rgba(100, 100, 100, 0.1); padding:10px; border-radius:4px;\">\n",
    "<pre><code>\n",
    "File ~/github/oc_p9/backend/.venv/lib/python3.11/site-packages/mamba_ssm/__init__.py:3\n",
    "      1 __version__ = \"2.2.4\"\n",
    "----> 3 from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn\n",
    "      4 from mamba_ssm.modules.mamba_simple import Mamba\n",
    "      5 from mamba_ssm.modules.mamba2 import Mamba2\n",
    "\n",
    "File ~/github/oc_p9/backend/.venv/lib/python3.11/site-packages/mamba_ssm/ops/selective_scan_interface.py:18\n",
    "     14     causal_conv1d_cuda = None\n",
    "     16 from mamba_ssm.ops.triton.layer_norm import _layer_norm_fwd\n",
    "---> 18 import selective_scan_cuda\n",
    "     21 class SelectiveScanFn(torch.autograd.Function):\n",
    "     23     @staticmethod\n",
    "     24     def forward(ctx, u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False,\n",
    "     25                 return_last_state=False):\n",
    "\n",
    "ImportError: /home/hedredo/github/oc_p9/backend/.venv/lib/python3.11/site-packages/selective_scan_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c107WarningC1ESt7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEERKNS_14SourceLocationESsb\n",
    "</code></pre>\n",
    "</div>\n",
    "\n",
    "Enfin la version la plus récente `tensorflow==2.19` est incompatible avec la dépendance `nvidia-cudnn-cu12==9.1` de `torch==2.6`.<br>\n",
    "Pour cette raison, la version de `tensorflow==2.15` a été choisie en raison de sa compatibilité avec la dépendance.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d9aa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import warnings\n",
    "from time import time\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor, AutoModel\n",
    "import requests\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d976f292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 2.2.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# display numpy version\n",
    "print(\"Numpy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "651be0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Désactiver les alertes de FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48bd2a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "12.4\n",
      "90100\n"
     ]
    }
   ],
   "source": [
    "# Print the torch version\n",
    "print(torch.__version__)\n",
    "# Print the CUDA version\n",
    "print(torch.version.cuda)\n",
    "# Print the cudnn version\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e15cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test = \"/home/hedredo/github/oc_p9/data/images/2aaa6083689193df5ab01fe37dea1b5e.jpg\"\n",
    "url = \"http://images.cocodataset.org/val2017/000000020247.jpg\"\n",
    "model_name_or_path = \"google/vit-base-patch16-224-in21k\" # or \"nvidia/MambaVision-T-1K\"\n",
    "models = [\n",
    "    \"nvidia/MambaVision-T-1K\",\n",
    "    \"nvidia/MambaVision-T2-1K\",\n",
    "    \"nvidia/MambaVision-S-1K\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af879910",
   "metadata": {},
   "source": [
    "[COPY] Récupération des paramètres de train-test-split sur le jeu utilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d9afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050 entries, 0 to 1049\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   image   1050 non-null   object\n",
      " 1   class   1050 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 16.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "image_df = pd.read_pickle('/home/hedredo/github/oc_p9/data/dataset_cleaned.pkl')\n",
    "image_df.drop(columns=['product_name', 'description'], inplace=True)\n",
    "# Assigne le nombre de classes\n",
    "n_classes = image_df['class'].nunique()\n",
    "\n",
    "# Assigne la liste des classes\n",
    "classes = list(image_df['class'].unique())\n",
    "\n",
    "# Encode les labels\n",
    "label_encoder = LabelEncoder()\n",
    "image_df['class'] = label_encoder.fit_transform(image_df['class'])\n",
    "print(image_df.info())\n",
    "# train test split avec un jeu de test de 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_df['image'], image_df['class'], test_size=0.2, random_state=314, stratify=image_df['class'])\n",
    "\n",
    "# Regroupement des données en dataframe de train et de test\n",
    "train = pd.concat([X_train, y_train], axis=1).sample(32)\n",
    "test = pd.concat([X_test, y_test], axis=1).sample(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08d4ee5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "58156de5-9233-45e7-8b96-e190fea81294",
       "rows": [
        [
         "922",
         "486e98154514ed485f0b2f9bc9f24549.jpg",
         "3"
        ],
        [
         "372",
         "ea82cb68a6e79d3c10fe9c4255c6a508.jpg",
         "1"
        ],
        [
         "1003",
         "0c70a580d96e50966130e2885d8c3505.jpg",
         "5"
        ],
        [
         "365",
         "7bcabeb38f148041087fafdad40e2c57.jpg",
         "1"
        ],
        [
         "211",
         "e99d9abb115a9bd580bcccf9ff4d4881.jpg",
         "3"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>486e98154514ed485f0b2f9bc9f24549.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>ea82cb68a6e79d3c10fe9c4255c6a508.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>0c70a580d96e50966130e2885d8c3505.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>7bcabeb38f148041087fafdad40e2c57.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>e99d9abb115a9bd580bcccf9ff4d4881.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     image  class\n",
       "922   486e98154514ed485f0b2f9bc9f24549.jpg      3\n",
       "372   ea82cb68a6e79d3c10fe9c4255c6a508.jpg      1\n",
       "1003  0c70a580d96e50966130e2885d8c3505.jpg      5\n",
       "365   7bcabeb38f148041087fafdad40e2c57.jpg      1\n",
       "211   e99d9abb115a9bd580bcccf9ff4d4881.jpg      3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6ba2b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dir\n",
    "image_dir = '/home/hedredo/github/oc_p9/data/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8012a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block DataLoader\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing image file names and labels.\n",
    "            image_dir (str): Directory where images are stored.\n",
    "            processor (AutoImageProcessor, optional):  Hugging Face processor for image preprocessing. Defaults to None.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])  # Assuming image file names are in the first column\n",
    "        image = Image.open(img_name).convert('RGB')  # Ensure consistent color format\n",
    "\n",
    "        label = self.dataframe.iloc[idx, 1]  # Assuming labels are in the second column\n",
    "\n",
    "        if self.processor:\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            image = inputs['pixel_values'].squeeze()  # Remove batch dimension\n",
    "        elif self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116edde",
   "metadata": {},
   "source": [
    "# **EXPERIMENTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48664e75",
   "metadata": {},
   "source": [
    "## **Google VIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "767b5ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "# Set the classification head with 7 classes\n",
    "model_name_or_path = \"google/vit-base-patch16-224-in21k\" # or \"nvidia/MambaVision-T-1K\"\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "        model_name_or_path, num_labels=7, trust_remote_code=True\n",
    "    )\n",
    "\n",
    "# Block Transform\n",
    "\n",
    "# If you don't want to use a processor, you can use transforms\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# Block processor \n",
    "\n",
    "# 3. Initialize processor/transforms\n",
    "processor = AutoImageProcessor.from_pretrained(model_name_or_path) # Use if you want to use a processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2a01cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([8, 3, 224, 224])\n",
      "Labels batch shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "dataset = ImageDataset(dataframe=train, image_dir=image_dir, processor=processor)\n",
    "dataset[0]\n",
    "\n",
    "# 5. Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Example of iterating through the dataloader\n",
    "for images, labels in dataloader:\n",
    "    print(f\"Image batch shape: {images.shape}\")\n",
    "    print(f\"Labels batch shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad29d623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.9564\n",
      "Epoch [2/3], Loss: 1.8762\n",
      "Epoch [3/3], Loss: 1.8195\n"
     ]
    }
   ],
   "source": [
    "# Training loop for the model with the train dataloader\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move model to the device\n",
    "model.to(device)\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# Set the number of epochs\n",
    "num_epochs = 3\n",
    "# Set the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in dataloader:\n",
    "        # Move images and labels to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images).logits\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "# Save the model\n",
    "# model.save_pretrained(\"mamba_model\")\n",
    "# processor.save_pretrained(\"mamba_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f29670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           Home Furnishing       0.00      0.00      0.00         4\n",
      "                 Baby Care       0.00      0.00      0.00         5\n",
      "                   Watches       0.00      0.00      0.00         4\n",
      "Home Decor & Festive Needs       0.60      0.50      0.55         6\n",
      "          Kitchen & Dining       0.25      0.60      0.35         5\n",
      "  Beauty and Personal Care       0.00      0.00      0.00         5\n",
      "                 Computers       0.75      1.00      0.86         3\n",
      "\n",
      "                  accuracy                           0.28        32\n",
      "                 macro avg       0.23      0.30      0.25        32\n",
      "              weighted avg       0.22      0.28      0.24        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference on test set\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Initialize the processor\n",
    "processor = AutoImageProcessor.from_pretrained(model_name_or_path)\n",
    "# Initialize the test dataset\n",
    "test_dataset = ImageDataset(dataframe=test, image_dir=image_dir, processor=processor)\n",
    "# Initialize the test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "# Initialize the predictions list\n",
    "predictions = []\n",
    "# Initialize the labels list\n",
    "labels_list = []\n",
    "# Inference loop\n",
    "for images, labels in test_dataloader:\n",
    "    # Move images to the device\n",
    "    images = images.to(device)\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images).logits\n",
    "    # Get the predicted labels\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    # Append the predictions and labels to the lists\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    labels_list.extend(labels.cpu().numpy())\n",
    "# Convert the predictions and labels to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "labels_list = np.array(labels_list)\n",
    "# Print the classification report\n",
    "print(classification_report(labels_list, predictions, target_names=classes))\n",
    "# Save the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e274bf",
   "metadata": {},
   "source": [
    "## **MAMBA S 1 K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b494483c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the averaged pool features: torch.Size([1, 768])\n",
      "Number of stages in extracted features: 4\n",
      "Size of extracted features in stage 1: torch.Size([1, 96, 56, 56])\n",
      "Size of extracted features in stage 4: torch.Size([1, 768, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"nvidia/MambaVision-S-1K\", trust_remote_code=True)\n",
    "\n",
    "# eval mode for inference\n",
    "model.cuda().eval()\n",
    "\n",
    "# prepare image for the model\n",
    "url = 'http://images.cocodataset.org/val2017/000000020247.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=False,\n",
    "                             mean=model.config.mean,\n",
    "                             std=model.config.std,\n",
    "                             crop_mode=model.config.crop_mode,\n",
    "                             crop_pct=model.config.crop_pct)\n",
    "inputs = transform(image).unsqueeze(0).cuda()\n",
    "# model inference\n",
    "out_avg_pool, features = model(inputs)\n",
    "print(\"Size of the averaged pool features:\", out_avg_pool.size())  # torch.Size([1, 768])\n",
    "print(\"Number of stages in extracted features:\", len(features)) # 4 stages\n",
    "print(\"Size of extracted features in stage 1:\", features[0].size()) # torch.Size([1, 96, 56, 56])\n",
    "print(\"Size of extracted features in stage 4:\", features[3].size()) # torch.Size([1, 768, 7, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0ad1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "\n",
    "class MambaClassifier(nn.Module):\n",
    "    def __init__(self, backbone, num_classes, hidden_dim=768): # Added hidden_dim\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.config = self.backbone.config\n",
    "        # self.classifier = nn.Linear(640, num_classes) # Original classifier\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes) # Modified classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_avg_pool, _ = self.backbone(x)\n",
    "        logits = self.classifier(out_avg_pool)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "933fb051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilités prédites: tensor([[0.1360, 0.1641, 0.1267, 0.1375, 0.1245, 0.1609, 0.1504]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mamba = MambaClassifier(model, num_classes=num_classes).cuda().eval()\n",
    "\n",
    "image = Image.open(img_test)\n",
    "\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=False,\n",
    "                             mean=mamba.config.mean,\n",
    "                             std=mamba.config.std,\n",
    "                             crop_mode=mamba.config.crop_mode,\n",
    "                             crop_pct=mamba.config.crop_pct)\n",
    "\n",
    "\n",
    "inputs = transform(image).unsqueeze(0).cuda()\n",
    "# Prédiction\n",
    "with torch.no_grad():\n",
    "    logits = mamba(inputs)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "print(\"Probabilités prédites:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5380a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaClassifier(AutoModel.from_pretrained(\"nvidia/MambaVision-S-1K\", trust_remote_code=True), num_classes=num_classes).cuda().train()\n",
    "\n",
    "image = Image.open(img_test)\n",
    "\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=True,\n",
    "                             mean=mamba.config.mean,\n",
    "                             std=mamba.config.std,\n",
    "                             crop_mode=mamba.config.crop_mode,\n",
    "                             crop_pct=mamba.config.crop_pct)\n",
    "\n",
    "dataset = ImageDataset(dataframe=train, image_dir=image_dir, transform=transform)\n",
    "\n",
    "# 5. Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b5f97d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.9875\n",
      "Epoch [2/3], Loss: 1.9445\n",
      "Epoch [3/3], Loss: 1.9724\n"
     ]
    }
   ],
   "source": [
    "# Training loop for the model with the train dataloader\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move model to the device\n",
    "model.to(device)\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# Set the number of epochs\n",
    "num_epochs = 3\n",
    "# Set the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in dataloader:\n",
    "        # Move images and labels to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # inputs = transform(image).unsqueeze(0).cuda()\n",
    "        # model inference\n",
    "        # outputs = model(inputs)\n",
    "        # end_time = time()\n",
    "        # logits = outputs[\"logits\"]\n",
    "        # predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "# Save the model\n",
    "# model.save_pretrained(\"mamba_model\")\n",
    "# processor.save_pretrained(\"mamba_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6d9d5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           Home Furnishing       0.00      0.00      0.00         4\n",
      "                 Baby Care       0.00      0.00      0.00         5\n",
      "                   Watches       0.00      0.00      0.00         4\n",
      "Home Decor & Festive Needs       0.00      0.00      0.00         6\n",
      "          Kitchen & Dining       0.40      0.40      0.40         5\n",
      "  Beauty and Personal Care       0.10      0.20      0.13         5\n",
      "                 Computers       0.00      0.00      0.00         3\n",
      "\n",
      "                  accuracy                           0.09        32\n",
      "                 macro avg       0.07      0.09      0.08        32\n",
      "              weighted avg       0.08      0.09      0.08        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference on test set\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Initialize the test dataset\n",
    "test_dataset = ImageDataset(dataframe=test, image_dir=image_dir, transform=transform)\n",
    "# Initialize the test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "# Initialize the predictions list\n",
    "predictions = []\n",
    "# Initialize the labels list\n",
    "labels_list = []\n",
    "# Inference loop\n",
    "for images, labels in test_dataloader:\n",
    "    # Move images to the device\n",
    "    images = images.to(device)\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    # Get the predicted labels\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    # Append the predictions and labels to the lists\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    labels_list.extend(labels.cpu().numpy())\n",
    "# Convert the predictions and labels to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "labels_list = np.array(labels_list)\n",
    "# Print the classification report\n",
    "print(classification_report(labels_list, predictions, target_names=classes))\n",
    "# Save the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da230c",
   "metadata": {},
   "source": [
    "## **MAMBA B 21 K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6d6291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaClassifier(AutoModel.from_pretrained(\"nvidia/MambaVision-B-21K\", trust_remote_code=True), num_classes=num_classes, hidden_dim=1024).cuda().train()\n",
    "\n",
    "image = Image.open(img_test)\n",
    "\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=True,\n",
    "                             mean=mamba.config.mean,\n",
    "                             std=mamba.config.std,\n",
    "                             crop_mode=mamba.config.crop_mode,\n",
    "                             crop_pct=mamba.config.crop_pct)\n",
    "\n",
    "dataset = ImageDataset(dataframe=train, image_dir=image_dir, transform=transform)\n",
    "\n",
    "# 5. Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbe35508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 2.1026\n",
      "Epoch [2/3], Loss: 2.0404\n",
      "Epoch [3/3], Loss: 1.9177\n"
     ]
    }
   ],
   "source": [
    "# Training loop for the model with the train dataloader\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move model to the device\n",
    "model.to(device)\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# Set the number of epochs\n",
    "num_epochs = 3\n",
    "# Set the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in dataloader:\n",
    "        # Move images and labels to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # inputs = transform(image).unsqueeze(0).cuda()\n",
    "        # model inference\n",
    "        # outputs = model(inputs)\n",
    "        # end_time = time()\n",
    "        # logits = outputs[\"logits\"]\n",
    "        # predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "# Save the model\n",
    "# model.save_pretrained(\"mamba_model\")\n",
    "# processor.save_pretrained(\"mamba_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23fb4b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           Home Furnishing       0.00      0.00      0.00         4\n",
      "                 Baby Care       0.50      0.40      0.44         5\n",
      "                   Watches       0.17      0.25      0.20         4\n",
      "Home Decor & Festive Needs       0.20      0.17      0.18         6\n",
      "          Kitchen & Dining       0.25      0.20      0.22         5\n",
      "  Beauty and Personal Care       0.25      0.20      0.22         5\n",
      "                 Computers       0.00      0.00      0.00         3\n",
      "\n",
      "                  accuracy                           0.19        32\n",
      "                 macro avg       0.20      0.17      0.18        32\n",
      "              weighted avg       0.21      0.19      0.20        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference on test set\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Initialize the test dataset\n",
    "test_dataset = ImageDataset(dataframe=test, image_dir=image_dir, transform=transform)\n",
    "# Initialize the test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "# Initialize the predictions list\n",
    "predictions = []\n",
    "# Initialize the labels list\n",
    "labels_list = []\n",
    "# Inference loop\n",
    "for images, labels in test_dataloader:\n",
    "    # Move images to the device\n",
    "    images = images.to(device)\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    # Get the predicted labels\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    # Append the predictions and labels to the lists\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    labels_list.extend(labels.cpu().numpy())\n",
    "# Convert the predictions and labels to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "labels_list = np.array(labels_list)\n",
    "# Print the classification report\n",
    "print(classification_report(labels_list, predictions, target_names=classes))\n",
    "# Save the predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
