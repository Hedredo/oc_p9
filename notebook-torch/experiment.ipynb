{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc430ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Faire les titres des sections avec un plan défini\n",
    "# TODO : Faire un sommaire en MD\n",
    "# TODO : Ajouter les commentaires\n",
    "# TODO : Ajouter la routine de data augmentation à tous les modèles (tester directement l'intégration du transform de mamba sur les autres modèles)\n",
    "# TODO : Faire meilleure routine de data augmentation ?\n",
    "# TODO : Faire pointer les runs de tensorboard sur un répertoire racine qui sera partagé par les 2 notebooks\n",
    "# TODO : Effacer les expériences et aussi les artifacts en dehors de efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9aa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor, AutoModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from constants import ROOT_FOLDER, SEED, VAL_SIZE, TEST_SIZE, BATCH_SIZE, SAMPLING, INPUT_RESOLUTION, MAMBA_HIDDEN_SIZES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "651be0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  2.6.0+cu124\n",
      "Cuda version:  12.4\n",
      "CUDNN version:  90100\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Désactiver les alertes de FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Print the torch? cuda and cudnn version\n",
    "print(\"Torch version: \", torch.__version__)\n",
    "print(\"Cuda version: \", torch.version.cuda)\n",
    "print(\"CUDNN version: \", torch.backends.cudnn.version())\n",
    "\n",
    "# Set the device to GPU if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85ca3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the folder path containing the images\n",
    "IMAGE_FOLDER = ROOT_FOLDER / \"data\" / \"images\"\n",
    "IMAGE_TEST = ROOT_FOLDER / \"data\" / \"images\" / \"2aaa6083689193df5ab01fe37dea1b5e.jpg\"\n",
    "# Assign the folder path containing the former H5 efficientnet weights\n",
    "ARTIFACTS_FOLDER = ROOT_FOLDER / \"artifacts\"\n",
    "# Assign the folder path with the pickle dataset with labels, images filenames and metadata\n",
    "DATASET_PATH = ROOT_FOLDER / \"data\" / \"dataset_cleaned.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37f68941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1050, 2)\n",
      "Dataset columns: Index(['image', 'class'], dtype='object')\n",
      "Number of classes: 7\n",
      "Classes: ['Baby Care', 'Beauty and Personal Care', 'Computers', 'Home Decor & Festive Needs', 'Home Furnishing', 'Kitchen & Dining', 'Watches']\n"
     ]
    }
   ],
   "source": [
    "# Loading the pickle dataset_cleaned used with the previous project as a pandas df\n",
    "df = pd.read_pickle(DATASET_PATH).drop(columns=['product_name', 'description'])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Dataset columns: {df.columns}\")\n",
    "\n",
    "# Encode the labels with LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df[\"class\"])\n",
    "n_classes = len(le.classes_)\n",
    "classes = le.classes_.tolist()\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# Finally transform the class column to the encoded labels\n",
    "df[\"class\"] = le.transform(df[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91d9afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (758, 2)\n",
      "Val shape: (134, 2)\n",
      "Test shape: (158, 2)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the datasets into train, val and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(df['image'], df['class'], test_size=TEST_SIZE, random_state=SEED, stratify=df['class'], shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=VAL_SIZE, random_state=SEED, stratify=y_temp, shuffle=True)\n",
    "\n",
    "# Concat X and y for each set\n",
    "train = pd.concat([X_train, y_train], axis=1).sample(SAMPLING) if SAMPLING else pd.concat([X_train, y_train], axis=1)\n",
    "val = pd.concat([X_val, y_val], axis=1).sample(SAMPLING) if SAMPLING else pd.concat([X_val, y_val], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1).sample(SAMPLING) if SAMPLING else pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Print the shape of each set\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Val shape: {val.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08d4ee5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "22e6a6e9-a152-4c63-8ed2-43a22b88aac6",
       "rows": [
        [
         "229",
         "caabe6014b914fe2874a9a8d7284f79b.jpg",
         "3"
        ],
        [
         "450",
         "95feec21a9d076cff084159d61bf9b8e.jpg",
         "0"
        ],
        [
         "798",
         "9993de7e2bcced43dc9edb3b2c81f23d.jpg",
         "1"
        ],
        [
         "230",
         "968a2b3be84193e3f755c2fe71033a2c.jpg",
         "3"
        ],
        [
         "293",
         "c2efa8aa11898bdb5fc4e46201973a42.jpg",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>caabe6014b914fe2874a9a8d7284f79b.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>95feec21a9d076cff084159d61bf9b8e.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>9993de7e2bcced43dc9edb3b2c81f23d.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>968a2b3be84193e3f755c2fe71033a2c.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>c2efa8aa11898bdb5fc4e46201973a42.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "229  caabe6014b914fe2874a9a8d7284f79b.jpg      3\n",
       "450  95feec21a9d076cff084159d61bf9b8e.jpg      0\n",
       "798  9993de7e2bcced43dc9edb3b2c81f23d.jpg      1\n",
       "230  968a2b3be84193e3f755c2fe71033a2c.jpg      3\n",
       "293  c2efa8aa11898bdb5fc4e46201973a42.jpg      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4eede8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "5432fbf7-5588-446c-9c62-09079e4288a4",
       "rows": [
        [
         "979",
         "c44a5dc5b5ebe5b3e0535b7c2b7921e4.jpg",
         "0"
        ],
        [
         "49",
         "02a53d335775b652f22f41b529b9d646.jpg",
         "1"
        ],
        [
         "567",
         "97fba8a02361aa56eaa9fa51bc1d7661.jpg",
         "6"
        ],
        [
         "494",
         "a124d6e4c30b00918c594289266a383c.jpg",
         "6"
        ],
        [
         "773",
         "109e235d4838002246599f987d935c21.jpg",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>c44a5dc5b5ebe5b3e0535b7c2b7921e4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>02a53d335775b652f22f41b529b9d646.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>97fba8a02361aa56eaa9fa51bc1d7661.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>a124d6e4c30b00918c594289266a383c.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>109e235d4838002246599f987d935c21.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "979  c44a5dc5b5ebe5b3e0535b7c2b7921e4.jpg      0\n",
       "49   02a53d335775b652f22f41b529b9d646.jpg      1\n",
       "567  97fba8a02361aa56eaa9fa51bc1d7661.jpg      6\n",
       "494  a124d6e4c30b00918c594289266a383c.jpg      6\n",
       "773  109e235d4838002246599f987d935c21.jpg      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6ba2b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2a4beceb-5ef3-4939-91a2-0887e14c72c5",
       "rows": [
        [
         "11",
         "08452abdadb3db1e686b94a9c52fc7b6.jpg",
         "6"
        ],
        [
         "548",
         "2541b59d54a3a9f2681c0049f7ddd85c.jpg",
         "6"
        ],
        [
         "696",
         "82fbc93cd45ab747e7e606f2c52c7335.jpg",
         "3"
        ],
        [
         "238",
         "2e8df36b35d22cf219cf8bae6c2af752.jpg",
         "5"
        ],
        [
         "963",
         "bcb51cec3d290e6a661586d0df30e091.jpg",
         "4"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>08452abdadb3db1e686b94a9c52fc7b6.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>2541b59d54a3a9f2681c0049f7ddd85c.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>82fbc93cd45ab747e7e606f2c52c7335.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2e8df36b35d22cf219cf8bae6c2af752.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>bcb51cec3d290e6a661586d0df30e091.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "11   08452abdadb3db1e686b94a9c52fc7b6.jpg      6\n",
       "548  2541b59d54a3a9f2681c0049f7ddd85c.jpg      6\n",
       "696  82fbc93cd45ab747e7e606f2c52c7335.jpg      3\n",
       "238  2e8df36b35d22cf219cf8bae6c2af752.jpg      5\n",
       "963  bcb51cec3d290e6a661586d0df30e091.jpg      4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116edde",
   "metadata": {},
   "source": [
    "# **CLASSES AND FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8012a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block DataLoader\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing image file names and labels.\n",
    "            image_dir (str): Directory where images are stored.\n",
    "            processor (AutoImageProcessor, optional):  Hugging Face processor for image preprocessing. Defaults to None.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])  # Assuming image file names are in the first column\n",
    "        image = Image.open(img_name).convert('RGB')  # Ensure consistent color format\n",
    "\n",
    "        label = self.dataframe.iloc[idx, 1]  # Assuming labels are in the second column\n",
    "\n",
    "        if self.processor:\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            image = inputs['pixel_values'].squeeze()  # Remove batch dimension\n",
    "        elif self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "971655ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: AutoModel,\n",
    "        num_classes: int,\n",
    "        hidden_dim: int , # The hidden dimension of the backbone is stored in the MAMBA_HIDDEN_SIZES dict with the model card as the key\n",
    "        fc_layer: int = None # Add the int number of layers before the classifier\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.config = self.backbone.config\n",
    "        if fc_layer:\n",
    "            self.fc_layers = nn.ModuleList()\n",
    "            for i in range(fc_layer):\n",
    "                if i == 0:\n",
    "                    self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                else:\n",
    "                    self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                self.fc_layers.append(nn.ReLU())\n",
    "                self.fc_layers.append(nn.Dropout(0.1))\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_avg_pool, _ = self.backbone(x)\n",
    "        if hasattr(self, 'fc_layers'):\n",
    "            for layer in self.fc_layers:\n",
    "                out_avg_pool = layer(out_avg_pool)\n",
    "        logits = self.classifier(out_avg_pool)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe2b48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_to_markdown(y_true, y_pred, target_names):\n",
    "    \"\"\"\n",
    "    Convert the classification report to a markdown table.\n",
    "    \"\"\"\n",
    "    report = classification_report(y_true, y_pred, target_names=target_names, zero_division=0, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df = report_df.drop(columns=['support'])\n",
    "    report_df = report_df.rename_axis('Classes').reset_index()\n",
    "    return report_df.to_markdown(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8edb3b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "    model: AutoModelForImageClassification|MambaClassifier,\n",
    "    experiment_id: str,\n",
    "    test_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    writer: SummaryWriter = None\n",
    "    ):\n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(ARTIFACTS_FOLDER / f'{experiment_id}.pth'))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Check if the model is a MambaClassifier instance\n",
    "    mamba = \"MambaClassifier\" in type(model).__name__\n",
    "    \n",
    "    # Initialize the test metrics\n",
    "    test_loss, correct_test, total_test = .0, 0, 0\n",
    "    running_test_steps = 0\n",
    "    running_test_time_by_step = .0\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            step_time = time()\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(inputs).logits if not mamba else model(inputs)\n",
    "            test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct_test += (preds == labels).sum().item()\n",
    "            total_test += inputs.size(0)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            running_test_time_by_step += time() - step_time\n",
    "            running_test_steps += 1\n",
    "            if writer:\n",
    "                writer.add_scalar('TimingByStep/test', running_test_time_by_step, running_test_steps)\n",
    "\n",
    "    epoch_test_loss = test_loss / total_test\n",
    "    epoch_test_acc  = correct_test / total_test\n",
    "    if writer:\n",
    "        writer.add_scalar('Accuracy/test', epoch_test_acc, 0)\n",
    "\n",
    "    print(f\"Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_acc:.4f}\")\n",
    "    \n",
    "    # Save Classification report in tensorboard\n",
    "    classification_report_md = classification_report_to_markdown(y_true, y_pred, target_names=classes)\n",
    "    cm = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true, y_pred,\n",
    "        labels=range(n_classes),\n",
    "        normalize='true',\n",
    "        display_labels=classes,\n",
    "        xticks_rotation=\"vertical\",\n",
    "        cmap=plt.cm.Blues\n",
    "        )\n",
    "    # Save the classification report as a markdown file in tensorboard\n",
    "    if writer:\n",
    "        writer.add_text('ClassificationReport/test', classification_report_md, 0)\n",
    "        writer.add_figure('ConfusionMatrix/test', cm.figure_, 0)\n",
    "    # Delete the model from GPU memory\n",
    "    del model, inputs, labels, outputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fec3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_model(\n",
    "    model: AutoModelForImageClassification|MambaClassifier,\n",
    "    model_card: str,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    num_epochs: int,):\n",
    "    # --- LOOP ---\n",
    "    # Initialize SummaryWriter\n",
    "    experiment_id = \"_\".join([datetime.now().strftime(\"%Y%m%d-%H%M%S\"), model_card.split(\"/\")[-1]])\n",
    "    log_dir = ROOT_FOLDER / os.getcwd().split(\"/\")[-1] / \"runs\" / \"_\".join([experiment_id, model_card.split(\"/\")[-1]])\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # Check if the model is a MambaClassifier instance\n",
    "    mamba = \"MambaClassifier\" in type(model).__name__\n",
    "\n",
    "    # Initialize commun metrics\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Initialize the training metrics\n",
    "    running_train_time_by_step = .0\n",
    "    running_train_time_by_epoch = .0\n",
    "    running_train_steps = 0\n",
    "    writer.add_scalar('TimingByEpoch/train', running_train_time_by_epoch, 0)\n",
    "\n",
    "    # Initialize the validation metrics\n",
    "    best_val_metric = float('-inf')\n",
    "    running_val_time_by_step = .0\n",
    "    running_val_time_by_epoch = .0\n",
    "    running_val_steps = 0\n",
    "    writer.add_scalar('TimingByEpoch/validation', running_val_time_by_epoch, 0)\n",
    "    \n",
    "    writer.add_scalar('TimingByEpoch/train', running_train_time_by_epoch, 0)\n",
    "    # Move model to the device\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- 1. TRAINING LOOP ---\n",
    "        model.train()\n",
    "        train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "        epoch_time = time()\n",
    "        for i, (inputs, labels) in enumerate(train_loader): # Use enumerate for step count\n",
    "            step_time = time()\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            # Compute the model outputs given the mamba variable\n",
    "            outputs = model(inputs).logits if not mamba else model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            step_loss = loss.item() * inputs.size(0)\n",
    "            train_loss += step_loss\n",
    "            preds_train = outputs.argmax(dim=1)\n",
    "            correct_train += (preds_train == labels).sum().item()\n",
    "            total_train += inputs.size(0)\n",
    "            running_train_steps += 1\n",
    "            running_train_time_by_step += time() - step_time\n",
    "            writer.add_scalar('TimingByStep/train', running_train_time_by_step, running_train_steps)\n",
    "            writer.add_scalar('LossByStep/train', step_loss, running_train_steps)\n",
    "\n",
    "        running_train_time_by_epoch += time() - epoch_time\n",
    "        epoch_train_loss = train_loss / total_train\n",
    "        epoch_train_acc = correct_train / total_train\n",
    "        # Log training metrics per epoch\n",
    "        writer.add_scalar('LossByEpoch/train', epoch_train_loss, epoch)\n",
    "        writer.add_scalar('AccuracyByEpoch/train', epoch_train_acc, epoch)\n",
    "        writer.add_scalar('TimingByEpoch/train', running_train_time_by_epoch, epoch)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}\")\n",
    "\n",
    "\n",
    "        # --- 2. VALIDATION LOOP ---\n",
    "        model.eval()\n",
    "        val_loss, correct_val, total_val = .0, 0, 0\n",
    "        epoch_time = time()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move images and labels to the device\n",
    "                step_time = time()\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                outputs = model(inputs).logits if not mamba else model(inputs)\n",
    "                val_step_loss = criterion(outputs, labels).item() * inputs.size(0)\n",
    "                val_loss   += val_step_loss\n",
    "                preds      = outputs.argmax(dim=1)\n",
    "                correct_val   += (preds == labels).sum().item()\n",
    "                total_val     += inputs.size(0)\n",
    "                running_val_steps += 1\n",
    "                running_val_time_by_step += time() - step_time\n",
    "                writer.add_scalar('TimingByStep/validation', running_val_time_by_step, running_val_steps)\n",
    "                writer.add_scalar('LossByStep/validation', val_step_loss, running_train_steps)\n",
    "\n",
    "        running_val_time_by_epoch += time() - epoch_time\n",
    "        epoch_val_loss = val_loss / total_val\n",
    "        epoch_val_acc  = correct_val / total_val\n",
    "        # Log validation metrics per epoch\n",
    "        writer.add_scalar('Loss/validation', epoch_val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/validation', epoch_val_acc, epoch)\n",
    "        writer.add_scalar('TimingByEpoch/validation', running_val_time_by_epoch, epoch)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "        # --- 3. UPDATE BEST MODEL ---\n",
    "        # Save the model if the validation accuracy is better than the best one\n",
    "        if epoch_val_acc > best_val_metric:\n",
    "            best_val_metric = epoch_val_acc\n",
    "            best_epoch      = epoch\n",
    "            torch.save(model.state_dict(), ARTIFACTS_FOLDER / f'{experiment_id}.pth')\n",
    "            print(f\"Best model updated at epoch {best_epoch} with val acc: {best_val_metric:.4f}\")\n",
    "    \n",
    "    # Delete the model from GPU memory\n",
    "    del inputs, labels, outputs, optimizer, loss, val_loss, step_loss, val_step_loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- 4. TESTING LOOP ---\n",
    "    test_model(model, experiment_id, test_loader, criterion, writer)\n",
    "\n",
    "    # --- After the training loop ---\n",
    "    writer.close() # Close the writer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c3a8c",
   "metadata": {},
   "source": [
    "# EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5abcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Ajouter une card dans le tensorboard pour les paramètres du modèle\n",
    "# TODO : warning PIL decompression bomb warning\n",
    "# TODO : Nettoyer les fichiers useless\n",
    "# TODO : Tester le batch size de 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03437169",
   "metadata": {},
   "source": [
    "## **Google VIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d242b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Define the model\n",
    "model = AutoModelForImageClassification.from_pretrained(model_card, num_labels=n_classes, trust_remote_code=True)\n",
    "\n",
    "# Define the image processor\n",
    "processor = AutoImageProcessor.from_pretrained(model_card)\n",
    "\n",
    "# Prepare the dataloaders for training, validation and testing\n",
    "dataset = ImageDataset(dataframe=train, image_dir=IMAGE_FOLDER, processor=processor)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataset = ImageDataset(dataframe=val, image_dir=IMAGE_FOLDER, processor=processor)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "dataset = ImageDataset(dataframe=test, image_dir=IMAGE_FOLDER, processor=processor)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e2e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 1.9503, Train Acc: 0.1172\n",
      "Epoch [1/5], Val Loss: 1.9338, Val Acc: 0.1562\n",
      "Best model updated at epoch 0 with val acc: 0.1562\n",
      "Epoch [2/5], Train Loss: 1.8478, Train Acc: 0.5312\n",
      "Epoch [2/5], Val Loss: 1.8971, Val Acc: 0.3281\n",
      "Best model updated at epoch 1 with val acc: 0.3281\n",
      "Epoch [3/5], Train Loss: 1.7559, Train Acc: 0.7969\n",
      "Epoch [3/5], Val Loss: 1.8617, Val Acc: 0.4453\n",
      "Best model updated at epoch 2 with val acc: 0.4453\n",
      "Epoch [4/5], Train Loss: 1.6605, Train Acc: 0.9219\n",
      "Epoch [4/5], Val Loss: 1.8227, Val Acc: 0.5156\n",
      "Best model updated at epoch 3 with val acc: 0.5156\n",
      "Epoch [5/5], Train Loss: 1.5614, Train Acc: 0.9688\n",
      "Epoch [5/5], Val Loss: 1.7789, Val Acc: 0.5781\n",
      "Best model updated at epoch 4 with val acc: 0.5781\n",
      "Test Loss: 1.8062, Test Acc: 0.5391\n"
     ]
    }
   ],
   "source": [
    "# Set the training parameters\n",
    "num_epochs = 5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Run the training workflow for AutoModelForImageClassification\n",
    "train_and_eval_model(\n",
    "    model = model,\n",
    "    model_card = model_card,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    test_loader = test_loader,\n",
    "    optimizer = optimizer,\n",
    "    criterion = criterion,\n",
    "    num_epochs = num_epochs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea071149",
   "metadata": {},
   "source": [
    "## MOBILENETV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"google/mobilenet_v2_1.0_224\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e274bf",
   "metadata": {},
   "source": [
    "## **MAMBA T 1 K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b41c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"nvidia/MambaVision-T-1K\"\n",
    "\n",
    "# Define the model with the MambaClassifier class\n",
    "model = MambaClassifier(\n",
    "    AutoModel.from_pretrained(model_card, trust_remote_code=True),\n",
    "    num_classes=n_classes,\n",
    "    hidden_dim=MAMBA_HIDDEN_SIZES.get(model_card),\n",
    "    )\n",
    "\n",
    "# Define the image processor following MAMBA's preprocessing instructions\n",
    "transform_train = create_transform(input_size=INPUT_RESOLUTION,\n",
    "                             is_training=True, # Use the data augmentation for training\n",
    "                             mean=model.config.mean,\n",
    "                             std=model.config.std,\n",
    "                             crop_mode=model.config.crop_mode,\n",
    "                             crop_pct=model.config.crop_pct)\n",
    "\n",
    "transform_val = create_transform(input_size=INPUT_RESOLUTION,\n",
    "                             is_training=False,\n",
    "                             mean=model.config.mean,\n",
    "                             std=model.config.std,\n",
    "                             crop_mode=model.config.crop_mode,\n",
    "                             crop_pct=model.config.crop_pct)\n",
    "\n",
    "# Prepare the dataloaders for training, validation and testing\n",
    "dataset = ImageDataset(dataframe=train, image_dir=IMAGE_FOLDER, transform=transform_train)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataset = ImageDataset(dataframe=val, image_dir=IMAGE_FOLDER, transform=transform_val)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "dataset = ImageDataset(dataframe=test, image_dir=IMAGE_FOLDER, transform=transform_val)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc16f66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-torch/.venv/lib/python3.11/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 1.8855, Train Acc: 0.2797\n",
      "Epoch [1/5], Val Loss: 1.7303, Val Acc: 0.5149\n",
      "Best model updated at epoch 0 with val acc: 0.5149\n",
      "Epoch [2/5], Train Loss: 1.7018, Train Acc: 0.5290\n",
      "Epoch [2/5], Val Loss: 1.4372, Val Acc: 0.7015\n",
      "Best model updated at epoch 1 with val acc: 0.7015\n",
      "Epoch [3/5], Train Loss: 1.4369, Train Acc: 0.6781\n",
      "Epoch [3/5], Val Loss: 1.0409, Val Acc: 0.8433\n",
      "Best model updated at epoch 2 with val acc: 0.8433\n",
      "Epoch [4/5], Train Loss: 1.1507, Train Acc: 0.7216\n",
      "Epoch [4/5], Val Loss: 0.7072, Val Acc: 0.8731\n",
      "Best model updated at epoch 3 with val acc: 0.8731\n",
      "Epoch [5/5], Train Loss: 0.9697, Train Acc: 0.7414\n",
      "Epoch [5/5], Val Loss: 0.5560, Val Acc: 0.8806\n",
      "Best model updated at epoch 4 with val acc: 0.8806\n",
      "Test Loss: 0.7440, Test Acc: 0.7785\n"
     ]
    }
   ],
   "source": [
    "# Set the training parameters\n",
    "num_epochs = 5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Run the training workflow for AutoModelForImageClassification\n",
    "train_and_eval_model(\n",
    "    model = model,\n",
    "    model_card = model_card,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    test_loader = test_loader,\n",
    "    optimizer = optimizer,\n",
    "    criterion = criterion,\n",
    "    num_epochs = num_epochs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da230c",
   "metadata": {},
   "source": [
    "## **MAMBA B 21 K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6d6291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = \"nvidia/MambaVision-B-21K\"\n",
    "\n",
    "# Define the model with the MambaClassifier class\n",
    "model = MambaClassifier(\n",
    "    AutoModel.from_pretrained(model_card, trust_remote_code=True),\n",
    "    num_classes=n_classes,\n",
    "    hidden_dim=MAMBA_HIDDEN_SIZES.get(model_card),\n",
    "    )\n",
    "\n",
    "# Define the image processor following MAMBA's preprocessing instructions\n",
    "transform_train = create_transform(input_size=INPUT_RESOLUTION,\n",
    "                             is_training=True, # Use the data augmentation for training\n",
    "                             mean=model.config.mean,\n",
    "                             std=model.config.std,\n",
    "                             crop_mode=model.config.crop_mode,\n",
    "                             crop_pct=model.config.crop_pct)\n",
    "\n",
    "transform_val = create_transform(input_size=INPUT_RESOLUTION,\n",
    "                             is_training=False,\n",
    "                             mean=model.config.mean,\n",
    "                             std=model.config.std,\n",
    "                             crop_mode=model.config.crop_mode,\n",
    "                             crop_pct=model.config.crop_pct)\n",
    "\n",
    "# Prepare the dataloaders for training, validation and testing\n",
    "dataset = ImageDataset(dataframe=train, image_dir=IMAGE_FOLDER, transform=transform_train)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataset = ImageDataset(dataframe=val, image_dir=IMAGE_FOLDER, transform=transform_val)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "dataset = ImageDataset(dataframe=test, image_dir=IMAGE_FOLDER, transform=transform_val)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbe35508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 1.9823, Train Acc: 0.1172\n",
      "Epoch [1/5], Val Loss: 1.9017, Val Acc: 0.2422\n",
      "Best model updated at epoch 0 with val acc: 0.2422\n",
      "Epoch [2/5], Train Loss: 1.8869, Train Acc: 0.1875\n",
      "Epoch [2/5], Val Loss: 1.8000, Val Acc: 0.3438\n",
      "Best model updated at epoch 1 with val acc: 0.3438\n",
      "Epoch [3/5], Train Loss: 1.8043, Train Acc: 0.3359\n",
      "Epoch [3/5], Val Loss: 1.7076, Val Acc: 0.4297\n",
      "Best model updated at epoch 2 with val acc: 0.4297\n",
      "Epoch [4/5], Train Loss: 1.6682, Train Acc: 0.4766\n",
      "Epoch [4/5], Val Loss: 1.6223, Val Acc: 0.5156\n",
      "Best model updated at epoch 3 with val acc: 0.5156\n",
      "Epoch [5/5], Train Loss: 1.5941, Train Acc: 0.5703\n",
      "Epoch [5/5], Val Loss: 1.5438, Val Acc: 0.5938\n",
      "Best model updated at epoch 4 with val acc: 0.5938\n",
      "Test Loss: 1.6233, Test Acc: 0.4688\n"
     ]
    }
   ],
   "source": [
    "# Set the training parameters\n",
    "num_epochs = 5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Run the training workflow for AutoModelForImageClassification\n",
    "train_and_eval_model(\n",
    "    model = model,\n",
    "    model_card = model_card,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    test_loader = test_loader,\n",
    "    optimizer = optimizer,\n",
    "    criterion = criterion,\n",
    "    num_epochs = num_epochs,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
