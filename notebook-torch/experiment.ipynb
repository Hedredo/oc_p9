{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07aa478c",
   "metadata": {},
   "source": [
    "- Difficultés à gérer\n",
    "    - [ ] upskilling timm\n",
    "    - [ ] Définir les paramètres de base du transform\n",
    "    - [ ] Encdor les labels proprement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9aa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from time import time\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor, AutoModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from constants import ROOT_FOLDER, SEED, VAL_SIZE, TEST_SIZE, BATCH_SIZE, SAMPLING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651be0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  2.6.0+cu124\n",
      "Cuda version:  12.4\n",
      "CUDNN version:  90100\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Désactiver les alertes de FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Print the torch? cuda and cudnn version\n",
    "print(\"Torch version: \", torch.__version__)\n",
    "print(\"Cuda version: \", torch.version.cuda)\n",
    "print(\"CUDNN version: \", torch.backends.cudnn.version())\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af879910",
   "metadata": {},
   "source": [
    "[COPY] Récupération des paramètres de train-test-split sur le jeu utilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ca3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the folder path containing the images\n",
    "IMAGE_FOLDER = ROOT_FOLDER / \"data\" / \"images\"\n",
    "IMAGE_TEST = ROOT_FOLDER / \"data\" / \"images\" / \"2aaa6083689193df5ab01fe37dea1b5e.jpg\"\n",
    "# Assign the folder path containing the former H5 efficientnet weights\n",
    "ARTIFACTS_FOLDER = ROOT_FOLDER / \"artifacts\"\n",
    "# Assign the folder path with the pickle dataset with labels, images filenames and metadata\n",
    "DATASET_PATH = ROOT_FOLDER / \"data\" / \"dataset_cleaned.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f68941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1050, 2)\n",
      "Dataset columns: Index(['image', 'class'], dtype='object')\n",
      "Number of classes: 7\n",
      "Classes: ['Baby Care', 'Beauty and Personal Care', 'Computers', 'Home Decor & Festive Needs', 'Home Furnishing', 'Kitchen & Dining', 'Watches']\n"
     ]
    }
   ],
   "source": [
    "# Loading the pickle dataset_cleaned used with the previous project as a pandas df\n",
    "df = pd.read_pickle(DATASET_PATH).drop(columns=['product_name', 'description'])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Dataset columns: {df.columns}\")\n",
    "\n",
    "# Encode the labels with LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df[\"class\"])\n",
    "n_classes = len(le.classes_)\n",
    "classes = le.classes_.tolist()\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# Finally transform the class column to the encoded labels\n",
    "df[\"class\"] = le.transform(df[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91d9afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (32, 2)\n",
      "Val shape: (32, 2)\n",
      "Test shape: (32, 2)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the datasets into train, val and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(df['image'], df['class'], test_size=TEST_SIZE, random_state=SEED, stratify=df['class'], shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=VAL_SIZE, random_state=SEED, stratify=y_temp, shuffle=True)\n",
    "\n",
    "# Concat X and y for each set\n",
    "train = pd.concat([X_train, y_train], axis=1).sample(SAMPLING) if SAMPLING else pd.concat([X_train, y_train], axis=1)\n",
    "val = pd.concat([X_val, y_val], axis=1).sample(SAMPLING) if SAMPLING else pd.concat([X_val, y_val], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1).sample(SAMPLING) if SAMPLING else pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Print the shape of each set\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Val shape: {val.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d4ee5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "d89e3b45-0178-4c0d-ae42-b20a9b1ea32c",
       "rows": [
        [
         "708",
         "ce9207944cedeaa82e4ea6269586af2a.jpg",
         "4"
        ],
        [
         "344",
         "6e44d107ee32412243b19b0ed9b415f3.jpg",
         "1"
        ],
        [
         "307",
         "a12d9ae5720ae41446e084911f0c2865.jpg",
         "3"
        ],
        [
         "234",
         "ed139e0d1b5c973495e1aa35dd4a5533.jpg",
         "5"
        ],
        [
         "461",
         "3b80ac036843b278083fabfd9a3c84ff.jpg",
         "5"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>ce9207944cedeaa82e4ea6269586af2a.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>6e44d107ee32412243b19b0ed9b415f3.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>a12d9ae5720ae41446e084911f0c2865.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>ed139e0d1b5c973495e1aa35dd4a5533.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>3b80ac036843b278083fabfd9a3c84ff.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "708  ce9207944cedeaa82e4ea6269586af2a.jpg      4\n",
       "344  6e44d107ee32412243b19b0ed9b415f3.jpg      1\n",
       "307  a12d9ae5720ae41446e084911f0c2865.jpg      3\n",
       "234  ed139e0d1b5c973495e1aa35dd4a5533.jpg      5\n",
       "461  3b80ac036843b278083fabfd9a3c84ff.jpg      5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4eede8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "4c165769-57bd-4fcb-b964-e914e312da19",
       "rows": [
        [
         "388",
         "98ad5b99ad96695568d8f143b11ab740.jpg",
         "1"
        ],
        [
         "7",
         "dd0e3470a7e6ed76fd69c2da27721041.jpg",
         "6"
        ],
        [
         "282",
         "a8ea6fc2b3cd95f46bced80853ce8e0e.jpg",
         "0"
        ],
        [
         "622",
         "672d1c3272eae4586eb5994fe408c12a.jpg",
         "0"
        ],
        [
         "447",
         "8e961e4fd248c9496ca54808d2d2b25f.jpg",
         "2"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>98ad5b99ad96695568d8f143b11ab740.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dd0e3470a7e6ed76fd69c2da27721041.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>a8ea6fc2b3cd95f46bced80853ce8e0e.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>672d1c3272eae4586eb5994fe408c12a.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>8e961e4fd248c9496ca54808d2d2b25f.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "388  98ad5b99ad96695568d8f143b11ab740.jpg      1\n",
       "7    dd0e3470a7e6ed76fd69c2da27721041.jpg      6\n",
       "282  a8ea6fc2b3cd95f46bced80853ce8e0e.jpg      0\n",
       "622  672d1c3272eae4586eb5994fe408c12a.jpg      0\n",
       "447  8e961e4fd248c9496ca54808d2d2b25f.jpg      2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6ba2b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a73136a7-2528-4027-b4f2-eadb29530482",
       "rows": [
        [
         "864",
         "19d2dbc1789653c58bce08c169662cf2.jpg",
         "2"
        ],
        [
         "861",
         "ad08a6efe82576ab162a9336feff647f.jpg",
         "2"
        ],
        [
         "846",
         "82466269245f199d7f850509307a497b.jpg",
         "2"
        ],
        [
         "74",
         "7a3f11f380a1bf85338ab3771ff81e9f.jpg",
         "0"
        ],
        [
         "899",
         "f0ab5f12bd777e28a401a728fcb93a4b.jpg",
         "3"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>19d2dbc1789653c58bce08c169662cf2.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>ad08a6efe82576ab162a9336feff647f.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>82466269245f199d7f850509307a497b.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>7a3f11f380a1bf85338ab3771ff81e9f.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>f0ab5f12bd777e28a401a728fcb93a4b.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "864  19d2dbc1789653c58bce08c169662cf2.jpg      2\n",
       "861  ad08a6efe82576ab162a9336feff647f.jpg      2\n",
       "846  82466269245f199d7f850509307a497b.jpg      2\n",
       "74   7a3f11f380a1bf85338ab3771ff81e9f.jpg      0\n",
       "899  f0ab5f12bd777e28a401a728fcb93a4b.jpg      3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8012a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block DataLoader\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing image file names and labels.\n",
    "            image_dir (str): Directory where images are stored.\n",
    "            processor (AutoImageProcessor, optional):  Hugging Face processor for image preprocessing. Defaults to None.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])  # Assuming image file names are in the first column\n",
    "        image = Image.open(img_name).convert('RGB')  # Ensure consistent color format\n",
    "\n",
    "        label = self.dataframe.iloc[idx, 1]  # Assuming labels are in the second column\n",
    "\n",
    "        if self.processor:\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            image = inputs['pixel_values'].squeeze()  # Remove batch dimension\n",
    "        elif self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116edde",
   "metadata": {},
   "source": [
    "# **EXPERIMENTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48664e75",
   "metadata": {},
   "source": [
    "## **Google VIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d242b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Define the model\n",
    "model = AutoModelForImageClassification.from_pretrained(model_card, num_labels=n_classes, trust_remote_code=True)\n",
    "\n",
    "# Define the image processor\n",
    "processor = AutoImageProcessor.from_pretrained(model_card)\n",
    "\n",
    "# Prepare the dataloaders for training, validation and testing\n",
    "dataset = ImageDataset(dataframe=train, image_dir=IMAGE_FOLDER, processor=processor)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataset = ImageDataset(dataframe=val, image_dir=IMAGE_FOLDER, processor=processor)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "dataset = ImageDataset(dataframe=test, image_dir=IMAGE_FOLDER, processor=processor)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad29d623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.9417\n",
      "Epoch [1/3], Val Loss: 1.9310, Val Acc: 0.1250\n",
      "Best model updated at epoch 1 with val acc: 0.1250\n",
      "Epoch [2/3], Loss: 1.8607\n",
      "Epoch [2/3], Val Loss: 1.9246, Val Acc: 0.1562\n",
      "Best model updated at epoch 2 with val acc: 0.1562\n",
      "Epoch [3/3], Loss: 1.7995\n",
      "Epoch [3/3], Val Loss: 1.9185, Val Acc: 0.1875\n",
      "Best model updated at epoch 3 with val acc: 0.1875\n"
     ]
    }
   ],
   "source": [
    "best_val_metric = float('-inf')\n",
    "best_epoch = 0\n",
    "\n",
    "# Move model to the device\n",
    "model.to(device)\n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# Set the number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "# Set the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # --- 1. TRAINING LOOP ---\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move images and labels to the device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # --- 2. VALIDATION LOOP ---\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            # Move images and labels to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs).logits\n",
    "            val_loss   += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            preds      = outputs.argmax(dim=1)\n",
    "            correct   += (preds == labels).sum().item()\n",
    "            total     += inputs.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / total\n",
    "    val_acc      = correct / total\n",
    "    running_loss = 0.0\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # --- 3. UPDATE BEST MODEL ---\n",
    "    if val_acc > best_val_metric:\n",
    "        best_val_metric = val_acc\n",
    "        best_epoch      = epoch\n",
    "        torch.save(model.state_dict(), ARTIFACTS_FOLDER / f'best_{model_card.split(\"/\")[-1].replace(\"-\", \"_\")}.pth')\n",
    "    print(f\"Best model updated at epoch {best_epoch + 1} with val acc: {best_val_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44467fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 1.5189 seconds\n",
      "Test Loss: 1.9254, Test Acc: 0.1875\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(ARTIFACTS_FOLDER / f'best_{model_card.split(\"/\")[-1].replace(\"-\", \"_\")}.pth'))\n",
    "\n",
    "# --- 4. TESTING LOOP ---\n",
    "model.eval()\n",
    "test_loss, correct, total = 0.0, 0, 0\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Compute the inference time\n",
    "start_time = time()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move images and labels to the device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs).logits\n",
    "        test_loss   += criterion(outputs, labels).item() * inputs.size(0)\n",
    "        preds      = outputs.argmax(dim=1)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        correct   += (preds == labels).sum().item()\n",
    "        total     += inputs.size(0)\n",
    "end_time = time()\n",
    "# Compute the inference time\n",
    "inference_time = end_time - start_time\n",
    "print(f\"Inference time: {inference_time:.4f} seconds\")\n",
    "# Compute the average test loss and accuracy\n",
    "avg_test_loss = test_loss / total\n",
    "test_acc      = correct / total\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8793fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: Predicted  0  1  2  3  4  5  6  All\n",
      "True                               \n",
      "0          2  2  1  1  0  0  1    7\n",
      "1          0  1  0  0  0  1  0    2\n",
      "2          0  0  2  2  0  0  1    5\n",
      "3          3  0  1  0  1  0  1    6\n",
      "4          1  2  0  0  0  2  0    5\n",
      "5          0  1  0  0  0  1  1    3\n",
      "6          2  2  0  0  0  0  0    4\n",
      "All        8  8  4  3  1  4  4   32\n"
     ]
    }
   ],
   "source": [
    "# Compute the confusion matrix\n",
    "print(f\"Confusion matrix: {pd.crosstab(pd.Series(y_true), pd.Series(y_pred), rownames=['True'], colnames=['Predicted'], margins=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ec95a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                 Baby Care       0.25      0.29      0.27         7\n",
      "  Beauty and Personal Care       0.12      0.50      0.20         2\n",
      "                 Computers       0.50      0.40      0.44         5\n",
      "Home Decor & Festive Needs       0.00      0.00      0.00         6\n",
      "           Home Furnishing       0.00      0.00      0.00         5\n",
      "          Kitchen & Dining       0.25      0.33      0.29         3\n",
      "                   Watches       0.00      0.00      0.00         4\n",
      "\n",
      "                  accuracy                           0.19        32\n",
      "                 macro avg       0.16      0.22      0.17        32\n",
      "              weighted avg       0.16      0.19      0.17        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=classes, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea071149",
   "metadata": {},
   "source": [
    "## MOBILENETV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"google/mobilenet_v2_1.0_224\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e274bf",
   "metadata": {},
   "source": [
    "## **MAMBA S 1 K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b494483c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the averaged pool features: torch.Size([1, 768])\n",
      "Number of stages in extracted features: 4\n",
      "Size of extracted features in stage 1: torch.Size([1, 96, 56, 56])\n",
      "Size of extracted features in stage 4: torch.Size([1, 768, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"nvidia/MambaVision-S-1K\", trust_remote_code=True)\n",
    "\n",
    "# eval mode for inference\n",
    "model.cuda().eval()\n",
    "\n",
    "# prepare image for the model\n",
    "url = 'http://images.cocodataset.org/val2017/000000020247.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=False,\n",
    "                             mean=model.config.mean,\n",
    "                             std=model.config.std,\n",
    "                             crop_mode=model.config.crop_mode,\n",
    "                             crop_pct=model.config.crop_pct)\n",
    "inputs = transform(image).unsqueeze(0).cuda()\n",
    "# model inference\n",
    "out_avg_pool, features = model(inputs)\n",
    "print(\"Size of the averaged pool features:\", out_avg_pool.size())  # torch.Size([1, 768])\n",
    "print(\"Number of stages in extracted features:\", len(features)) # 4 stages\n",
    "print(\"Size of extracted features in stage 1:\", features[0].size()) # torch.Size([1, 96, 56, 56])\n",
    "print(\"Size of extracted features in stage 4:\", features[3].size()) # torch.Size([1, 768, 7, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0ad1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "\n",
    "class MambaClassifier(nn.Module):\n",
    "    def __init__(self, backbone, num_classes, hidden_dim=768): # Added hidden_dim\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.config = self.backbone.config\n",
    "        # self.classifier = nn.Linear(640, num_classes) # Original classifier\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes) # Modified classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_avg_pool, _ = self.backbone(x)\n",
    "        logits = self.classifier(out_avg_pool)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "933fb051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilités prédites: tensor([[0.1360, 0.1641, 0.1267, 0.1375, 0.1245, 0.1609, 0.1504]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mamba = MambaClassifier(model, num_classes=num_classes).cuda().eval()\n",
    "\n",
    "image = Image.open(img_test)\n",
    "\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=False,\n",
    "                             mean=mamba.config.mean,\n",
    "                             std=mamba.config.std,\n",
    "                             crop_mode=mamba.config.crop_mode,\n",
    "                             crop_pct=mamba.config.crop_pct)\n",
    "\n",
    "\n",
    "inputs = transform(image).unsqueeze(0).cuda()\n",
    "# Prédiction\n",
    "with torch.no_grad():\n",
    "    logits = mamba(inputs)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "print(\"Probabilités prédites:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5380a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaClassifier(AutoModel.from_pretrained(\"nvidia/MambaVision-S-1K\", trust_remote_code=True), num_classes=num_classes).cuda().train()\n",
    "\n",
    "image = Image.open(img_test)\n",
    "\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=True,\n",
    "                             mean=mamba.config.mean,\n",
    "                             std=mamba.config.std,\n",
    "                             crop_mode=mamba.config.crop_mode,\n",
    "                             crop_pct=mamba.config.crop_pct)\n",
    "\n",
    "dataset = ImageDataset(dataframe=train, image_dir=image_dir, transform=transform)\n",
    "\n",
    "# 5. Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b5f97d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.9875\n",
      "Epoch [2/3], Loss: 1.9445\n",
      "Epoch [3/3], Loss: 1.9724\n"
     ]
    }
   ],
   "source": [
    "# Training loop for the model with the train dataloader\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move model to the device\n",
    "model.to(device)\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# Set the number of epochs\n",
    "num_epochs = 3\n",
    "# Set the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in dataloader:\n",
    "        # Move images and labels to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # inputs = transform(image).unsqueeze(0).cuda()\n",
    "        # model inference\n",
    "        # outputs = model(inputs)\n",
    "        # end_time = time()\n",
    "        # logits = outputs[\"logits\"]\n",
    "        # predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "# Save the model\n",
    "# model.save_pretrained(\"mamba_model\")\n",
    "# processor.save_pretrained(\"mamba_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6d9d5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           Home Furnishing       0.00      0.00      0.00         4\n",
      "                 Baby Care       0.00      0.00      0.00         5\n",
      "                   Watches       0.00      0.00      0.00         4\n",
      "Home Decor & Festive Needs       0.00      0.00      0.00         6\n",
      "          Kitchen & Dining       0.40      0.40      0.40         5\n",
      "  Beauty and Personal Care       0.10      0.20      0.13         5\n",
      "                 Computers       0.00      0.00      0.00         3\n",
      "\n",
      "                  accuracy                           0.09        32\n",
      "                 macro avg       0.07      0.09      0.08        32\n",
      "              weighted avg       0.08      0.09      0.08        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference on test set\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Initialize the test dataset\n",
    "test_dataset = ImageDataset(dataframe=test, image_dir=image_dir, transform=transform)\n",
    "# Initialize the test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "# Initialize the predictions list\n",
    "predictions = []\n",
    "# Initialize the labels list\n",
    "labels_list = []\n",
    "# Inference loop\n",
    "for images, labels in test_dataloader:\n",
    "    # Move images to the device\n",
    "    images = images.to(device)\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    # Get the predicted labels\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    # Append the predictions and labels to the lists\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    labels_list.extend(labels.cpu().numpy())\n",
    "# Convert the predictions and labels to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "labels_list = np.array(labels_list)\n",
    "# Print the classification report\n",
    "print(classification_report(labels_list, predictions, target_names=classes))\n",
    "# Save the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da230c",
   "metadata": {},
   "source": [
    "## **MAMBA B 21 K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6d6291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaClassifier(AutoModel.from_pretrained(\"nvidia/MambaVision-B-21K\", trust_remote_code=True), num_classes=num_classes, hidden_dim=1024).cuda().train()\n",
    "\n",
    "image = Image.open(img_test)\n",
    "\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=True,\n",
    "                             mean=mamba.config.mean,\n",
    "                             std=mamba.config.std,\n",
    "                             crop_mode=mamba.config.crop_mode,\n",
    "                             crop_pct=mamba.config.crop_pct)\n",
    "\n",
    "dataset = ImageDataset(dataframe=train, image_dir=image_dir, transform=transform)\n",
    "\n",
    "# 5. Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbe35508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 2.1026\n",
      "Epoch [2/3], Loss: 2.0404\n",
      "Epoch [3/3], Loss: 1.9177\n"
     ]
    }
   ],
   "source": [
    "# Training loop for the model with the train dataloader\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move model to the device\n",
    "model.to(device)\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# Set the number of epochs\n",
    "num_epochs = 3\n",
    "# Set the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in dataloader:\n",
    "        # Move images and labels to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # inputs = transform(image).unsqueeze(0).cuda()\n",
    "        # model inference\n",
    "        # outputs = model(inputs)\n",
    "        # end_time = time()\n",
    "        # logits = outputs[\"logits\"]\n",
    "        # predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "# Save the model\n",
    "# model.save_pretrained(\"mamba_model\")\n",
    "# processor.save_pretrained(\"mamba_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23fb4b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           Home Furnishing       0.00      0.00      0.00         4\n",
      "                 Baby Care       0.50      0.40      0.44         5\n",
      "                   Watches       0.17      0.25      0.20         4\n",
      "Home Decor & Festive Needs       0.20      0.17      0.18         6\n",
      "          Kitchen & Dining       0.25      0.20      0.22         5\n",
      "  Beauty and Personal Care       0.25      0.20      0.22         5\n",
      "                 Computers       0.00      0.00      0.00         3\n",
      "\n",
      "                  accuracy                           0.19        32\n",
      "                 macro avg       0.20      0.17      0.18        32\n",
      "              weighted avg       0.21      0.19      0.20        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference on test set\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Initialize the test dataset\n",
    "test_dataset = ImageDataset(dataframe=test, image_dir=image_dir, transform=transform)\n",
    "# Initialize the test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "# Initialize the predictions list\n",
    "predictions = []\n",
    "# Initialize the labels list\n",
    "labels_list = []\n",
    "# Inference loop\n",
    "for images, labels in test_dataloader:\n",
    "    # Move images to the device\n",
    "    images = images.to(device)\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    # Get the predicted labels\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    # Append the predictions and labels to the lists\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    labels_list.extend(labels.cpu().numpy())\n",
    "# Convert the predictions and labels to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "labels_list = np.array(labels_list)\n",
    "# Print the classification report\n",
    "print(classification_report(labels_list, predictions, target_names=classes))\n",
    "# Save the predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
