{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07aa478c",
   "metadata": {},
   "source": [
    "- Boucle d'entraînement\n",
    "    - [ ] Définir proprement les métriques d'évaluation\n",
    "    - [ ] Keep track des métriques d'évaluation\n",
    "    - [ ] Afficher les métriques d'évaluation\n",
    "- Mamba\n",
    "    - [ ] Tester deux configurations de paramètres (celle du papier et une + classique)\n",
    "    - [ ] Créer un environnement avec mamba\n",
    "    - [ ] Installer les dépendances avec mamba\n",
    "- Tester tous les modèles à comparer\n",
    "    - [ ] Créer un script pour tester tous les modèles à comparer\n",
    "    - [ ] Créer un script pour tester tous les modèles à comparer avec mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d9aa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor, AutoModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from constants import ROOT_FOLDER, SEED, VAL_SIZE, TEST_SIZE, BATCH_SIZE, SAMPLING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651be0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  2.6.0+cu124\n",
      "Cuda version:  12.4\n",
      "CUDNN version:  90100\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Désactiver les alertes de FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Print the torch? cuda and cudnn version\n",
    "print(\"Torch version: \", torch.__version__)\n",
    "print(\"Cuda version: \", torch.version.cuda)\n",
    "print(\"CUDNN version: \", torch.backends.cudnn.version())\n",
    "\n",
    "# Set the device to GPU if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af879910",
   "metadata": {},
   "source": [
    "[COPY] Récupération des paramètres de train-test-split sur le jeu utilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ca3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the folder path containing the images\n",
    "IMAGE_FOLDER = ROOT_FOLDER / \"data\" / \"images\"\n",
    "IMAGE_TEST = ROOT_FOLDER / \"data\" / \"images\" / \"2aaa6083689193df5ab01fe37dea1b5e.jpg\"\n",
    "# Assign the folder path containing the former H5 efficientnet weights\n",
    "ARTIFACTS_FOLDER = ROOT_FOLDER / \"artifacts\"\n",
    "# Assign the folder path with the pickle dataset with labels, images filenames and metadata\n",
    "DATASET_PATH = ROOT_FOLDER / \"data\" / \"dataset_cleaned.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f68941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1050, 2)\n",
      "Dataset columns: Index(['image', 'class'], dtype='object')\n",
      "Number of classes: 7\n",
      "Classes: ['Baby Care', 'Beauty and Personal Care', 'Computers', 'Home Decor & Festive Needs', 'Home Furnishing', 'Kitchen & Dining', 'Watches']\n"
     ]
    }
   ],
   "source": [
    "# Loading the pickle dataset_cleaned used with the previous project as a pandas df\n",
    "df = pd.read_pickle(DATASET_PATH).drop(columns=['product_name', 'description'])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Dataset columns: {df.columns}\")\n",
    "\n",
    "# Encode the labels with LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df[\"class\"])\n",
    "n_classes = len(le.classes_)\n",
    "classes = le.classes_.tolist()\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# Finally transform the class column to the encoded labels\n",
    "df[\"class\"] = le.transform(df[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91d9afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (128, 2)\n",
      "Val shape: (128, 2)\n",
      "Test shape: (128, 2)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the datasets into train, val and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(df['image'], df['class'], test_size=TEST_SIZE, random_state=SEED, stratify=df['class'], shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=VAL_SIZE, random_state=SEED, stratify=y_temp, shuffle=True)\n",
    "\n",
    "# Concat X and y for each set\n",
    "train = pd.concat([X_train, y_train], axis=1).sample(SAMPLING) if SAMPLING else pd.concat([X_train, y_train], axis=1)\n",
    "val = pd.concat([X_val, y_val], axis=1).sample(SAMPLING) if SAMPLING else pd.concat([X_val, y_val], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1).sample(SAMPLING) if SAMPLING else pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Print the shape of each set\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Val shape: {val.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08d4ee5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "407026a2-d08c-4bfc-ab42-89d9e3e975a8",
       "rows": [
        [
         "739",
         "52877a6306aef18af67ab54c8233c931.jpg",
         "5"
        ],
        [
         "366",
         "47d7792e50e69b048b1f17176f170141.jpg",
         "1"
        ],
        [
         "687",
         "d61b368146f83075937e144dab93c6a1.jpg",
         "1"
        ],
        [
         "305",
         "906b5b16c78ba1c718501138702cb32c.jpg",
         "3"
        ],
        [
         "610",
         "2414e4b7e5948263b8dbb4843557be4d.jpg",
         "3"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>52877a6306aef18af67ab54c8233c931.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>47d7792e50e69b048b1f17176f170141.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>d61b368146f83075937e144dab93c6a1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>906b5b16c78ba1c718501138702cb32c.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>2414e4b7e5948263b8dbb4843557be4d.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "739  52877a6306aef18af67ab54c8233c931.jpg      5\n",
       "366  47d7792e50e69b048b1f17176f170141.jpg      1\n",
       "687  d61b368146f83075937e144dab93c6a1.jpg      1\n",
       "305  906b5b16c78ba1c718501138702cb32c.jpg      3\n",
       "610  2414e4b7e5948263b8dbb4843557be4d.jpg      3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4eede8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a5df066a-8601-477b-be47-33376690bc2c",
       "rows": [
        [
         "330",
         "394c2c627914e1eed9b8ac343583a679.jpg",
         "2"
        ],
        [
         "465",
         "da2ce8d17a17f9d19b368434aedea5e9.jpg",
         "0"
        ],
        [
         "401",
         "50be4cd92fda2755e1e884421a52e345.jpg",
         "1"
        ],
        [
         "304",
         "95cb9561009bd6707c67f05e6a00c16e.jpg",
         "3"
        ],
        [
         "596",
         "e4922f01eda047582cd72e9d1063ab7a.jpg",
         "3"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>394c2c627914e1eed9b8ac343583a679.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>da2ce8d17a17f9d19b368434aedea5e9.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>50be4cd92fda2755e1e884421a52e345.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>95cb9561009bd6707c67f05e6a00c16e.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>e4922f01eda047582cd72e9d1063ab7a.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "330  394c2c627914e1eed9b8ac343583a679.jpg      2\n",
       "465  da2ce8d17a17f9d19b368434aedea5e9.jpg      0\n",
       "401  50be4cd92fda2755e1e884421a52e345.jpg      1\n",
       "304  95cb9561009bd6707c67f05e6a00c16e.jpg      3\n",
       "596  e4922f01eda047582cd72e9d1063ab7a.jpg      3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ba2b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "59205f9a-9622-4c9d-a27d-82503558ecbe",
       "rows": [
        [
         "1046",
         "fd6cbcc22efb6b761bd564c28928483c.jpg",
         "0"
        ],
        [
         "805",
         "0b793c1727ef52285a25dedf8b86626b.jpg",
         "1"
        ],
        [
         "480",
         "88ca277594f22bb7494f2d0bbc51f8ba.jpg",
         "2"
        ],
        [
         "1049",
         "f2f027ad6a6df617c9f125173da71e44.jpg",
         "0"
        ],
        [
         "591",
         "62a1044ba64031b9e40f98a4fb890d9d.jpg",
         "4"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>fd6cbcc22efb6b761bd564c28928483c.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>0b793c1727ef52285a25dedf8b86626b.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>88ca277594f22bb7494f2d0bbc51f8ba.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>f2f027ad6a6df617c9f125173da71e44.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>62a1044ba64031b9e40f98a4fb890d9d.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     image  class\n",
       "1046  fd6cbcc22efb6b761bd564c28928483c.jpg      0\n",
       "805   0b793c1727ef52285a25dedf8b86626b.jpg      1\n",
       "480   88ca277594f22bb7494f2d0bbc51f8ba.jpg      2\n",
       "1049  f2f027ad6a6df617c9f125173da71e44.jpg      0\n",
       "591   62a1044ba64031b9e40f98a4fb890d9d.jpg      4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8012a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block DataLoader\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing image file names and labels.\n",
    "            image_dir (str): Directory where images are stored.\n",
    "            processor (AutoImageProcessor, optional):  Hugging Face processor for image preprocessing. Defaults to None.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])  # Assuming image file names are in the first column\n",
    "        image = Image.open(img_name).convert('RGB')  # Ensure consistent color format\n",
    "\n",
    "        label = self.dataframe.iloc[idx, 1]  # Assuming labels are in the second column\n",
    "\n",
    "        if self.processor:\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            image = inputs['pixel_values'].squeeze()  # Remove batch dimension\n",
    "        elif self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116edde",
   "metadata": {},
   "source": [
    "# **EXPERIMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe2b48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_to_markdown(y_true, y_pred, target_names):\n",
    "    \"\"\"\n",
    "    Convert the classification report to a markdown table.\n",
    "    \"\"\"\n",
    "    report = classification_report(y_true, y_pred, target_names=target_names, zero_division=0, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df = report_df.drop(columns=['support'])\n",
    "    report_df = report_df.rename_axis('Classes').reset_index()\n",
    "    return report_df.to_markdown(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8edb3b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: AutoModelForImageClassification, experiment_id: str, test_loader: DataLoader, criterion: nn.Module, writer: SummaryWriter = None):\n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(ARTIFACTS_FOLDER / f'{experiment_id}.pth'))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the test metrics\n",
    "    test_loss, correct_test, total_test = .0, 0, 0\n",
    "    running_test_steps = 0\n",
    "    running_test_time_by_step = .0\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            step_time = time()\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(inputs).logits\n",
    "            test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct_test += (preds == labels).sum().item()\n",
    "            total_test += inputs.size(0)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            running_test_time_by_step += time() - step_time\n",
    "            running_test_steps += 1\n",
    "            if writer:\n",
    "                writer.add_scalar('TimingByStep/test', running_test_time_by_step, running_test_steps)\n",
    "\n",
    "    epoch_test_loss = test_loss / total_test\n",
    "    epoch_test_acc  = correct_test / total_test\n",
    "    if writer:\n",
    "        writer.add_scalar('Accuracy/test', epoch_test_acc, 0)\n",
    "\n",
    "    print(f\"Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_acc:.4f}\")\n",
    "    \n",
    "    # Save Classification report in tensorboard\n",
    "    classification_report_md = classification_report_to_markdown(y_true, y_pred, target_names=classes)\n",
    "    cm = ConfusionMatrixDisplay.from_predictions(y_true, y_pred, labels=range(n_classes), normalize='true', display_labels=classes, xticks_rotation=\"vertical\", cmap=plt.cm.Blues)\n",
    "    # Save the classification report as a markdown file in tensorboard\n",
    "    if writer:\n",
    "        writer.add_text('ClassificationReport/test', classification_report_md, 0)\n",
    "        writer.add_figure('ConfusionMatrix/test', cm.figure_, 0)\n",
    "    # Delete the model from GPU memory\n",
    "    del model, inputs, labels, outputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2fec3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_amfic(\n",
    "    model: AutoModelForImageClassification,\n",
    "    model_card: str,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    num_epochs: int,):\n",
    "    # --- LOOP ---\n",
    "    # Initialize SummaryWriter\n",
    "    experiment_id = \"_\".join([datetime.now().strftime(\"%Y%m%d-%H%M%S\"), model_card.split(\"/\")[-1]])\n",
    "    log_dir = ROOT_FOLDER / os.getcwd().split(\"/\")[-1] / \"runs\" / \"_\".join([experiment_id, model_card.split(\"/\")[-1]])\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # Initialize commun metrics\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Initialize the training metrics\n",
    "    running_train_time_by_step = .0\n",
    "    running_train_time_by_epoch = .0\n",
    "    running_train_steps = 0\n",
    "    writer.add_scalar('TimingByEpoch/train', running_train_time_by_epoch, 0)\n",
    "\n",
    "    # Initialize the validation metrics\n",
    "    best_val_metric = float('-inf')\n",
    "    running_val_time_by_step = .0\n",
    "    running_val_time_by_epoch = .0\n",
    "    running_val_steps = 0\n",
    "    writer.add_scalar('TimingByEpoch/validation', running_val_time_by_epoch, 0)\n",
    "    \n",
    "    writer.add_scalar('TimingByEpoch/train', running_train_time_by_epoch, 0)\n",
    "    # Move model to the device\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- 1. TRAINING LOOP ---\n",
    "        model.train()\n",
    "        train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "        epoch_time = time()\n",
    "        for i, (inputs, labels) in enumerate(train_loader): # Use enumerate for step count\n",
    "            step_time = time()\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            preds_train = outputs.argmax(dim=1)\n",
    "            correct_train += (preds_train == labels).sum().item()\n",
    "            total_train += inputs.size(0)\n",
    "            running_train_steps += 1\n",
    "            running_train_time_by_step += time() - step_time\n",
    "            writer.add_scalar('TimingByStep/train', running_train_time_by_step, running_train_steps)\n",
    "\n",
    "        running_train_time_by_epoch += time() - epoch_time\n",
    "        epoch_train_loss = train_loss / total_train\n",
    "        epoch_train_acc = correct_train / total_train\n",
    "        # Log training metrics per epoch\n",
    "        writer.add_scalar('Loss/train', epoch_train_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', epoch_train_acc, epoch)\n",
    "        writer.add_scalar('TimingByEpoch/train', running_train_time_by_epoch, epoch)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}\")\n",
    "\n",
    "\n",
    "        # --- 2. VALIDATION LOOP ---\n",
    "        model.eval()\n",
    "        val_loss, correct_val, total_val = .0, 0, 0\n",
    "        epoch_time = time()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move images and labels to the device\n",
    "                step_time = time()\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                outputs = model(inputs).logits\n",
    "                val_loss   += criterion(outputs, labels).item() * inputs.size(0)\n",
    "                preds      = outputs.argmax(dim=1)\n",
    "                correct_val   += (preds == labels).sum().item()\n",
    "                total_val     += inputs.size(0)\n",
    "                running_val_steps += 1\n",
    "                running_val_time_by_step += time() - step_time\n",
    "                writer.add_scalar('TimingByStep/validation', running_val_time_by_step, running_val_steps)\n",
    "\n",
    "        running_val_time_by_epoch += time() - epoch_time\n",
    "        epoch_val_loss = val_loss / total_val\n",
    "        epoch_val_acc  = correct_val / total_val\n",
    "        # Log validation metrics per epoch\n",
    "        writer.add_scalar('Loss/validation', epoch_val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/validation', epoch_val_acc, epoch)\n",
    "        writer.add_scalar('TimingByEpoch/validation', running_val_time_by_epoch, epoch)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "        # --- 3. UPDATE BEST MODEL ---\n",
    "        # Save the model if the validation accuracy is better than the best one\n",
    "        if epoch_val_acc > best_val_metric:\n",
    "            best_val_metric = epoch_val_acc\n",
    "            best_epoch      = epoch\n",
    "            torch.save(model.state_dict(), ARTIFACTS_FOLDER / f'{experiment_id}.pth')\n",
    "            print(f\"Best model updated at epoch {best_epoch} with val acc: {best_val_metric:.4f}\")\n",
    "    \n",
    "    # Delete the model from GPU memory\n",
    "    del inputs, labels, outputs, optimizer, loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- 4. TESTING LOOP ---\n",
    "    test_model(model, experiment_id, test_loader, criterion, writer)\n",
    "\n",
    "    # --- After the training loop ---\n",
    "    writer.close() # Close the writer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03437169",
   "metadata": {},
   "source": [
    "## **Google VIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d242b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "# Assigne the model card name\n",
    "model_card = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Define the model\n",
    "model = AutoModelForImageClassification.from_pretrained(model_card, num_labels=n_classes, trust_remote_code=True)\n",
    "\n",
    "# Define the image processor\n",
    "processor = AutoImageProcessor.from_pretrained(model_card)\n",
    "\n",
    "# Prepare the dataloaders for training, validation and testing\n",
    "dataset = ImageDataset(dataframe=train, image_dir=IMAGE_FOLDER, processor=processor)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataset = ImageDataset(dataframe=val, image_dir=IMAGE_FOLDER, processor=processor)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "dataset = ImageDataset(dataframe=test, image_dir=IMAGE_FOLDER, processor=processor)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7b4e2e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5], Train Loss: 1.9501, Train Acc: 0.1016\n",
      "Epoch [0/5], Val Loss: 1.9016, Val Acc: 0.2891\n",
      "Best model updated at epoch 0 with val acc: 0.2891\n",
      "Epoch [1/5], Train Loss: 1.8419, Train Acc: 0.4844\n",
      "Epoch [1/5], Val Loss: 1.8628, Val Acc: 0.4141\n",
      "Best model updated at epoch 1 with val acc: 0.4141\n",
      "Epoch [2/5], Train Loss: 1.7522, Train Acc: 0.7500\n",
      "Epoch [2/5], Val Loss: 1.8219, Val Acc: 0.5312\n",
      "Best model updated at epoch 2 with val acc: 0.5312\n",
      "Epoch [3/5], Train Loss: 1.6580, Train Acc: 0.8750\n",
      "Epoch [3/5], Val Loss: 1.7769, Val Acc: 0.6094\n",
      "Best model updated at epoch 3 with val acc: 0.6094\n",
      "Epoch [4/5], Train Loss: 1.5608, Train Acc: 0.9531\n",
      "Epoch [4/5], Val Loss: 1.7335, Val Acc: 0.6328\n",
      "Best model updated at epoch 4 with val acc: 0.6328\n",
      "Test Loss: 1.7634, Test Acc: 0.5547\n"
     ]
    }
   ],
   "source": [
    "# TODO : Ajouter une card dans le tensorboard pour les paramètres du modèle\n",
    "# TODO : warning PIL decompression bomb warning\n",
    "# TODO : Nettoyer les fichiers useless\n",
    "\n",
    "# Set the training parameters\n",
    "num_epochs = 5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Run the training workflow for AutoModelForImageClassification\n",
    "train_and_eval_amfic(\n",
    "    model = model,\n",
    "    model_card = model_card,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    test_loader = test_loader,\n",
    "    optimizer = optimizer,\n",
    "    criterion = criterion,\n",
    "    num_epochs = num_epochs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea071149",
   "metadata": {},
   "source": [
    "## MOBILENETV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"google/mobilenet_v2_1.0_224\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e274bf",
   "metadata": {},
   "source": [
    "## **MAMBA S 1 K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b494483c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the averaged pool features: torch.Size([1, 768])\n",
      "Number of stages in extracted features: 4\n",
      "Size of extracted features in stage 1: torch.Size([1, 96, 56, 56])\n",
      "Size of extracted features in stage 4: torch.Size([1, 768, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"nvidia/MambaVision-S-1K\", trust_remote_code=True)\n",
    "\n",
    "# eval mode for inference\n",
    "model.cuda().eval()\n",
    "\n",
    "# prepare image for the model\n",
    "url = 'http://images.cocodataset.org/val2017/000000020247.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=False,\n",
    "                             mean=model.config.mean,\n",
    "                             std=model.config.std,\n",
    "                             crop_mode=model.config.crop_mode,\n",
    "                             crop_pct=model.config.crop_pct)\n",
    "inputs = transform(image).unsqueeze(0).cuda()\n",
    "# model inference\n",
    "out_avg_pool, features = model(inputs)\n",
    "print(\"Size of the averaged pool features:\", out_avg_pool.size())  # torch.Size([1, 768])\n",
    "print(\"Number of stages in extracted features:\", len(features)) # 4 stages\n",
    "print(\"Size of extracted features in stage 1:\", features[0].size()) # torch.Size([1, 96, 56, 56])\n",
    "print(\"Size of extracted features in stage 4:\", features[3].size()) # torch.Size([1, 768, 7, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0ad1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "\n",
    "class MambaClassifier(nn.Module):\n",
    "    def __init__(self, backbone, num_classes, hidden_dim=768): # Added hidden_dim\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.config = self.backbone.config\n",
    "        # self.classifier = nn.Linear(640, num_classes) # Original classifier\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes) # Modified classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_avg_pool, _ = self.backbone(x)\n",
    "        logits = self.classifier(out_avg_pool)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "933fb051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilités prédites: tensor([[0.1360, 0.1641, 0.1267, 0.1375, 0.1245, 0.1609, 0.1504]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mamba = MambaClassifier(model, num_classes=num_classes).cuda().eval()\n",
    "\n",
    "image = Image.open(img_test)\n",
    "\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=False,\n",
    "                             mean=mamba.config.mean,\n",
    "                             std=mamba.config.std,\n",
    "                             crop_mode=mamba.config.crop_mode,\n",
    "                             crop_pct=mamba.config.crop_pct)\n",
    "\n",
    "\n",
    "inputs = transform(image).unsqueeze(0).cuda()\n",
    "# Prédiction\n",
    "with torch.no_grad():\n",
    "    logits = mamba(inputs)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "print(\"Probabilités prédites:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5380a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaClassifier(AutoModel.from_pretrained(\"nvidia/MambaVision-S-1K\", trust_remote_code=True), num_classes=num_classes).cuda().train()\n",
    "\n",
    "image = Image.open(img_test)\n",
    "\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=True,\n",
    "                             mean=mamba.config.mean,\n",
    "                             std=mamba.config.std,\n",
    "                             crop_mode=mamba.config.crop_mode,\n",
    "                             crop_pct=mamba.config.crop_pct)\n",
    "\n",
    "dataset = ImageDataset(dataframe=train, image_dir=image_dir, transform=transform)\n",
    "\n",
    "# 5. Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b5f97d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.9875\n",
      "Epoch [2/3], Loss: 1.9445\n",
      "Epoch [3/3], Loss: 1.9724\n"
     ]
    }
   ],
   "source": [
    "# Training loop for the model with the train dataloader\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move model to the device\n",
    "model.to(device)\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# Set the number of epochs\n",
    "num_epochs = 3\n",
    "# Set the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in dataloader:\n",
    "        # Move images and labels to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # inputs = transform(image).unsqueeze(0).cuda()\n",
    "        # model inference\n",
    "        # outputs = model(inputs)\n",
    "        # end_time = time()\n",
    "        # logits = outputs[\"logits\"]\n",
    "        # predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "# Save the model\n",
    "# model.save_pretrained(\"mamba_model\")\n",
    "# processor.save_pretrained(\"mamba_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6d9d5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           Home Furnishing       0.00      0.00      0.00         4\n",
      "                 Baby Care       0.00      0.00      0.00         5\n",
      "                   Watches       0.00      0.00      0.00         4\n",
      "Home Decor & Festive Needs       0.00      0.00      0.00         6\n",
      "          Kitchen & Dining       0.40      0.40      0.40         5\n",
      "  Beauty and Personal Care       0.10      0.20      0.13         5\n",
      "                 Computers       0.00      0.00      0.00         3\n",
      "\n",
      "                  accuracy                           0.09        32\n",
      "                 macro avg       0.07      0.09      0.08        32\n",
      "              weighted avg       0.08      0.09      0.08        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference on test set\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Initialize the test dataset\n",
    "test_dataset = ImageDataset(dataframe=test, image_dir=image_dir, transform=transform)\n",
    "# Initialize the test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "# Initialize the predictions list\n",
    "predictions = []\n",
    "# Initialize the labels list\n",
    "labels_list = []\n",
    "# Inference loop\n",
    "for images, labels in test_dataloader:\n",
    "    # Move images to the device\n",
    "    images = images.to(device)\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    # Get the predicted labels\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    # Append the predictions and labels to the lists\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    labels_list.extend(labels.cpu().numpy())\n",
    "# Convert the predictions and labels to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "labels_list = np.array(labels_list)\n",
    "# Print the classification report\n",
    "print(classification_report(labels_list, predictions, target_names=classes))\n",
    "# Save the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da230c",
   "metadata": {},
   "source": [
    "## **MAMBA B 21 K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6d6291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaClassifier(AutoModel.from_pretrained(\"nvidia/MambaVision-B-21K\", trust_remote_code=True), num_classes=num_classes, hidden_dim=1024).cuda().train()\n",
    "\n",
    "image = Image.open(img_test)\n",
    "\n",
    "input_resolution = (3, 224, 224)  # MambaVision supports any input resolutions\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=True,\n",
    "                             mean=mamba.config.mean,\n",
    "                             std=mamba.config.std,\n",
    "                             crop_mode=mamba.config.crop_mode,\n",
    "                             crop_pct=mamba.config.crop_pct)\n",
    "\n",
    "dataset = ImageDataset(dataframe=train, image_dir=image_dir, transform=transform)\n",
    "\n",
    "# 5. Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbe35508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 2.1026\n",
      "Epoch [2/3], Loss: 2.0404\n",
      "Epoch [3/3], Loss: 1.9177\n"
     ]
    }
   ],
   "source": [
    "# Training loop for the model with the train dataloader\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move model to the device\n",
    "model.to(device)\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# Set the number of epochs\n",
    "num_epochs = 3\n",
    "# Set the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in dataloader:\n",
    "        # Move images and labels to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # inputs = transform(image).unsqueeze(0).cuda()\n",
    "        # model inference\n",
    "        # outputs = model(inputs)\n",
    "        # end_time = time()\n",
    "        # logits = outputs[\"logits\"]\n",
    "        # predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "# Save the model\n",
    "# model.save_pretrained(\"mamba_model\")\n",
    "# processor.save_pretrained(\"mamba_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23fb4b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           Home Furnishing       0.00      0.00      0.00         4\n",
      "                 Baby Care       0.50      0.40      0.44         5\n",
      "                   Watches       0.17      0.25      0.20         4\n",
      "Home Decor & Festive Needs       0.20      0.17      0.18         6\n",
      "          Kitchen & Dining       0.25      0.20      0.22         5\n",
      "  Beauty and Personal Care       0.25      0.20      0.22         5\n",
      "                 Computers       0.00      0.00      0.00         3\n",
      "\n",
      "                  accuracy                           0.19        32\n",
      "                 macro avg       0.20      0.17      0.18        32\n",
      "              weighted avg       0.21      0.19      0.20        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference on test set\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Initialize the test dataset\n",
    "test_dataset = ImageDataset(dataframe=test, image_dir=image_dir, transform=transform)\n",
    "# Initialize the test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "# Initialize the predictions list\n",
    "predictions = []\n",
    "# Initialize the labels list\n",
    "labels_list = []\n",
    "# Inference loop\n",
    "for images, labels in test_dataloader:\n",
    "    # Move images to the device\n",
    "    images = images.to(device)\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    # Get the predicted labels\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    # Append the predictions and labels to the lists\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    labels_list.extend(labels.cpu().numpy())\n",
    "# Convert the predictions and labels to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "labels_list = np.array(labels_list)\n",
    "# Print the classification report\n",
    "print(classification_report(labels_list, predictions, target_names=classes))\n",
    "# Save the predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
