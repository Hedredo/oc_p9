{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65a37f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Continuer à travailler la classe pour l'évaluation sur le jeu de test et finir le pipeline\n",
    "# TODO: Intégrer les métriques du jeu de test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bda673",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "#TODO Ce notebook présente une analyse de données sur les ventes de jeux vidéo. L'objectif est d'explorer les tendances des ventes en fonction de la plateforme, du genre et de la région."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b7fc58",
   "metadata": {},
   "source": [
    "# PRE-REQUIS\n",
    "\n",
    "Ce bloc contient tout ce qui est nécessaire pour le fonctionnement des expériences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae5ce0b",
   "metadata": {},
   "source": [
    "## Imports & Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93f0c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from pathlib import PosixPath\n",
    "import re\n",
    "import io\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from constants import ROOT_FOLDER, IMAGE_FOLDER, ARTIFACTS_FOLDER, DATASET_PATH\n",
    "from constants import SEED, VAL_SIZE, TEST_SIZE, BATCH_SIZE, SAMPLING, INPUT_RESOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a149778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion des avertissements\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2835ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.19.0\n",
      "Cuda version:  12.8\n",
      "CUDNN version:  9.3.0.75\n",
      "\n",
      "Tensorflow using GPU:  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# Configuration de cuda avec PyTorch\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "cuda_version = subprocess.check_output([\"nvidia-smi\", \"--version\"]).decode().strip().split(': ')[-1]\n",
    "print(\"Cuda version: \", cuda_version)\n",
    "cudnn_version = subprocess.check_output([\"grep\", \"-oPm 1\", \"nvidia_cudnn_cu12-\\K[0-9.]+(?=-py3)\", \"uv.lock\"]).decode().strip()\n",
    "print(\"CUDNN version: \", cudnn_version)\n",
    "print()\n",
    "print(\"Tensorflow using GPU: \", tf.config.list_physical_devices('GPU')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c96a7",
   "metadata": {},
   "source": [
    "## Classes et Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73c30b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block DataLoader\n",
    "class ImageDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe: pd.DataFrame,\n",
    "        image_dir: PosixPath,\n",
    "        processor=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing image file names and labels.\n",
    "            image_dir (PosixPath): Directory where images are stored.\n",
    "            processor (AutoImageProcessor, optional): Hugging Face processor for image preprocessing. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.dataframe) / BATCH_SIZE))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Check if the index is valid\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\n",
    "                f\"Index {idx} out of range for dataset of length {len(self)}\"\n",
    "            )\n",
    "        # Get the batch of data\n",
    "        batch_data = self.dataframe.iloc[idx * BATCH_SIZE : (idx + 1) * BATCH_SIZE, :]\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for _, row in batch_data.iterrows():\n",
    "            name, label = row\n",
    "            img_name = (\n",
    "                self.image_dir / name\n",
    "            )  # Assuming image file names are in the first column\n",
    "            image = Image.open(img_name).convert(\n",
    "                \"RGB\"\n",
    "            )  # Ensure consistent color format\n",
    "            image = image.resize(\n",
    "                (INPUT_RESOLUTION[0], INPUT_RESOLUTION[1]), Image.Resampling.BILINEAR\n",
    "            )  # Resize to model input size\n",
    "\n",
    "            if self.processor:\n",
    "                image = self.processor(image)\n",
    "\n",
    "            images.append(np.array(image))\n",
    "            labels.append(label)\n",
    "\n",
    "        # Convert to TensorFlow tensors\n",
    "        images = tf.convert_to_tensor(images, dtype=tf.float32)\n",
    "        labels = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "        labels = tf.one_hot(labels, depth=N_CLASSES)\n",
    "\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6d4c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, train_path, val_path, test_path):\n",
    "    # Splitting the datasets into train, val and test sets\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        df[\"image\"],\n",
    "        df[\"class\"],\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=SEED,\n",
    "        stratify=df[\"class\"],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=VAL_SIZE,\n",
    "        random_state=SEED,\n",
    "        stratify=y_temp,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Concat X and y for each set\n",
    "    train = (\n",
    "        pd.concat([X_train, y_train], axis=1).sample(SAMPLING)\n",
    "        if SAMPLING\n",
    "        else pd.concat([X_train, y_train], axis=1)\n",
    "    )\n",
    "    train.to_pickle(train_path)\n",
    "    val = (\n",
    "        pd.concat([X_val, y_val], axis=1).sample(SAMPLING)\n",
    "        if SAMPLING\n",
    "        else pd.concat([X_val, y_val], axis=1)\n",
    "    )\n",
    "    val.to_pickle(val_path)\n",
    "    test = (\n",
    "        pd.concat([X_test, y_test], axis=1).sample(SAMPLING)\n",
    "        if SAMPLING\n",
    "        else pd.concat([X_test, y_test], axis=1)\n",
    "    )\n",
    "    test.to_pickle(test_path)\n",
    "\n",
    "\n",
    "def load_splits(train_path, val_path, test_path):\n",
    "    # Load the saved files if they exist\n",
    "    try:\n",
    "        train = pd.read_pickle(train_path)\n",
    "        val = pd.read_pickle(val_path)\n",
    "        test = pd.read_pickle(test_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        print(\"This file has not been found. Please check the paths before.\")\n",
    "\n",
    "    # Finally print the shapes of the datasets\n",
    "    print(f\"Train shape: {train.shape}\")\n",
    "    print(f\"Val shape: {val.shape}\")\n",
    "    print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f207939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prewarming the model\n",
    "def warming_up(model, dataset):\n",
    "    \"\"\"\n",
    "    Warming up the model by running a batch through it.\n",
    "    \"\"\"\n",
    "    for x_batch in dataset:\n",
    "        inputs, labels = x_batch\n",
    "        _ = model.predict(inputs)\n",
    "        print(\"Warming up the model...\")\n",
    "        break\n",
    "\n",
    "\n",
    "def plot_to_image(fig):\n",
    "    \"\"\"Convert a matplotlib figure to a PNG image with a batch dim.\"\"\"\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format=\"png\")  # Sauvegarde la figure dans un buffer\n",
    "    plt.close(fig)  # Libère la mémoire\n",
    "    buf.seek(0)  # Repositionne le curseur au début\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)  # Décodage en tensor RVBA\n",
    "    image = tf.expand_dims(image, 0)  # Ajoute une dimension pour le batch\n",
    "    return image\n",
    "\n",
    "\n",
    "def split_labels_on_and_or_ampersand(labels):\n",
    "    \"\"\"\n",
    "    Insert a newline after each '&' ou 'and' in label names.\n",
    "    \"\"\"\n",
    "    return [re.sub(r\"\\s*(and|&)\\s*\", r\"\\n\\1 \", label) for label in labels]\n",
    "\n",
    "\n",
    "def generate_experiment_id(model_card, freeze_backbone):\n",
    "    \"\"\"\n",
    "    Generate a unique experiment ID based on the current date and time.\n",
    "    \"\"\"\n",
    "    freeze_str = \"freezed\" if freeze_backbone else \"unfreezed\"\n",
    "    return \"_\".join(\n",
    "        [\n",
    "            datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "            model_card.split(\"/\")[-1],\n",
    "            freeze_str,\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc1f6e8",
   "metadata": {},
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add7b38c",
   "metadata": {},
   "source": [
    "### Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61bfcfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1050, 2)\n",
      "Dataset columns: Index(['image', 'class'], dtype='object')\n",
      "Number of classes: 7\n",
      "Classes: ['Baby Care', 'Beauty and Personal Care', 'Computers', 'Home Decor & Festive Needs', 'Home Furnishing', 'Kitchen & Dining', 'Watches']\n"
     ]
    }
   ],
   "source": [
    "# Loading the pickle dataset_cleaned used with the previous project as a pandas df\n",
    "df = pd.read_pickle(DATASET_PATH).drop(columns=[\"product_name\", \"description\"])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Dataset columns: {df.columns}\")\n",
    "\n",
    "# Encode the labels with LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df[\"class\"])\n",
    "N_CLASSES = len(le.classes_)\n",
    "CLASSES = le.classes_.tolist()\n",
    "print(f\"Number of classes: {N_CLASSES}\")\n",
    "print(f\"Classes: {CLASSES}\")\n",
    "\n",
    "# Finally transform the class column to the encoded labels\n",
    "df[\"class\"] = le.transform(df[\"class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4a89e",
   "metadata": {},
   "source": [
    "### Séparation des données (train/validation/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "222888e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (758, 2)\n",
      "Val shape: (134, 2)\n",
      "Test shape: (158, 2)\n"
     ]
    }
   ],
   "source": [
    "# Define the path to save the splitted cleaned datasets\n",
    "completion = SAMPLING if SAMPLING else \"full\"\n",
    "train_path = ROOT_FOLDER / \"data\" / f\"trainset_{completion}.pickle\"\n",
    "val_path = ROOT_FOLDER / \"data\" / f\"valset_{completion}.pickle\"\n",
    "test_path = ROOT_FOLDER / \"data\" / f\"testset_{completion}.pickle\"\n",
    "\n",
    "# Load the splitted datasets if they exist\n",
    "if (\n",
    "    os.path.exists(train_path)\n",
    "    and os.path.exists(val_path)\n",
    "    and os.path.exists(test_path)\n",
    "):\n",
    "    train, val, test = load_splits(train_path, val_path, test_path)\n",
    "else:\n",
    "    # If the one or more files do not exist, split the dataset and save/overwrite the files\n",
    "    split_dataset(df, train_path, val_path, test_path)\n",
    "    train, val, test = load_splits(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0e30435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "38805386-a639-4bc3-ba66-cab011035d5e",
       "rows": [
        [
         "229",
         "caabe6014b914fe2874a9a8d7284f79b.jpg",
         "3"
        ],
        [
         "450",
         "95feec21a9d076cff084159d61bf9b8e.jpg",
         "0"
        ],
        [
         "798",
         "9993de7e2bcced43dc9edb3b2c81f23d.jpg",
         "1"
        ],
        [
         "230",
         "968a2b3be84193e3f755c2fe71033a2c.jpg",
         "3"
        ],
        [
         "293",
         "c2efa8aa11898bdb5fc4e46201973a42.jpg",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>caabe6014b914fe2874a9a8d7284f79b.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>95feec21a9d076cff084159d61bf9b8e.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>9993de7e2bcced43dc9edb3b2c81f23d.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>968a2b3be84193e3f755c2fe71033a2c.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>c2efa8aa11898bdb5fc4e46201973a42.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "229  caabe6014b914fe2874a9a8d7284f79b.jpg      3\n",
       "450  95feec21a9d076cff084159d61bf9b8e.jpg      0\n",
       "798  9993de7e2bcced43dc9edb3b2c81f23d.jpg      1\n",
       "230  968a2b3be84193e3f755c2fe71033a2c.jpg      3\n",
       "293  c2efa8aa11898bdb5fc4e46201973a42.jpg      0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b854797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "da4f3968-a24a-42d4-8a90-5636f2431711",
       "rows": [
        [
         "979",
         "c44a5dc5b5ebe5b3e0535b7c2b7921e4.jpg",
         "0"
        ],
        [
         "49",
         "02a53d335775b652f22f41b529b9d646.jpg",
         "1"
        ],
        [
         "567",
         "97fba8a02361aa56eaa9fa51bc1d7661.jpg",
         "6"
        ],
        [
         "494",
         "a124d6e4c30b00918c594289266a383c.jpg",
         "6"
        ],
        [
         "773",
         "109e235d4838002246599f987d935c21.jpg",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>c44a5dc5b5ebe5b3e0535b7c2b7921e4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>02a53d335775b652f22f41b529b9d646.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>97fba8a02361aa56eaa9fa51bc1d7661.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>a124d6e4c30b00918c594289266a383c.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>109e235d4838002246599f987d935c21.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "979  c44a5dc5b5ebe5b3e0535b7c2b7921e4.jpg      0\n",
       "49   02a53d335775b652f22f41b529b9d646.jpg      1\n",
       "567  97fba8a02361aa56eaa9fa51bc1d7661.jpg      6\n",
       "494  a124d6e4c30b00918c594289266a383c.jpg      6\n",
       "773  109e235d4838002246599f987d935c21.jpg      0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f3dd379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8beb3e17-24cf-4463-926b-80a4ab3b8c66",
       "rows": [
        [
         "11",
         "08452abdadb3db1e686b94a9c52fc7b6.jpg",
         "6"
        ],
        [
         "548",
         "2541b59d54a3a9f2681c0049f7ddd85c.jpg",
         "6"
        ],
        [
         "696",
         "82fbc93cd45ab747e7e606f2c52c7335.jpg",
         "3"
        ],
        [
         "238",
         "2e8df36b35d22cf219cf8bae6c2af752.jpg",
         "5"
        ],
        [
         "963",
         "bcb51cec3d290e6a661586d0df30e091.jpg",
         "4"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>08452abdadb3db1e686b94a9c52fc7b6.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>2541b59d54a3a9f2681c0049f7ddd85c.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>82fbc93cd45ab747e7e606f2c52c7335.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2e8df36b35d22cf219cf8bae6c2af752.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>bcb51cec3d290e6a661586d0df30e091.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image  class\n",
       "11   08452abdadb3db1e686b94a9c52fc7b6.jpg      6\n",
       "548  2541b59d54a3a9f2681c0049f7ddd85c.jpg      6\n",
       "696  82fbc93cd45ab747e7e606f2c52c7335.jpg      3\n",
       "238  2e8df36b35d22cf219cf8bae6c2af752.jpg      5\n",
       "963  bcb51cec3d290e6a661586d0df30e091.jpg      4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f719df23",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d389b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(\n",
    "    train,\n",
    "    image_dir=IMAGE_FOLDER,\n",
    "    processor=preprocess_input,\n",
    ")\n",
    "val_dataset = ImageDataset(\n",
    "    val,\n",
    "    image_dir=IMAGE_FOLDER,\n",
    "    processor=preprocess_input,\n",
    ")\n",
    "test_dataset = ImageDataset(\n",
    "    test,\n",
    "    image_dir=IMAGE_FOLDER,\n",
    "    processor=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e28c15",
   "metadata": {},
   "source": [
    "# EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba5ed547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  <KerasTensor shape=(None, 224, 224, 3), dtype=float32, sparse=False, ragged=False, name=keras_tensor_482>\n",
      "Output:  <KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, ragged=False, name=keras_tensor_719>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github/oc_p9/notebook-tf/.venv/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 432 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Enregistre les paramètres du modèle\n",
    "model_params = {\n",
    "    \"include_top\": False,\n",
    "    \"weights\": \"imagenet\",\n",
    "    \"input_shape\": (224, 224, 3),\n",
    "}\n",
    "\n",
    "# Charge le modèle EfficientNetB0\n",
    "model = EfficientNetB0(**model_params)\n",
    "\n",
    "# Affiche les couches du modèle d'entrée et de sortie\n",
    "print(\"Input: \", model.input)\n",
    "print(\"Output: \", model.output)\n",
    "\n",
    "# Ajoute les dernières couches denses du modèle\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
    "predictions = tf.keras.layers.Dense(\n",
    "    N_CLASSES, activation=\"softmax\", name=\"predictions\"\n",
    ")(x)\n",
    "\n",
    "# Crée un modèle à partir des inputs et des outputs\n",
    "model = Model(inputs=model.input, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Définit le nom du modèle à charger\n",
    "model_name = ARTIFACTS_FOLDER / \"effnet.weights.h5\"\n",
    "\n",
    "# Charger les poids du modèle sauvegardés\n",
    "model.load_weights(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71ef5737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING EXPERIMENT ID <20250507-143801_EfficientNetB0_custom_unfreezed>\n",
      "==========================\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "Warming up the model...\n",
      "Test Loss: 13.0194 | Test Acc: 0.7911 | Running steps test time: 1.11 s.\n"
     ]
    }
   ],
   "source": [
    "# Define the experiment ID\n",
    "model_card = \"EfficientNetB0_custom\"\n",
    "experiment_id = generate_experiment_id(model_card, freeze_backbone=False)\n",
    "log_dir = ROOT_FOLDER / \"runs\" / experiment_id\n",
    "\n",
    "# Create the writer for TensorBoard\n",
    "writer = tf.summary.create_file_writer(str(log_dir))\n",
    "\n",
    "with writer.as_default():\n",
    "    # TESTING LOOP\n",
    "    print(f\"TESTING EXPERIMENT ID <{experiment_id}>\")\n",
    "    print(\"==========================\")\n",
    "    # Warming up the model\n",
    "    warming_up(model, test_dataset)\n",
    "    # Iterate over the dataset batch by batch\n",
    "    batch_times = []\n",
    "    loss, running_time = 0.0, 0.0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for step, x_batch in enumerate(test_dataset):\n",
    "        t0 = time()\n",
    "        inputs, labels = x_batch\n",
    "        preds = model.predict_on_batch(inputs)\n",
    "        t1 = time() - t0\n",
    "        batch_times.append(t1)\n",
    "        # Compute the loss with preds & running time\n",
    "        loss += model.evaluate(inputs, labels, verbose=0)[0]\n",
    "        running_time += t1\n",
    "        tf.summary.scalar(\"TimingByStep/test\", running_time, step=step)\n",
    "        # Add the predictions and labels with the argmax to loose the one-hot encoding\n",
    "        y_pred.extend(np.argmax(preds, axis=1))\n",
    "        y_true.extend(np.argmax(labels, axis=1))\n",
    "\n",
    "    classifier_report = classification_report(\n",
    "        y_true, y_pred, target_names=CLASSES, zero_division=0, output_dict=True\n",
    "    )\n",
    "    formatted_labels = split_labels_on_and_or_ampersand(CLASSES)\n",
    "\n",
    "    # Print the test metrics\n",
    "    print(\n",
    "        f\"Test Loss: {loss:.4f} | Test Acc: {classifier_report['accuracy']:.4f} | Running steps test time: {np.sum(batch_times):.2f} s.\"\n",
    "    )\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    cm = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=range(N_CLASSES),\n",
    "        normalize=None,\n",
    "        display_labels=formatted_labels,\n",
    "        values_format=\".2g\",\n",
    "        xticks_rotation=\"vertical\",\n",
    "        colorbar=False,\n",
    "        cmap=plt.cm.Blues,\n",
    "    )\n",
    "    cm.figure_.tight_layout()\n",
    "    fig = plot_to_image(cm.figure_)\n",
    "\n",
    "    # Write the confusion matrix\n",
    "    tf.summary.image(\"ConfusionMatrix/test\", fig, 0)\n",
    "    # Save the loss and accuracy\n",
    "    tf.summary.scalar(\"Accuracy/test\", classifier_report[\"accuracy\"], 0)\n",
    "    # Save the classification report\n",
    "    for label, metrics in classifier_report.items():\n",
    "        if isinstance(metrics, dict):\n",
    "            for metric, value in metrics.items():\n",
    "                # Add the metric to the scalar\n",
    "                tf.summary.scalar(f\"ClassificationReport/{label}/{metric}\", value, 0)\n",
    "        else:\n",
    "            tf.summary.scalar(f\"ClassificationReport/{label}\", metrics, 0)\n",
    "\n",
    "# Flush the writer\n",
    "writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
